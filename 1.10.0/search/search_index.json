{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"\ud83c\udf1f Keras Data Processor (KDP)","text":"Transform your raw data into powerful ML-ready features <p>A high-performance preprocessing library for tabular data built on TensorFlow. KDP combines the best of traditional preprocessing with advanced neural approaches to create state-of-the-art feature transformations.</p> \ud83d\ude80 Getting Started <ul> <li>Quick Start Guide</li> <li>Why KDP Exists</li> <li>Installation</li> <li>Architecture Overview</li> </ul> \ud83d\udee0\ufe0f Feature Processing <ul> <li>Feature Types Overview</li> <li>Numerical Features</li> <li>Categorical Features</li> <li>Text Features</li> <li>Date Features</li> <li>Cross Features</li> </ul> \ud83e\udde0 Advanced Features <ul> <li>Distribution-Aware Encoding</li> <li>Tabular Attention</li> <li>Feature-wise Mixture of Experts</li> <li>Feature Selection</li> <li>Advanced Numerical Embeddings</li> <li>Transformer Blocks</li> </ul> \u26a1 Optimization <ul> <li>Tabular Optimization</li> <li>Auto-Configuration</li> <li>Feature Selection</li> </ul> \ud83d\udd17 Integrations <ul> <li>Integration Overview</li> </ul> \ud83d\udcda Examples <ul> <li>Basic Examples</li> <li>Complex Examples</li> </ul> \ud83d\udcda Reference <ul> <li>API Reference</li> </ul> \ud83e\udd1d Contributing <ul> <li>Contribution Guide</li> <li>Auto-Documentation</li> </ul> \ud83d\udcc8 Key Features <ul> <li>\u2713 Smart distribution detection</li> <li>\u2713 Neural feature interactions</li> <li>\u2713 Feature-wise Mixture of Experts</li> <li>\u2713 Memory-efficient processing</li> <li>\u2713 Single-pass optimization</li> <li>\u2713 Production-ready scaling</li> </ul>"},{"location":"index.html#why-choose-kdp","title":"\ud83c\udfc6 Why Choose KDP?","text":"Challenge Traditional Approach KDP's Solution Complex Distributions Fixed binning strategies \ud83d\udcca Distribution-Aware Encoding that adapts to your specific data Interaction Discovery Manual feature crosses \ud83d\udc41\ufe0f Tabular Attention that automatically finds important relationships Heterogeneous Features Uniform processing \ud83e\udde9 Feature-wise Mixture of Experts that specializes processing per feature Feature Importance Post-hoc analysis \ud83c\udfaf Built-in Feature Selection during training Performance at Scale Memory issues with large datasets \u26a1 Optimized Processing Pipeline with batching and caching"},{"location":"index.html#quick-example","title":"\ud83d\ude80 Quick Example","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Define your features\nfeatures = {\n    \"age\": FeatureType.FLOAT_NORMALIZED,\n    \"income\": FeatureType.FLOAT_RESCALED,\n    \"occupation\": FeatureType.STRING_CATEGORICAL,\n    \"description\": FeatureType.TEXT\n}\n\n# Create and build your preprocessor\npreprocessor = PreprocessingModel(\n    path_data=\"data.csv\",\n    features_specs=features,\n    use_distribution_aware=True,  # Smart distribution handling\n    tabular_attention=True,       # Automatic feature interactions\n    use_feature_moe=True,         # Specialized processing per feature\n    feature_moe_num_experts=4     # Number of specialized experts\n)\n\n# Build and use\nresult = preprocessor.build_preprocessor()\nmodel = result[\"model\"]\n</code></pre>"},{"location":"index.html#architecture-diagram","title":"\ud83d\udd04 Architecture Diagram","text":"<pre><code>%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#f0f7ff', 'primaryTextColor': '#333', 'primaryBorderColor': '#4a86e8', 'lineColor': '#4a86e8', 'secondaryColor': '#fff0f7', 'tertiaryColor': '#f7fff0' }}}%%\ngraph TD\n    A[Raw Data] --&gt; B[PreprocessingModel]\n    B --&gt; |Numerical Features| C1[Distribution-Aware Encoding]\n    B --&gt; |Categorical Features| C2[Smart Encoding]\n    B --&gt; |Text Features| C3[Text Vectorization]\n    B --&gt; |Date Features| C4[Date Preprocessing]\n\n    C1 --&gt; D[Feature-wise MoE]\n    C2 --&gt; D\n    C3 --&gt; D\n    C4 --&gt; D\n\n    D --&gt; E[Tabular Attention]\n    E --&gt; F[Feature Selection]\n    F --&gt; G[ML-Ready Features]\n\n    classDef input fill:#e6f3ff,stroke:#4a86e8,stroke-width:2px,rx:8px,ry:8px;\n    classDef process fill:#fff9e6,stroke:#ffb74d,stroke-width:2px,rx:8px,ry:8px;\n    classDef feature fill:#e6fff9,stroke:#26a69a,stroke-width:2px,rx:8px,ry:8px;\n    classDef output fill:#e8f5e9,stroke:#66bb6a,stroke-width:2px,rx:8px,ry:8px;\n\n    class A input;\n    class B,C1,C2,C3,C4 process;\n    class D,E,F feature;\n    class G output;</code></pre>"},{"location":"index.html#find-what-you-need","title":"\ud83d\udd0d Find What You Need","text":"\ud83d\udd30 New to KDP? Start with the Quick Start Guide \ud83d\udd0d Specific feature type? Check the Feature Processing section    \u26a1 Performance issues? See the Optimization guides    \ud83d\udd0c Integration help? Visit the Integration Overview section    \ud83d\udcdd Practical examples? Browse our Examples \ud83d\udcda API details? Refer to the API Reference documentation"},{"location":"index.html#community-support","title":"\ud83d\udce3 Community &amp; Support","text":"\ud83d\udc19 GitHub Repository \ud83d\udc1b Issue Tracker \ud83d\udcdc MIT License - Open source and free to use"},{"location":"advanced/custom-preprocessing.html","title":"\ud83d\udee0\ufe0f Custom Preprocessing Pipelines","text":"Create specialized preprocessing flows with complete control <p>Design custom transformations for your features when standard preprocessing doesn't meet your specific needs.</p>"},{"location":"advanced/custom-preprocessing.html#overview","title":"\ud83d\udccb Overview","text":"<p>KDP allows you to define custom preprocessing pipelines for your features, giving you complete control over how each feature is processed before being fed into your model. This is particularly useful when the standard preprocessing options don't meet your specific needs.</p>"},{"location":"advanced/custom-preprocessing.html#key-benefits","title":"\u2728 Key Benefits","text":"\ud83d\udd0d Specific Transformations <p>Define custom preprocessing steps not covered by built-in options</p> \ud83d\udd04 Combined Techniques <p>Combine multiple preprocessing techniques in a single pipeline</p> \ud83e\uddea Domain-Specific <p>Handle specialized data with custom preprocessing logic</p> \ud83d\udd2c Novel Approaches <p>Experiment with new preprocessing methods</p> \ud83e\udde9 Legacy Integration <p>Incorporate existing preprocessing logic</p>"},{"location":"advanced/custom-preprocessing.html#getting-started","title":"\ud83d\ude80 Getting Started","text":"1 Basic Example <pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom tensorflow.keras.layers import Normalization, Dense, Activation\n\n# Create a feature with custom preprocessing steps\nlog_transform_feature = NumericalFeature(\n    name=\"revenue\",\n    feature_type=FeatureType.FLOAT_NORMALIZED,\n    preprocessors=[\n        \"Lambda\",  # Using a standard Keras layer by name\n        \"Dense\",   # Another standard layer\n        \"ReLU\"     # Activation function\n    ],\n    # Parameters for the layers\n    function=lambda x: tf.math.log1p(x),  # For Lambda layer\n    units=16,  # For Dense layer\n)\n</code></pre>   ## \ud83e\udd14 When to Use Custom Preprocessing  Consider using custom preprocessing pipelines when:  - \ud83d\udd0d You need specific transformations not covered by the built-in options - \ud83d\udd04 You want to combine multiple preprocessing techniques - \ud83e\uddea You're working with domain-specific data that requires specialized handling - \ud83d\udd2c You want to experiment with novel preprocessing approaches - \ud83e\udde9 You have legacy preprocessing logic that you want to incorporate  ## \ud83d\udce6 Custom Preprocessors with PreprocessingModel  ### \ud83c\udfc1 Basic Approach  The simplest way to define custom preprocessing is by specifying a list of preprocessors when creating a feature:  <pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom tensorflow.keras.layers import Normalization, Dense, Activation\n\n# Create a feature with custom preprocessing steps\nlog_transform_feature = NumericalFeature(\n    name=\"revenue\",\n    feature_type=FeatureType.FLOAT_NORMALIZED,\n    preprocessors=[\n        \"Lambda\",  # Using a standard Keras layer by name\n        \"Dense\",   # Another standard layer\n        \"ReLU\"     # Activation function\n    ],\n    # Parameters for the layers\n    function=lambda x: tf.math.log1p(x),  # For Lambda layer\n    units=16,  # For Dense layer\n)\n</code></pre>  ### \ud83d\ude80 Advanced Usage with Layer Parameters  For more control, you can provide specific parameters to each preprocessing layer:  <pre><code>from kdp.features import CategoricalFeature, FeatureType\nfrom kdp.layers_factory import PreprocessorLayerFactory\n\n# Advanced categorical feature with custom preprocessing\nadvanced_categorical = CategoricalFeature(\n    name=\"product_category\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    preprocessors=[\n        \"StringLookup\",\n        \"Embedding\",\n        \"Dropout\"\n    ],\n    # Parameters for layers\n    num_oov_indices=2,  # For StringLookup\n    input_dim=100,      # For Embedding\n    output_dim=32,      # For Embedding\n    rate=0.2            # For Dropout\n)\n</code></pre>  ### \ud83c\udfed Using the PreprocessorLayerFactory  For more complex scenarios, you can use the `PreprocessorLayerFactory` directly:  <pre><code>from kdp.features import TextFeature, FeatureType\nfrom kdp.layers_factory import PreprocessorLayerFactory\n\n# Create a text feature with custom preprocessing using the factory\ntext_feature = TextFeature(\n    name=\"review_text\",\n    feature_type=FeatureType.TEXT,\n    preprocessors=[\n        PreprocessorLayerFactory.text_preprocessing_layer,\n        \"TextVectorization\",\n        \"Embedding\"\n    ],\n    # Parameters\n    stop_words=[\"the\", \"and\", \"is\"],  # For TextPreprocessingLayer\n    max_tokens=10000,                 # For TextVectorization\n    output_sequence_length=50,        # For TextVectorization\n    output_dim=64                     # For Embedding\n)\n</code></pre>  ### \ud83e\udde9 Mixing Built-in and Custom Layers  You can combine KDP's specialized layers with standard Keras layers:  <pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom kdp.layers_factory import PreprocessorLayerFactory\n\n# Mix custom and specialized layers\nnumeric_feature = NumericalFeature(\n    name=\"transaction_amount\",\n    feature_type=FeatureType.FLOAT,\n    preprocessors=[\n        PreprocessorLayerFactory.cast_to_float32_layer,\n        \"Lambda\",\n        PreprocessorLayerFactory.distribution_aware_encoder,\n        \"Dense\"\n    ],\n    # Parameters\n    function=lambda x: tf.clip_by_value(x, 0, 1000),  # For Lambda\n    num_bins=100,                                     # For DistributionAwareEncoder\n    units=32                                          # For Dense\n)\n</code></pre>  ### \ud83c\udf1f Advanced KDP Layer Examples  KDP provides several specialized preprocessing layers that you can use for advanced feature processing:  #### \ud83d\udcb9 Distribution Transformation Layer  <pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom kdp.layers_factory import PreprocessorLayerFactory\n\n# Create a feature that applies distribution transformations\nskewed_feature = NumericalFeature(\n    name=\"highly_skewed_metric\",\n    feature_type=FeatureType.FLOAT,\n    preprocessors=[\n        PreprocessorLayerFactory.cast_to_float32_layer,\n        PreprocessorLayerFactory.distribution_transform_layer,\n    ],\n    # Parameters for DistributionTransformLayer\n    transform_type=\"box-cox\",  # Apply Box-Cox transformation\n    lambda_param=0.5,         # Parameter for Box-Cox\n    epsilon=1e-6              # Prevent numerical issues\n)\n\n# Automatic transformation selection\nauto_transform_feature = NumericalFeature(\n    name=\"unknown_distribution\",\n    feature_type=FeatureType.FLOAT,\n    preprocessors=[\n        PreprocessorLayerFactory.cast_to_float32_layer,\n        PreprocessorLayerFactory.distribution_transform_layer,\n    ],\n    # Let the layer choose the best transformation\n    transform_type=\"auto\",\n    auto_candidates=[\"log\", \"sqrt\", \"box-cox\", \"yeo-johnson\"]\n)\n</code></pre>  #### \ud83e\uddee Numerical Embeddings  <pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom kdp.layers_factory import PreprocessorLayerFactory\n\n# Create a feature with numerical embedding\nembedded_numeric = NumericalFeature(\n    name=\"user_age\",\n    feature_type=FeatureType.FLOAT,\n    preprocessors=[\n        PreprocessorLayerFactory.cast_to_float32_layer,\n        PreprocessorLayerFactory.numerical_embedding_layer,\n    ],\n    # Parameters for NumericalEmbedding\n    embedding_dim=16,       # Output dimension\n    mlp_hidden_units=32,    # MLP hidden units\n    num_bins=20,            # Number of bins for discretization\n    init_min=18,            # Minimum value for initialization\n    init_max=100,           # Maximum value for initialization\n    dropout_rate=0.2,       # Dropout rate\n    use_batch_norm=True     # Apply batch normalization\n)\n</code></pre>  #### \ud83c\udf10 Global Numerical Embedding  <pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom kdp.layers_factory import PreprocessorLayerFactory\n\n# Process multiple numeric features as a group with global pooling\nglobal_numerics = NumericalFeature(\n    name=\"numeric_group\",\n    feature_type=FeatureType.FLOAT,\n    preprocessors=[\n        PreprocessorLayerFactory.cast_to_float32_layer,\n        PreprocessorLayerFactory.global_numerical_embedding_layer,\n    ],\n    # Parameters for GlobalNumericalEmbedding\n    global_embedding_dim=32,        # Final embedding dimension\n    global_mlp_hidden_units=64,     # MLP hidden units\n    global_num_bins=15,             # Number of bins\n    global_dropout_rate=0.1,        # Dropout rate\n    global_use_batch_norm=True,     # Apply batch normalization\n    global_pooling=\"average\"        # Pooling method (\"average\" or \"max\")\n)\n</code></pre>  #### \ud83d\udd00 Gated Linear Unit  <pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom kdp.layers_factory import PreprocessorLayerFactory\n\n# Apply a gated linear unit to a feature\ngated_feature = NumericalFeature(\n    name=\"sales_volume\",\n    feature_type=FeatureType.FLOAT,\n    preprocessors=[\n        PreprocessorLayerFactory.cast_to_float32_layer,\n        \"Normalization\",\n        PreprocessorLayerFactory.gated_linear_unit_layer,\n    ],\n    # Parameters for GatedLinearUnit\n    units=32  # Output dimension\n)\n</code></pre>  #### \ud83d\udd04 Gated Residual Network  <pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom kdp.layers_factory import PreprocessorLayerFactory\n\n# Apply a gated residual network to a feature\ngrn_feature = NumericalFeature(\n    name=\"complex_metric\",\n    feature_type=FeatureType.FLOAT,\n    preprocessors=[\n        PreprocessorLayerFactory.cast_to_float32_layer,\n        PreprocessorLayerFactory.gated_residual_network_layer,\n    ],\n    # Parameters for GatedResidualNetwork\n    units=64,            # Output dimension\n    dropout_rate=0.3     # Dropout rate\n)\n</code></pre>  ### \u2795 Adding Preprocessors Dynamically  You can also add preprocessors dynamically after feature creation:  <pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom tensorflow.keras.layers import Dense\n\n# Create a feature\nfeature = NumericalFeature(\n    name=\"age\",\n    feature_type=FeatureType.FLOAT_NORMALIZED\n)\n\n# Add preprocessors later\nfeature.add_preprocessor(\"Normalization\")\nfeature.add_preprocessor(Dense, units=16, activation=\"relu\")\nfeature.add_preprocessor(PreprocessorLayerFactory.distribution_aware_encoder, num_bins=50)\n</code></pre>  ## \ud83d\ude84 DynamicPreprocessingPipeline  For even more flexibility and direct control over your preprocessing workflow, KDP offers the `DynamicPreprocessingPipeline`. This powerful tool allows you to create custom preprocessing pipelines with explicit dependency tracking between layers and seamless integration with TensorFlow's data pipelines.  ### \u2728 Key Benefits  - \ud83d\udd04 **Layer Dependencies**: Automatically tracks dependencies between layers - \ud83c\udfaf **Selective Computation**: Only computes what's needed based on dependencies - \ud83d\udcbe **Memory Efficiency**: Doesn't keep unnecessary intermediate tensors - \ud83d\udd0c **TF Data Integration**: Works directly with `tf.data.Dataset` objects - \ud83d\udee0\ufe0f **Customizability**: Use any Keras layer in your preprocessing pipeline - \ud83d\udcdd **Simplicity**: Cleaner approach for complex preprocessing compared to feature-based methods  ### \ud83c\udfc1 Basic Usage  <pre><code>import tensorflow as tf\nfrom kdp.dynamic_pipeline import DynamicPreprocessingPipeline\n\n# Create custom layers\nclass ScalingLayer(tf.keras.layers.Layer):\n    def __init__(self, scaling_factor=2.0, **kwargs):\n        super().__init__(**kwargs)\n        self.scaling_factor = scaling_factor\n\n    def call(self, inputs):\n        return inputs * self.scaling_factor\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"scaling_factor\": self.scaling_factor})\n        return config\n\nclass NormalizationLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def call(self, inputs):\n        mean = tf.reduce_mean(inputs, axis=0)\n        std = tf.math.reduce_std(inputs, axis=0)\n        return (inputs - mean) / (std + 1e-5)\n\n    def get_config(self):\n        return super().get_config()\n\n# Create the pipeline with custom layers\nscaling_layer = ScalingLayer(scaling_factor=3.0, name='scaling')\nnormalization_layer = NormalizationLayer(name='normalization')\npipeline = DynamicPreprocessingPipeline([scaling_layer, normalization_layer])\n\n# Create sample data with keys matching layer names\ndata = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]], dtype=np.float32)\ndataset = tf.data.Dataset.from_tensor_slices({\n    'scaling': data,\n    'normalization': data\n})\n\n# Process the data\nprocessed_dataset = pipeline.process(dataset)\n\n# Use the processed data\nfor element in processed_dataset:\n    print(\"Scaled data:\", element['scaling'].numpy())\n    print(\"Normalized data:\", element['normalization'].numpy())\n</code></pre>  ### \ud83d\udd17 Pipeline with Dependencies  The real power of `DynamicPreprocessingPipeline` comes from its ability to automatically handle dependencies between layers:  <pre><code>import tensorflow as tf\nfrom kdp.dynamic_pipeline import DynamicPreprocessingPipeline\n\n# Create a pipeline with a sequence of layers\nscaling_layer = ScalingLayer(scaling_factor=2.0, name='scaling')\nlog_layer = LogTransformLayer(name='log_transform')\nnorm_layer = NormalizationLayer(name='normalization')\n\n# Create pipeline with dependency order - each layer processes the output of the previous\npipeline = DynamicPreprocessingPipeline([scaling_layer, log_layer, norm_layer])\n\n# Only need to provide the initial input - the rest is handled automatically\ndata = np.array([[1.0], [5.0], [10.0], [50.0], [100.0]], dtype=np.float32)\ndataset = tf.data.Dataset.from_tensor_slices({\n    'scaling': data,  # Only provide the input for the first layer\n})\n\n# Process the data\nprocessed_dataset = pipeline.process(dataset)\n\n# Access all intermediate and final outputs\nfor element in processed_dataset:\n    print(\"Scaled data:\", element['scaling'].numpy())\n    print(\"Log-transformed data:\", element['log_transform'].numpy())\n    print(\"Normalized data:\", element['normalization'].numpy())\n</code></pre>  ### \ud83e\udde9 Processing Multiple Feature Types  You can create separate pipelines for different feature types:  <pre><code>import tensorflow as tf\nimport numpy as np\nfrom kdp.dynamic_pipeline import DynamicPreprocessingPipeline\n\n# Custom encoding layer for categorical features\nclass EncodingLayer(tf.keras.layers.Layer):\n    def __init__(self, vocabulary=None, **kwargs):\n        super().__init__(**kwargs)\n        self.vocabulary = vocabulary or []\n\n    def build(self, input_shape):\n        self.lookup_table = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary,\n            mask_token=None,\n            num_oov_indices=1\n        )\n        super().build(input_shape)\n\n    def call(self, inputs):\n        indices = self.lookup_table(inputs)\n        return tf.one_hot(indices, depth=len(self.vocabulary) + 1)\n\n# Create pipelines for different feature types\nnumeric_scaling = ScalingLayer(scaling_factor=2.0, name='numeric_scaling')\nnumeric_pipeline = DynamicPreprocessingPipeline([numeric_scaling])\n\ncategorical_encoding = EncodingLayer(\n    vocabulary=['A', 'B', 'C'],\n    name='categorical_encoding'\n)\ncategorical_pipeline = DynamicPreprocessingPipeline([categorical_encoding])\n\n# Process different types of data\nnumeric_data = np.array([[1.0], [2.0], [3.0], [4.0]], dtype=np.float32)\ncategorical_data = np.array([['A'], ['B'], ['C'], ['D']], dtype=np.object_)\n\nnumeric_dataset = tf.data.Dataset.from_tensor_slices({\n    'numeric_scaling': numeric_data\n})\n\ncategorical_dataset = tf.data.Dataset.from_tensor_slices({\n    'categorical_encoding': categorical_data\n})\n\n# Process each dataset\nprocessed_numeric = numeric_pipeline.process(numeric_dataset)\nprocessed_categorical = categorical_pipeline.process(categorical_dataset)\n</code></pre>  ### \ud83d\udd04 Integration with Keras Models  The `DynamicPreprocessingPipeline` can be easily integrated with Keras models:  <pre><code>import tensorflow as tf\nfrom kdp.dynamic_pipeline import DynamicPreprocessingPipeline\n\n# Create preprocessing pipeline\nscaling_layer = ScalingLayer(scaling_factor=2.0, name='scaling')\nnormalization_layer = NormalizationLayer(name='normalization')\npreprocess_pipeline = DynamicPreprocessingPipeline([scaling_layer, normalization_layer])\n\n# Create a simple Keras model\ninputs = tf.keras.Input(shape=(1,), name='model_input')\ndense1 = tf.keras.layers.Dense(10, activation='relu')(inputs)\noutputs = tf.keras.layers.Dense(1)(dense1)\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer='adam', loss='mse')\n\n# Prepare data\ndata = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]], dtype=np.float32)\ntargets = np.array([[2.0], [4.0], [6.0], [8.0], [10.0]], dtype=np.float32)\n\n# Create dataset and preprocess\ndataset = tf.data.Dataset.from_tensor_slices({\n    'scaling': data,\n    'normalization': data,\n    'y': targets\n}).batch(2)\n\nprocessed_dataset = preprocess_pipeline.process(dataset)\n\n# Create training data generator\ndef data_generator():\n    for batch in processed_dataset:\n        # Use the normalized data as model input\n        x = batch['normalization']\n        y = batch['y']\n        yield x, y\n\n# Create a dataset from the generator and train the model\ntrain_dataset = tf.data.Dataset.from_generator(\n    data_generator,\n    output_signature=(\n        tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n    )\n)\n\nmodel.fit(train_dataset, epochs=5)\n</code></pre>  ## \ud83d\udee0\ufe0f Creating Custom Preprocessing Layers  For even more flexibility, you can create your own custom preprocessing layer:  <pre><code>import tensorflow as tf\n\nclass CustomScalingLayer(tf.keras.layers.Layer):\n    def __init__(self, scaling_factor=10.0, **kwargs):\n        super().__init__(**kwargs)\n        self.scaling_factor = scaling_factor\n\n    def call(self, inputs):\n        return inputs * self.scaling_factor\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"scaling_factor\": self.scaling_factor})\n        return config\n\n# Use your custom layer in a feature\nfrom kdp.features import NumericalFeature, FeatureType\n\nfeature = NumericalFeature(\n    name=\"custom_scaled\",\n    feature_type=FeatureType.FLOAT,\n    preprocessors=[\n        CustomScalingLayer,\n        \"Dense\"\n    ],\n    scaling_factor=5.0,  # For CustomScalingLayer\n    units=16             # For Dense\n)\n</code></pre>  ## \ud83e\udd14 When to Use DynamicPreprocessingPipeline vs. PreprocessingModel  Both approaches have their strengths:  | \ud83d\ude84 **Use DynamicPreprocessingPipeline when:** | \ud83d\udce6 **Use PreprocessingModel when:** | |---|---| | \ud83d\udd0d You need fine-grained control over the preprocessing flow | \ud83d\udd04 You want the full KDP feature mapping system | | \ud83d\udd17 You want explicit dependency tracking between layers | \ud83e\udde9 You need integration with other KDP features (feature selection, etc.) | | \ud83d\udcca You're working with `tf.data.Dataset` and want efficient streaming | \ud83d\udcdd You prefer a declarative approach with feature specifications | | \ud83e\uddea You prefer a more procedural approach to preprocessing | \ud83d\udcbe You want to save the entire preprocessing model as one unit | | \u26a1 You want to avoid the overhead of the feature-based system | |  ## \ud83d\udccb Complete Example with DynamicPreprocessingPipeline  Here's a comprehensive example showing how to use `DynamicPreprocessingPipeline` with various custom layer types:  <pre><code>import tensorflow as tf\nimport numpy as np\nfrom kdp.dynamic_pipeline import DynamicPreprocessingPipeline\n\n# Define custom layers\nclass ScalingLayer(tf.keras.layers.Layer):\n    def __init__(self, scaling_factor=2.0, **kwargs):\n        super().__init__(**kwargs)\n        self.scaling_factor = scaling_factor\n\n    def call(self, inputs):\n        return inputs * self.scaling_factor\n\nclass LogTransformLayer(tf.keras.layers.Layer):\n    def __init__(self, offset=1.0, **kwargs):\n        super().__init__(**kwargs)\n        self.offset = offset\n\n    def call(self, inputs):\n        return tf.math.log(inputs + self.offset)\n\nclass NormalizationLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def call(self, inputs):\n        mean = tf.reduce_mean(inputs, axis=0)\n        std = tf.math.reduce_std(inputs, axis=0)\n        return (inputs - mean) / (std + 1e-5)\n\nclass EncodingLayer(tf.keras.layers.Layer):\n    def __init__(self, vocabulary=None, **kwargs):\n        super().__init__(**kwargs)\n        self.vocabulary = vocabulary or []\n\n    def build(self, input_shape):\n        self.lookup_table = tf.keras.layers.StringLookup(\n            vocabulary=self.vocabulary,\n            mask_token=None,\n            num_oov_indices=1\n        )\n        super().build(input_shape)\n\n    def call(self, inputs):\n        indices = self.lookup_table(inputs)\n        return tf.one_hot(indices, depth=len(self.vocabulary) + 1)\n\n# Create a multi-step pipeline\nscaling = ScalingLayer(scaling_factor=2.0, name='scaling')\nlog_transform = LogTransformLayer(name='log_transform')\nnormalization = NormalizationLayer(name='normalization')\npipeline = DynamicPreprocessingPipeline([scaling, log_transform, normalization])\n\n# Create sample data\nnumeric_data = np.array([[1.0], [5.0], [10.0], [50.0], [100.0]], dtype=np.float32)\ndataset = tf.data.Dataset.from_tensor_slices({\n    'scaling': numeric_data  # Initial input\n}).batch(2)\n\n# Process the data\nprocessed_dataset = pipeline.process(dataset)\n\n# Create a model\ninputs = tf.keras.Input(shape=(1,))\nx = tf.keras.layers.Dense(10, activation='relu')(inputs)\noutputs = tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer='adam', loss='mse')\n\n# Use the preprocessed data for training\ndef data_generator():\n    for batch in processed_dataset:\n        # Use the fully processed data\n        x = batch['normalization']\n        y = x * 2  # Synthetic targets\n        yield x, y\n\ntrain_dataset = tf.data.Dataset.from_generator(\n    data_generator,\n    output_signature=(\n        tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n        tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n    )\n)\n\n# Train the model\nmodel.fit(train_dataset, epochs=5)\n</code></pre>  ## \ud83d\udcdd Summary  KDP offers multiple approaches to custom preprocessing, from simple layer addition to sophisticated dynamic pipelines. The `DynamicPreprocessingPipeline` provides a powerful and flexible way to create custom preprocessing workflows with explicit dependency tracking, while the feature-based approach with `PreprocessingModel` offers integration with KDP's broader feature handling ecosystem. Choose the approach that best fits your specific needs and workflow preferences.  ## \ud83d\udca1 Best Practices  1. \ud83c\udfc1 **Start Simple**: Begin with the simplest preprocessing pipeline that meets your needs 2. \ud83e\uddea **Test Incrementally**: Add preprocessing steps one at a time and test their impact 3. \u26a1 **Consider Performance**: Complex preprocessing can impact training and inference speed 4. \ud83d\udcbe **Monitor Memory Usage**: Custom preprocessing can increase memory requirements 5. \ud83d\udcdd **Document Your Approach**: Document why custom preprocessing was necessary 6. \ud83d\udd01 **Ensure Reproducibility**: Make sure custom preprocessing is deterministic  ## \ud83e\udd16 Auto-Configuration Script  KDP provides an auto-configuration script that analyzes your dataset and recommends optimal preprocessing configurations. This tool can help you get started quickly by automatically detecting feature types and suggesting appropriate preprocessing steps.  ### \ud83d\ude80 Basic Usage  <pre><code>from kdp import auto_configure\n\n# Analyze your dataset and get recommendations\nconfig = auto_configure(\n    data_path=\"your_data.csv\",\n    batch_size=50000,\n    save_stats=True\n)\n\n# Review the recommendations\nprint(config[\"recommendations\"])  # Feature-specific recommendations\nprint(config[\"code_snippet\"])     # Ready-to-use code\n</code></pre>  ### \ud83d\udcca What It Analyzes  The auto-configuration script examines:  - \ud83d\udd0d **Data Distributions**: Identifies patterns in numerical data - \ud83d\udcc8 **Feature Statistics**: Calculates mean, variance, skewness, etc. - \ud83c\udfaf **Value Ranges**: Detects min/max values and outliers - \ud83d\udd04 **Value Patterns**: Distinguishes between discrete and continuous values  ### \ud83d\udee0\ufe0f Command Line Interface  You can also use the script from the command line:  <pre><code>python -m kdp.scripts.analyze_dataset \\\n    --data your_data.csv \\\n    --output recommendations.json \\\n    --stats features_stats.json \\\n    --batch-size 50000\n</code></pre>  ### \ud83d\udcdd Example Output  The script generates a comprehensive report including:  <pre><code>{\n    \"recommendations\": {\n        \"income\": {\n            \"feature_type\": \"NumericalFeature\",\n            \"preprocessing\": [\"NORMALIZATION\"],\n            \"detected_distribution\": \"log_normal\",\n            \"config\": {\n                \"embedding_dim\": 16,\n                \"num_bins\": 20\n            }\n        },\n        \"age\": {\n            \"feature_type\": \"NumericalFeature\",\n            \"preprocessing\": [\"NORMALIZATION\"],\n            \"detected_distribution\": \"normal\",\n            \"config\": {\n                \"embedding_dim\": 8,\n                \"num_bins\": 10\n            }\n        }\n    },\n    \"code_snippet\": \"# Generated code implementing the recommendations\",\n    \"statistics\": {\n        # Detailed feature statistics\n    }\n}\n</code></pre>  ### \ud83d\udca1 Pro Tips for Auto-Configuration  1. **Review Before Implementing**: Always review the recommendations before applying them 2. **Combine with Domain Knowledge**: Use the recommendations alongside your expertise 3. **Update When Data Changes**: Rerun the analysis when your data distribution changes 4. **Customize as Needed**: Modify the generated code to match your specific requirements  ## \u26a0\ufe0f Limitations and Considerations  - \ud83d\udcbe Custom preprocessing layers must be compatible with TensorFlow's serialization - \ud83d\udd04 All layers must implement `get_config()` and `from_config()` for proper saving/loading - \u23f1\ufe0f Complex custom preprocessing may impact performance - \ud83d\ude80 Consider using `tf.function` for performance-critical custom operations - \ud83e\uddea Ensure custom preprocessing works in both eager and graph execution modes  ## \ud83d\udd0d Advanced Topics  ### \ud83d\udd04 Handling Stateful Preprocessing  For preprocessing that requires state (like normalization), ensure proper initialization:  <pre><code># Stateful preprocessing example\nfrom kdp.features import NumericalFeature, FeatureType\nimport tensorflow as tf\n\nfeature = NumericalFeature(\n    name=\"height\",\n    feature_type=FeatureType.FLOAT,\n    preprocessors=[\n        \"Normalization\"\n    ]\n)\n\n# The normalization layer needs to be adapted to the data\nmodel = PreprocessingModel(features={\"height\": feature})\nmodel.fit(data)  # This initializes the normalization statistics\n</code></pre>  ### \ud83d\ude80 GPU-Accelerated Custom Preprocessing  Ensure your custom layers leverage GPU acceleration when available:  <pre><code>class GPUAwareCustomLayer(tf.keras.layers.Layer):\n    @tf.function  # Enable graph execution for better GPU performance\n    def call(self, inputs):\n        # Use TensorFlow operations that support GPU execution\n        return tf.nn.relu(inputs) * tf.math.sqrt(tf.abs(inputs))\n</code></pre>  ### \ud83d\udc1e Debugging Custom Preprocessing  To debug custom preprocessing pipelines:  <pre><code># Enable debug logging\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Create a model with your custom preprocessing\nmodel = PreprocessingModel(features=features)\n\n# Inspect the model layers\nmodel.build_model()\nmodel.model.summary()\n\n# Test with small batch\nsmall_batch = data.head(5)\nresult = model.transform(small_batch)\nprint(result)\n</code></pre>"},{"location":"advanced/distribution-aware-encoding.html","title":"\ud83d\udd04 Distribution-Aware Encoding","text":"Distribution-Aware Encoding <p>Automatically detect and handle various data distributions for optimal preprocessing.</p>"},{"location":"advanced/distribution-aware-encoding.html#overview","title":"\ud83d\udccb Overview","text":"<p>The Distribution-Aware Encoder is a powerful preprocessing layer that automatically detects and handles various data distributions. It intelligently transforms your data while preserving its statistical properties, leading to better model performance.</p> \ud83d\udd0d Automatic Distribution Detection <p>Identifies data patterns using statistical analysis</p> \u2699\ufe0f Smart Transformations <p>Applies distribution-specific preprocessing</p> \ud83d\ude80 Production-Ready <p>Built with pure TensorFlow operations for deployment</p> \ud83d\udd0c Flexible Integration <p>Works seamlessly with KDP's preprocessing pipeline</p> \ud83d\udcca Graph Mode Compatible <p>Works in both eager and graph execution modes</p> \ud83d\udce6 Memory Efficient <p>Optimized for large-scale datasets</p>"},{"location":"advanced/distribution-aware-encoding.html#use-cases","title":"\ud83c\udfaf Use Cases","text":"\ud83d\udcb0 Financial Data <p>Handling heavy-tailed distributions in price movements</p> \ud83d\udce1 Sensor Data <p>Processing periodic patterns in time series</p> \ud83d\udc64 User Behavior <p>Managing sparse data with many zeros</p> \ud83c\udf0d Natural Phenomena <p>Handling multimodal distributions</p> \ud83d\udd22 Count Data <p>Processing discrete and zero-inflated distributions</p>"},{"location":"advanced/distribution-aware-encoding.html#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"advanced/distribution-aware-encoding.html#basic-usage","title":"Basic Usage","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Define numerical features\nfeatures_specs = {\n    \"price\": FeatureType.FLOAT_NORMALIZED,\n    \"volume\": FeatureType.FLOAT_RESCALED,\n    \"rating\": FeatureType.FLOAT_NORMALIZED\n}\n\n# Initialize model with distribution-aware encoding\npreprocessor = PreprocessingModel(\n    path_data=\"data/my_data.csv\",\n    features_specs=features_specs,\n    use_distribution_aware=True,  # Enable distribution-aware encoding\n    distribution_aware_bins=1000  # Number of bins for distribution analysis\n)\n</code></pre>"},{"location":"advanced/distribution-aware-encoding.html#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from kdp.features import NumericalFeature\nfrom kdp.layers.distribution_aware_encoder_layer import DistributionType\n\nfeatures_specs = {\n    \"price\": NumericalFeature(\n        name=\"price\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        preferred_distribution=DistributionType.LOG_NORMAL  # Specify distribution\n    ),\n    \"volume\": NumericalFeature(\n        name=\"volume\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        preferred_distribution=DistributionType.ZERO_INFLATED  # Handle sparse data\n    )\n}\n\npreprocessor = PreprocessingModel(\n    path_data=\"data/my_data.csv\",\n    features_specs=features_specs,\n    use_distribution_aware=True,\n    distribution_aware_bins=1000,\n    detect_periodicity=True,  # Enable periodic pattern detection\n    handle_sparsity=True     # Enable sparse data handling\n)\n</code></pre>"},{"location":"advanced/distribution-aware-encoding.html#supported-distributions","title":"\ud83d\udcca Supported Distributions","text":"Distribution Type Description Detection Criteria Use Case Normal Standard bell curve Skewness &lt; 0.5, Kurtosis \u2248 3.0 Height, weight measurements Heavy-Tailed Longer tails than normal Kurtosis &gt; 4.0 Financial returns Multimodal Multiple peaks Multiple histogram peaks Mixed populations Uniform Even distribution Bounded between 0 and 1 Random sampling Exponential Exponential decay Positive values, skewness &gt; 1.0 Time between events Log-Normal Normal after log transform Positive values, skewness &gt; 2.0 Income distribution Discrete Finite distinct values Low unique value ratio (&lt; 0.1) Count data Periodic Cyclic patterns Significant autocorrelation Seasonal data Sparse Many zeros Zero ratio &gt; 0.5 User activity data Beta Bounded with shape parameters Bounded [0,1], skewness &gt; 0.5 Proportions Gamma Positive, right-skewed Positive values, mild skewness Waiting times Poisson Count data Discrete positive values Event counts Cauchy Extremely heavy-tailed Very high kurtosis (&gt; 10.0) Extreme events Zero-Inflated Excess zeros Moderate zero ratio (0.3-0.5) Rare events Bounded Known bounds Explicit bounds provided Physical measurements Ordinal Ordered categories Discrete ordered values Ratings, scores"},{"location":"advanced/distribution-aware-encoding.html#configuration-options","title":"\u2699\ufe0f Configuration Options","text":"Parameter Type Default Description <code>use_distribution_aware</code> bool False Enable distribution-aware encoding <code>distribution_aware_bins</code> int 1000 Number of bins for distribution analysis <code>detect_periodicity</code> bool True Detect and handle periodic patterns <code>handle_sparsity</code> bool True Special handling for sparse data <code>embedding_dim</code> int None Output dimension for feature projection <code>add_distribution_embedding</code> bool False Add learned distribution type embedding <code>epsilon</code> float 1e-6 Small value to prevent numerical issues <code>transform_type</code> str \"auto\" Type of transformation to apply"},{"location":"advanced/distribution-aware-encoding.html#best-practices","title":"\ud83c\udfaf Best Practices","text":"Distribution Detection <ul> <li>Start with automatic detection</li> <li>Specify preferred distributions only when confident</li> <li>Use appropriate bin sizes for your data scale</li> <li>Monitor detection accuracy with known distributions</li> </ul> Performance Optimization <ul> <li>Enable periodic detection for time series data</li> <li>Use sparse handling for data with many zeros</li> <li>Consider memory usage with large bin sizes</li> <li>Use appropriate embedding dimensions</li> </ul> Integration Tips <ul> <li>Combine with other KDP features for best results</li> <li>Use appropriate feature types (FLOAT_NORMALIZED, FLOAT_RESCALED)</li> <li>Monitor model performance with different configurations</li> <li>Consider using distribution embeddings for complex patterns</li> </ul>"},{"location":"advanced/distribution-aware-encoding.html#examples","title":"\ud83d\udd0d Examples","text":"Financial Data Processing <pre><code>from kdp import PreprocessingModel, FeatureType\nfrom kdp.features import NumericalFeature\nfrom kdp.layers.distribution_aware_encoder_layer import DistributionType\n\n# Define financial features\nfeatures_specs = {\n    \"price\": NumericalFeature(\n        name=\"price\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        preferred_distribution=DistributionType.LOG_NORMAL\n    ),\n    \"volume\": NumericalFeature(\n        name=\"volume\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        preferred_distribution=DistributionType.ZERO_INFLATED\n    ),\n    \"volatility\": NumericalFeature(\n        name=\"volatility\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        preferred_distribution=DistributionType.CAUCHY\n    )\n}\n\n# Create preprocessing model\npreprocessor = PreprocessingModel(\n    path_data=\"data/financial_data.csv\",\n    features_specs=features_specs,\n    use_distribution_aware=True,\n    distribution_aware_bins=1000,\n    detect_periodicity=True,  # For daily/weekly patterns\n    handle_sparsity=True,    # For low-volume periods\n    embedding_dim=32,        # Project to fixed dimension\n    add_distribution_embedding=True  # Add distribution information\n)\n</code></pre> Sensor Data Processing <pre><code>features_specs = {\n    \"temperature\": NumericalFeature(\n        name=\"temperature\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        preferred_distribution=DistributionType.NORMAL\n    ),\n    \"humidity\": NumericalFeature(\n        name=\"humidity\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        preferred_distribution=DistributionType.BETA  # Bounded between 0-100%\n    ),\n    \"pressure\": NumericalFeature(\n        name=\"pressure\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        preferred_distribution=DistributionType.NORMAL\n    )\n}\n\npreprocessor = PreprocessingModel(\n    path_data=\"data/sensor_data.csv\",\n    features_specs=features_specs,\n    use_distribution_aware=True,\n    distribution_aware_bins=500,  # Fewer bins for simpler distributions\n    detect_periodicity=True,      # For daily temperature cycles\n    handle_sparsity=False,       # No sparse data expected\n    embedding_dim=16             # Smaller embedding for simpler patterns\n)\n</code></pre>"},{"location":"advanced/distribution-aware-encoding.html#model-architecture","title":"\ud83d\udcca Model Architecture","text":"<p>The distribution-aware encoder architecture automatically adapts to your data's distribution, transforming numerical features to better match their underlying statistical properties, improving model performance.</p>"},{"location":"advanced/distribution-aware-encoding.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"\ud83d\udd22 Numerical Features \ud83e\uddee Advanced Numerical Embeddings \ud83d\udc41\ufe0f Tabular Attention \ud83c\udfaf Feature Selection \u2190 Cross Features Advanced Numerical Embeddings \u2192"},{"location":"advanced/feature-moe.html","title":"\ud83e\udde9 Feature-wise Mixture of Experts","text":"Feature-wise Mixture of Experts (MoE) <p>Specialized processing for heterogeneous tabular features</p>"},{"location":"advanced/feature-moe.html#overview","title":"\ud83d\udccb Overview","text":"<p>Feature-wise Mixture of Experts (MoE) is a powerful technique that applies different processing strategies to different features based on their characteristics. This approach allows for more specialized handling of each feature, improving model performance on complex, heterogeneous datasets.</p>"},{"location":"advanced/feature-moe.html#basic-usage","title":"\ud83d\ude80 Basic Usage","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Define features\nfeatures = {\n    \"age\": FeatureType.FLOAT_NORMALIZED,\n    \"income\": FeatureType.FLOAT_RESCALED,\n    \"occupation\": FeatureType.STRING_CATEGORICAL,\n    \"purchase_history\": FeatureType.FLOAT_ARRAY,\n}\n\n# Create preprocessor with Feature MoE\npreprocessor = PreprocessingModel(\n    path_data=\"data.csv\",\n    features_specs=features,\n    use_feature_moe=True,               # Turn on the magic\n    feature_moe_num_experts=4,          # Four specialized experts\n    feature_moe_expert_dim=64           # Size of expert representations\n)\n\n# Build and use\nresult = preprocessor.build_preprocessor()\nmodel = result[\"model\"]\n</code></pre>"},{"location":"advanced/feature-moe.html#how-feature-moe-works","title":"\ud83e\udde9 How Feature MoE Works","text":"<p>KDP's Feature MoE uses a \"divide and conquer\" approach with smart routing: each expert is a specialized neural network, a router determines which experts should process each feature, features can use multiple experts with different weights, and residual connections preserve original feature information.</p>"},{"location":"advanced/feature-moe.html#configuration-options","title":"\u2699\ufe0f Configuration Options","text":"Parameter Description Default Recommended Range <code>feature_moe_num_experts</code> Number of specialists 4 3-5 for most tasks, 6-8 for very complex data <code>feature_moe_expert_dim</code> Size of expert output 64 Larger (96-128) for complex patterns <code>feature_moe_routing</code> How to assign experts \"learned\" \"learned\" for automatic, \"predefined\" for control <code>feature_moe_sparsity</code> Use only top k experts 2 1-3 (lower = faster, higher = more accurate) <code>feature_moe_hidden_dims</code> Expert network size [64, 32] Deeper for complex relationships"},{"location":"advanced/feature-moe.html#pro-tips-for-feature-moe","title":"\ud83d\udca1 Pro Tips for Feature MoE","text":"Group Similar Features <p>Assign related features to the same expert for consistent processing, like grouping demographic, financial, product, and temporal features to different experts.</p> Visualize Expert Assignments <p>Examine which experts handle which features by plotting the assignments as a heatmap to understand your model's internal decisions.</p> Progressive Training <p>Start with frozen experts, then fine-tune to allow the model to learn basic patterns before specializing.</p>"},{"location":"advanced/feature-moe.html#when-to-use-feature-moe","title":"\ud83d\udd0d When to Use Feature MoE","text":"Heterogeneous Features <p>When your features have very different statistical properties (categorical, text, numerical, temporal).</p> Complex Multi-Modal Data <p>When features come from different sources or modalities (user features, item features, interaction features).</p> Transfer Learning <p>When adapting a model to new features with domain-specific experts for different feature groups.</p>"},{"location":"advanced/feature-moe.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"\ud83d\udd04 Distribution-Aware Encoding \ud83e\uddee Advanced Numerical Embeddings \ud83d\udc41\ufe0f Tabular Attention \ud83c\udfaf Feature Selection"},{"location":"advanced/numerical-embeddings.html","title":"\ud83d\udd22 Advanced Numerical Embeddings","text":"Transform raw numerical features into powerful representations <p>Enhance your model's ability to learn from numerical data with KDP's sophisticated dual-branch embedding architecture.</p>"},{"location":"advanced/numerical-embeddings.html#architecture-overview","title":"\ud83d\udccb Architecture Overview","text":"<p>Advanced Numerical Embeddings in KDP transform continuous values into meaningful embeddings using a dual-branch architecture:</p> 1 Continuous Branch <p>Processes raw values through a small MLP for smooth pattern learning</p> 2 Discrete Branch <p>Discretizes values into learnable bins with trainable boundaries</p> <p>The outputs from both branches are combined using a learnable gate mechanism, providing the perfect balance between continuous and discrete representations.</p>"},{"location":"advanced/numerical-embeddings.html#key-benefits","title":"\u2728 Key Benefits","text":"\ud83d\udee0\ufe0f Dual-Branch Architecture <p>Combines the best of both continuous and discrete processing</p> \ud83d\udccf Learnable Boundaries <p>Adapts bin edges during training for optimal discretization</p> \ud83c\udf9b\ufe0f Feature-Specific Processing <p>Each feature gets its own specialized embedding</p> \ud83d\udcbe Memory Efficient <p>Optimized for handling large-scale tabular datasets</p> \ud83d\udd17 Flexible Integration <p>Works seamlessly with other KDP features</p> \ud83d\udd27 Residual Connections <p>Ensures stability during training</p>"},{"location":"advanced/numerical-embeddings.html#getting-started","title":"\ud83d\ude80 Getting Started","text":"1 Basic Usage <pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Define numerical features\nfeatures_specs = {\n    \"age\": FeatureType.FLOAT_NORMALIZED,\n    \"income\": FeatureType.FLOAT_RESCALED,\n    \"credit_score\": FeatureType.FLOAT_NORMALIZED\n}\n\n# Initialize model with numerical embeddings\npreprocessor = PreprocessingModel(\n    path_data=\"data/my_data.csv\",\n    features_specs=features_specs,\n    use_numerical_embedding=True,  # Enable numerical embeddings\n    numerical_embedding_dim=8,     # Size of each feature's embedding\n    numerical_num_bins=10          # Number of bins for discretization\n)\n</code></pre> 2 Advanced Configuration <pre><code>from kdp import PreprocessingModel\nfrom kdp.features import NumericalFeature\nfrom kdp.enums import FeatureType\n\n# Define numerical features with customized embeddings\nfeatures_specs = {\n    \"age\": NumericalFeature(\n        name=\"age\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        use_embedding=True,\n        embedding_dim=8,\n        num_bins=10,\n        init_min=18,  # Domain-specific minimum\n        init_max=90   # Domain-specific maximum\n    ),\n    \"income\": NumericalFeature(\n        name=\"income\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        use_embedding=True,\n        embedding_dim=12,\n        num_bins=15,\n        init_min=0,     # Cannot be negative\n        init_max=500000 # Maximum expected\n    )\n}\n\n# Create preprocessing model\npreprocessor = PreprocessingModel(\n    path_data=\"data/my_data.csv\",\n    features_specs=features_specs,\n    use_numerical_embedding=True,\n    numerical_mlp_hidden_units=16,   # Hidden layer size for continuous branch\n    numerical_dropout_rate=0.1,      # Regularization\n    numerical_use_batch_norm=True    # Normalize activations\n)\n</code></pre>"},{"location":"advanced/numerical-embeddings.html#how-it-works","title":"\ud83e\udde0 How It Works","text":""},{"location":"advanced/numerical-embeddings.html#individual-feature-embeddings-numericalembedding","title":"Individual Feature Embeddings (<code>NumericalEmbedding</code>)","text":"<p>The <code>NumericalEmbedding</code> layer processes each numerical feature through two parallel branches:</p> <ol> <li>Continuous Branch:</li> <li>Processes each feature through a small MLP</li> <li>Applies dropout and optional batch normalization</li> <li> <p>Includes a residual connection for stability</p> </li> <li> <p>Discrete Branch:</p> </li> <li>Maps each value to a bin using learnable min/max boundaries</li> <li>Retrieves a learned embedding for each bin</li> <li> <p>Captures non-linear and discrete patterns</p> </li> <li> <p>Learnable Gate:</p> </li> <li>Combines outputs from both branches using a sigmoid gate</li> <li>Adaptively weights continuous vs. discrete representations</li> <li>Learns optimal combination per feature and dimension</li> </ol> <pre><code>Input value\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  MLP   \u2502    \u2502Binning \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502             \u2502\n         \u25bc             \u25bc\n   Continuous      Discrete\n   Embedding       Embedding\n         \u2502             \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n       Gating Mechanism\n               \u2502\n               \u25bc\n       Final Embedding\n</code></pre>"},{"location":"advanced/numerical-embeddings.html#global-feature-embeddings-globalnumericalembedding","title":"Global Feature Embeddings (<code>GlobalNumericalEmbedding</code>)","text":"<p>The <code>GlobalNumericalEmbedding</code> layer processes all numerical features together and returns a single compact representation:</p> <ol> <li>Flattens input features (if needed)</li> <li>Applies <code>NumericalEmbedding</code> to process all features</li> <li>Performs global pooling (average or max) across feature dimensions</li> <li>Returns a single vector representing all numerical features</li> </ol> <p>This approach is ideal for: - Processing large feature sets efficiently - Capturing cross-feature interactions - Reducing dimensionality of numerical data - Learning a unified numerical representation</p>"},{"location":"advanced/numerical-embeddings.html#configuration-options","title":"\u2699\ufe0f Configuration Options","text":""},{"location":"advanced/numerical-embeddings.html#individual-embeddings","title":"Individual Embeddings","text":"Parameter Type Default Description <code>use_numerical_embedding</code> bool False Enable numerical embeddings <code>numerical_embedding_dim</code> int 8 Size of each feature's embedding <code>numerical_mlp_hidden_units</code> int 16 Hidden layer size for continuous branch <code>numerical_num_bins</code> int 10 Number of bins for discretization <code>numerical_init_min</code> float/list -3.0 Initial minimum for scaling <code>numerical_init_max</code> float/list 3.0 Initial maximum for scaling <code>numerical_dropout_rate</code> float 0.1 Dropout rate for regularization <code>numerical_use_batch_norm</code> bool True Apply batch normalization"},{"location":"advanced/numerical-embeddings.html#global-embeddings","title":"Global Embeddings","text":"Parameter Type Default Description <code>use_global_numerical_embedding</code> bool False Enable global numerical embeddings <code>global_embedding_dim</code> int 8 Size of global embedding <code>global_mlp_hidden_units</code> int 16 Hidden layer size for continuous branch <code>global_num_bins</code> int 10 Number of bins for discretization <code>global_init_min</code> float/list -3.0 Initial minimum for scaling <code>global_init_max</code> float/list 3.0 Initial maximum for scaling <code>global_dropout_rate</code> float 0.1 Dropout rate for regularization <code>global_use_batch_norm</code> bool True Apply batch normalization <code>global_pooling</code> str \"average\" Pooling method (\"average\" or \"max\")"},{"location":"advanced/numerical-embeddings.html#best-use-cases","title":"\ud83c\udfaf Best Use Cases","text":""},{"location":"advanced/numerical-embeddings.html#when-to-use-individual-embeddings","title":"When to Use Individual Embeddings","text":"<ul> <li>When each numerical feature conveys distinct information</li> <li>When features have different scales or distributions</li> <li>When you need fine-grained control of each feature's representation</li> <li>When memory usage is a concern (more efficient with many features)</li> <li>For explainability (each feature has its own embedding)</li> </ul>"},{"location":"advanced/numerical-embeddings.html#when-to-use-global-embeddings","title":"When to Use Global Embeddings","text":"<ul> <li>When you have many numerical features</li> <li>When features have strong interdependencies</li> <li>When dimensionality reduction is desired</li> <li>When a unified representation of all numerical data is needed</li> <li>For simpler model architectures (single vector output)</li> </ul>"},{"location":"advanced/numerical-embeddings.html#examples","title":"\ud83d\udd0d Examples","text":""},{"location":"advanced/numerical-embeddings.html#financial-risk-modeling","title":"Financial Risk Modeling","text":"<pre><code>from kdp import PreprocessingModel\nfrom kdp.features import NumericalFeature\nfrom kdp.enums import FeatureType\n\n# Define financial features with domain knowledge\nfeatures_specs = {\n    \"income\": NumericalFeature(\n        name=\"income\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        use_embedding=True,\n        embedding_dim=8,\n        num_bins=15,\n        init_min=0,\n        init_max=1000000\n    ),\n    \"debt_ratio\": NumericalFeature(\n        name=\"debt_ratio\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        use_embedding=True,\n        embedding_dim=4,\n        num_bins=8,\n        init_min=0,\n        init_max=1  # Ratio typically between 0-1\n    ),\n    \"credit_score\": NumericalFeature(\n        name=\"credit_score\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        use_embedding=True,\n        embedding_dim=6,\n        num_bins=10,\n        init_min=300,\n        init_max=850  # Standard credit score range\n    ),\n    \"payment_history\": NumericalFeature(\n        name=\"payment_history\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        use_embedding=True,\n        embedding_dim=8,\n        num_bins=5,\n        init_min=0,\n        init_max=1  # Simplified score between 0-1\n    )\n}\n\n# Create preprocessing model\npreprocessor = PreprocessingModel(\n    path_data=\"data/financial_data.csv\",\n    features_specs=features_specs,\n    use_numerical_embedding=True,\n    numerical_mlp_hidden_units=16,\n    numerical_dropout_rate=0.2,  # Higher dropout for financial data\n    numerical_use_batch_norm=True\n)\n</code></pre>"},{"location":"advanced/numerical-embeddings.html#healthcare-patient-analysis","title":"Healthcare Patient Analysis","text":"<pre><code>from kdp import PreprocessingModel\nfrom kdp.features import NumericalFeature\nfrom kdp.enums import FeatureType\n\n# Define patient features\nfeatures_specs = {\n    # Define many health metrics\n    \"age\": NumericalFeature(...),\n    \"bmi\": NumericalFeature(...),\n    \"blood_pressure\": NumericalFeature(...),\n    \"cholesterol\": NumericalFeature(...),\n    \"glucose\": NumericalFeature(...),\n    # Many more metrics...\n}\n\n# Use global embedding to handle many numerical features\npreprocessor = PreprocessingModel(\n    path_data=\"data/patient_data.csv\",\n    features_specs=features_specs,\n    use_global_numerical_embedding=True,  # Process all features together\n    global_embedding_dim=32,              # Higher dimension for complex data\n    global_mlp_hidden_units=64,\n    global_num_bins=20,                   # More bins for medical precision\n    global_dropout_rate=0.1,\n    global_use_batch_norm=True,\n    global_pooling=\"max\"                  # Use max pooling to capture extremes\n)\n</code></pre>"},{"location":"advanced/numerical-embeddings.html#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"<ol> <li>Choose the Right Embedding Type</li> <li>Use individual embeddings for interpretability and precise control</li> <li> <p>Use global embeddings for efficiency with many numerical features</p> </li> <li> <p>Distribution-Aware Initialization</p> </li> <li>Set <code>init_min</code> and <code>init_max</code> based on your data's actual distribution</li> <li>Use domain knowledge to set meaningful boundary points</li> <li> <p>Initialize closer to anticipated feature range for faster convergence</p> </li> <li> <p>Dimensionality Guidelines</p> </li> <li>Start with <code>embedding_dim</code> = 4-8 for simple features</li> <li>Use 8-16 for complex features with non-linear patterns</li> <li> <p>For global embeddings, scale with the number of features (16-64)</p> </li> <li> <p>Performance Tuning</p> </li> <li>Increase <code>num_bins</code> for more granular discrete representations</li> <li>Adjust <code>mlp_hidden_units</code> to 2-4x the embedding dimension</li> <li>Use batch normalization for faster, more stable training</li> <li> <p>Adjust dropout based on dataset size (higher for small datasets)</p> </li> <li> <p>Combine with Other KDP Features</p> </li> <li>Pair with distribution-aware encoding for optimal numerical handling</li> <li>Use with tabular attention to learn cross-feature interactions</li> <li>Combine with feature selection for automatic dimensionality reduction</li> </ol>"},{"location":"advanced/numerical-embeddings.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"<ul> <li>Numerical Features</li> <li>Distribution-Aware Encoding</li> <li>Tabular Attention</li> <li>Feature Selection</li> </ul> \u2190 Distribution-Aware Encoding Tabular Attention \u2192"},{"location":"advanced/numerical-embeddings.html#model-architecture","title":"\ud83d\udcca Model Architecture","text":"<p>Advanced numerical embeddings transform your numerical features into rich representations:</p> <p></p> <p>Global numerical embeddings allow coordinated embeddings across all features:</p> <p></p> <p>These diagrams illustrate how KDP transforms numerical features into rich embedding spaces, capturing complex patterns and non-linear relationships.</p>"},{"location":"advanced/numerical-embeddings.html#how-to-enable","title":"\ud83d\udca1 How to Enable","text":""},{"location":"advanced/numerical-embeddings.html#dependencies","title":"\ud83e\udde9 Dependencies","text":"Core Dependencies <ul> <li>\ud83d\udc0d Python 3.9+</li> <li>\ud83d\udd04 TensorFlow 2.18.0+</li> <li>\ud83d\udd22 NumPy 1.22.0+</li> <li>\ud83d\udcca Pandas 2.2.0+</li> <li>\ud83d\udcdd loguru 0.7.2+</li> </ul> Optional Dependencies Package Purpose Install Command scipy \ud83e\uddea Scientific computing and statistical functions <code>pip install \"kdp[dev]\"</code> ipython \ud83d\udd0d Interactive Python shell and notebook support <code>pip install \"kdp[dev]\"</code> pytest \u2705 Testing framework and utilities <code>pip install \"kdp[dev]\"</code> pydot \ud83d\udcca Graph visualization for model architecture <code>pip install \"kdp[dev]\"</code> Development Tools \ud83d\udee0\ufe0f All development dependencies <code>pip install \"kdp[dev]\"</code> Documentation Tools \ud83d\udcda Documentation generation tools <code>pip install \"kdp[doc]\"</code>"},{"location":"advanced/tabular-attention.html","title":"\ud83d\udc41\ufe0f Tabular Attention","text":"Tabular Attention <p>Discover hidden relationships in your data with sophisticated attention mechanisms</p>"},{"location":"advanced/tabular-attention.html#overview","title":"\ud83d\udccb Overview","text":"<p>Tabular Attention is a powerful feature in KDP that enables your models to automatically discover complex interactions between features in tabular data. Based on attention mechanisms from transformers, it helps your models focus on the most important feature relationships without explicit feature engineering.</p> \ud83d\udd0d Automatic Interaction Discovery <p>Finds complex feature relationships without manual engineering</p> \ud83c\udfaf Context-Aware Processing <p>Each feature is processed in the context of other features</p> \ud83d\udcc8 Improved Performance <p>Better predictions through enhanced feature understanding</p> \ud83d\udd04 Flexible Integration <p>Works seamlessly with other KDP processing techniques</p> \ud83d\udcca Hierarchical Learning <p>Captures both low-level and high-level patterns</p>"},{"location":"advanced/tabular-attention.html#getting-started","title":"\ud83d\ude80 Getting Started","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Define features\nfeatures_specs = {\n    \"age\": FeatureType.FLOAT_NORMALIZED,\n    \"income\": FeatureType.FLOAT_RESCALED,\n    \"occupation\": FeatureType.STRING_CATEGORICAL,\n    \"education\": FeatureType.INTEGER_CATEGORICAL\n}\n\n# Initialize model with standard tabular attention\npreprocessor = PreprocessingModel(\n    path_data=\"data/my_data.csv\",\n    features_specs=features_specs,\n    tabular_attention=True,              # Enable tabular attention\n    tabular_attention_heads=4,           # Number of attention heads\n    tabular_attention_dim=64,            # Attention dimension\n    tabular_attention_dropout=0.1        # Dropout rate\n)\n</code></pre>"},{"location":"advanced/tabular-attention.html#how-it-works","title":"\ud83e\udde0 How It Works","text":"<p>KDP's tabular attention mechanism transforms features through a multi-head attention mechanism, allowing the model to learn complex patterns across features.</p>"},{"location":"advanced/tabular-attention.html#standard-tabular-attention","title":"Standard Tabular AttentionInter-Feature AttentionInter-Sample AttentionFeed-Forward Networks","text":"1 <p>Features attend to each other within each sample, capturing dependencies between different features.</p> 2 <p>Samples attend to each other for each feature, capturing patterns across different samples.</p> 3 <p>Process attended features further with non-linear transformations.</p>"},{"location":"advanced/tabular-attention.html#multi-resolution-tabular-attention","title":"Multi-Resolution Tabular AttentionSpecialized ProcessingCross-AttentionType-Specific Projections","text":"1 <p>Numerical and categorical features processed through type-specific attention mechanisms.</p> 2 <p>Enables features to attend across different types, capturing complex interactions.</p> 3 <p>Each feature type gets custom embedding dimensions for optimal representation.</p>"},{"location":"advanced/tabular-attention.html#model-architecture","title":"\ud83d\udcca Model Architecture","text":"<p>KDP's tabular attention mechanism:</p> <p></p> <p>The diagram shows how tabular attention transforms features through a multi-head attention mechanism, allowing the model to learn complex patterns across features.</p>"},{"location":"advanced/tabular-attention.html#how-to-enable","title":"\ud83d\udca1 How to Enable","text":""},{"location":"advanced/tabular-attention.html#configuration-options","title":"\u2699\ufe0f Configuration Options","text":""},{"location":"advanced/tabular-attention.html#general-options","title":"General Options","text":"Parameter Type Default Description <code>tabular_attention</code> bool False Enable/disable attention mechanisms <code>tabular_attention_heads</code> int 4 Number of attention heads <code>tabular_attention_dim</code> int 64 Dimension of the attention model <code>tabular_attention_dropout</code> float 0.1 Dropout rate for regularization"},{"location":"advanced/tabular-attention.html#multi-resolution-options","title":"Multi-Resolution Options","text":"Parameter Type Default Description <code>tabular_attention_embedding_dim</code> int 32 Dimension for categorical embeddings <code>tabular_attention_placement</code> str \"ALL_FEATURES\" Where to apply attention"},{"location":"advanced/tabular-attention.html#placement-options","title":"Placement Options","text":"Option Description Best For <code>ALL_FEATURES</code> Apply to all features uniformly General purpose, balanced datasets <code>NUMERIC</code> Only numerical features Datasets dominated by numerical features <code>CATEGORICAL</code> Only categorical features Datasets with important categorical relationships <code>MULTI_RESOLUTION</code> Type-specific attention Mixed data types with different importance"},{"location":"advanced/tabular-attention.html#best-use-cases","title":"\ud83c\udfaf Best Use Cases","text":""},{"location":"advanced/tabular-attention.html#when-to-use-standard-tabular-attention","title":"When to Use Standard Tabular Attention","text":"<ul> <li>When your features are mostly of the same type</li> <li>When you have a balanced mix of numerical and categorical features</li> <li>When feature interactions are likely uniform across feature types</li> <li>When computational efficiency is a priority</li> </ul>"},{"location":"advanced/tabular-attention.html#when-to-use-multi-resolution-attention","title":"When to Use Multi-Resolution Attention","text":"<ul> <li>When you have distinctly different numerical and categorical features</li> <li>When categorical features need special handling (high cardinality)</li> <li>When feature interactions between types are expected to be important</li> <li>When certain feature types dominate your dataset</li> </ul>"},{"location":"advanced/tabular-attention.html#examples","title":"\ud83d\udd0d Examples","text":""},{"location":"advanced/tabular-attention.html#customer-analytics-with-standard-attention","title":"Customer Analytics with Standard Attention","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\nfrom kdp.enums import TabularAttentionPlacementOptions\n\nfeatures_specs = {\n    \"customer_age\": FeatureType.FLOAT_NORMALIZED,\n    \"account_age\": FeatureType.FLOAT_NORMALIZED,\n    \"avg_purchase\": FeatureType.FLOAT_RESCALED,\n    \"total_orders\": FeatureType.FLOAT_RESCALED,\n    \"customer_type\": FeatureType.STRING_CATEGORICAL,\n    \"region\": FeatureType.STRING_CATEGORICAL\n}\n\npreprocessor = PreprocessingModel(\n    path_data=\"data/customer_data.csv\",\n    features_specs=features_specs,\n    tabular_attention=True,\n    tabular_attention_heads=4,\n    tabular_attention_dim=64,\n    tabular_attention_dropout=0.1,\n    tabular_attention_placement=TabularAttentionPlacementOptions.ALL_FEATURES.value\n)\n</code></pre>"},{"location":"advanced/tabular-attention.html#product-recommendations-with-multi-resolution-attention","title":"Product Recommendations with Multi-Resolution Attention","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\nfrom kdp.enums import TabularAttentionPlacementOptions\n\nfeatures_specs = {\n    # Numerical features\n    \"user_age\": FeatureType.FLOAT_NORMALIZED,\n    \"days_since_last_purchase\": FeatureType.FLOAT_RESCALED,\n    \"avg_session_duration\": FeatureType.FLOAT_NORMALIZED,\n    \"total_spend\": FeatureType.FLOAT_RESCALED,\n    \"items_viewed\": FeatureType.FLOAT_RESCALED,\n\n    # Categorical features\n    \"gender\": FeatureType.STRING_CATEGORICAL,\n    \"product_category\": FeatureType.STRING_CATEGORICAL,\n    \"device_type\": FeatureType.STRING_CATEGORICAL,\n    \"subscription_tier\": FeatureType.INTEGER_CATEGORICAL,\n    \"day_of_week\": FeatureType.INTEGER_CATEGORICAL\n}\n\npreprocessor = PreprocessingModel(\n    path_data=\"data/recommendation_data.csv\",\n    features_specs=features_specs,\n    tabular_attention=True,\n    tabular_attention_heads=8,              # More heads for complex interactions\n    tabular_attention_dim=128,              # Larger dimension for rich representations\n    tabular_attention_dropout=0.15,         # Slightly higher dropout for regularization\n    tabular_attention_embedding_dim=64,     # Larger embedding for categorical features\n    tabular_attention_placement=TabularAttentionPlacementOptions.MULTI_RESOLUTION.value\n)\n</code></pre>"},{"location":"advanced/tabular-attention.html#performance-considerations","title":"\ud83d\udcca Performance Considerations","text":""},{"location":"advanced/tabular-attention.html#memory-usage","title":"Memory Usage","text":"<ul> <li>Standard Attention: O(n\u00b2) memory complexity for n features</li> <li>Multi-Resolution: O(n_num\u00b2 + n_cat\u00b2) memory complexity</li> <li>For large feature sets, multi-resolution is more efficient</li> </ul>"},{"location":"advanced/tabular-attention.html#computational-cost","title":"Computational Cost","text":"<ul> <li>Attention mechanisms introduce additional training time</li> <li>Multi-head attention scales linearly with number of heads</li> <li>Multi-resolution can be faster when categorical features dominate</li> </ul>"},{"location":"advanced/tabular-attention.html#guidelines","title":"Guidelines:","text":"Dataset Size Attention Type Recommended Heads Dimension Small (&lt;10K) Standard 2-4 32-64 Medium Standard/Multi-Resolution 4-8 64-128 Large (&gt;100K) Multi-Resolution 8-16 128-256"},{"location":"advanced/tabular-attention.html#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"Head Count Selection <p>Start with 4 heads for most problems, increase for complex feature interactions, but beware of overfitting with too many heads.</p> Dimension Tuning <p>Choose dimensions divisible by number of heads, larger for complex patterns, but balance with dataset size to avoid overfitting.</p> Placement Strategy <p>Use ALL_FEATURES for initial experimentation, MULTI_RESOLUTION for mixed data types, and NUMERIC/CATEGORICAL for targeted focus.</p>"},{"location":"advanced/tabular-attention.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"\ud83d\udd04 Distribution-Aware Encoding \ud83e\uddee Advanced Numerical Embeddings \u26a1 Transformer Blocks \ud83c\udfaf Feature Selection"},{"location":"advanced/transformer-blocks.html","title":"\u26a1 Transformer Blocks","text":"Transformer Blocks <p>Powerful self-attention mechanisms for tabular data</p>"},{"location":"advanced/transformer-blocks.html#overview","title":"\ud83d\udccb Overview","text":"<p>Transformer Blocks in KDP bring the power of self-attention mechanisms to tabular data processing. These blocks enable your models to capture complex feature interactions and dependencies through sophisticated attention mechanisms, leading to better model performance on structured data.</p> \ud83e\udde0 Self-Attention <p>Capture complex feature interactions</p> \ud83d\udd04 Multi-Head Processing <p>Learn diverse feature relationships</p> \u26a1 Efficient Computation <p>Optimized for tabular data</p> \ud83c\udfaf Feature Importance <p>Learn which features matter most</p>"},{"location":"advanced/transformer-blocks.html#getting-started","title":"\ud83d\ude80 Getting Started","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Define features\nfeatures_specs = {\n    \"age\": FeatureType.FLOAT_NORMALIZED,\n    \"income\": FeatureType.FLOAT_RESCALED,\n    \"occupation\": FeatureType.STRING_CATEGORICAL,\n    \"education\": FeatureType.INTEGER_CATEGORICAL\n}\n\n# Initialize model with transformer blocks\npreprocessor = PreprocessingModel(\n    path_data=\"data/my_data.csv\",\n    features_specs=features_specs,\n    use_transformer_blocks=True,         # Enable transformer blocks\n    transformer_num_blocks=3,            # Number of transformer blocks\n    transformer_num_heads=4,             # Number of attention heads\n    transformer_dim=64                   # Hidden dimension\n)\n</code></pre>"},{"location":"advanced/transformer-blocks.html#how-it-works","title":"\ud83e\udde0 How It Works","text":"<p>KDP's transformer blocks process tabular data through multiple layers of self-attention and feed-forward networks, enabling the model to learn complex feature interactions and dependencies.</p>"},{"location":"advanced/transformer-blocks.html#configuration-options","title":"\u2699\ufe0f Configuration Options","text":"Parameter Type Default Description <code>use_transformer_blocks</code> bool False Enable transformer blocks <code>transformer_num_blocks</code> int 3 Number of transformer blocks <code>transformer_num_heads</code> int 4 Number of attention heads <code>transformer_dim</code> int 64 Hidden dimension <code>transformer_dropout</code> float 0.1 Dropout rate"},{"location":"advanced/transformer-blocks.html#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"Block Configuration <p>Start with 2-3 blocks and increase based on feature complexity. More blocks can capture deeper interactions but may lead to overfitting.</p> Head Selection <p>Use 4-8 heads for most tasks. More heads can capture diverse relationships but increase computational cost.</p> Dimension Tuning <p>Choose dimensions divisible by number of heads. Larger dimensions capture more complex patterns but require more computation.</p>"},{"location":"advanced/transformer-blocks.html#examples","title":"\ud83d\udd0d Examples","text":"Customer Analytics <pre><code>features_specs = {\n    \"age\": FeatureType.FLOAT_NORMALIZED,\n    \"income\": FeatureType.FLOAT_RESCALED,\n    \"tenure\": FeatureType.FLOAT_NORMALIZED,\n    \"purchases\": FeatureType.FLOAT_RESCALED,\n    \"customer_type\": FeatureType.STRING_CATEGORICAL,\n    \"region\": FeatureType.STRING_CATEGORICAL\n}\n\npreprocessor = PreprocessingModel(\n    path_data=\"data/customer_data.csv\",\n    features_specs=features_specs,\n    use_transformer_blocks=True,\n    transformer_num_blocks=4,            # More blocks for complex customer patterns\n    transformer_num_heads=8,             # More heads for diverse relationships\n    transformer_dim=128,                 # Larger dimension for rich representations\n    transformer_dropout=0.2              # Higher dropout for regularization\n)\n</code></pre> Product Recommendations <pre><code>features_specs = {\n    \"user_id\": FeatureType.INTEGER_CATEGORICAL,\n    \"item_id\": FeatureType.INTEGER_CATEGORICAL,\n    \"category\": FeatureType.STRING_CATEGORICAL,\n    \"price\": FeatureType.FLOAT_NORMALIZED,\n    \"rating\": FeatureType.FLOAT_NORMALIZED,\n    \"timestamp\": FeatureType.DATE\n}\n\npreprocessor = PreprocessingModel(\n    path_data=\"data/recommendation_data.csv\",\n    features_specs=features_specs,\n    use_transformer_blocks=True,\n    transformer_num_blocks=3,            # Standard configuration\n    transformer_num_heads=4,             # Balanced number of heads\n    transformer_dim=64,                  # Moderate dimension\n    transformer_dropout=0.1              # Standard dropout\n)\n</code></pre>"},{"location":"advanced/transformer-blocks.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"\ud83d\udc41\ufe0f Tabular Attention \ud83e\udde9 Feature-wise MoE \ud83e\uddee Advanced Numerical Embeddings \ud83d\udd17 Cross Features"},{"location":"api/layers-factory.html","title":"\ud83c\udfed Preprocessing Layers Factory","text":"<p>The <code>PreprocessorLayerFactory</code> class provides a convenient way to create and manage preprocessing layers for your machine learning models. It supports both standard Keras preprocessing layers and custom layers defined within the KDP framework.</p>"},{"location":"api/layers-factory.html#using-keras-preprocessing-layers","title":"\ud83c\udfa1 Using Keras Preprocessing Layers","text":"<p>All preprocessing layers available in Keras can be used within the <code>PreprocessorLayerFactory</code>. You can access these layers by their class names. Here's an example of how to use a Keras preprocessing layer:</p> <p><pre><code>normalization_layer = PreprocessorLayerFactory.create_layer(\n    \"Normalization\",\n    axis=-1,\n    mean=None,\n    variance=None\n)\n</code></pre> Available layers:</p> <ul> <li> Normalization - Standardizes numerical features</li> <li> Discretization - Bins continuous features into discrete intervals</li> <li> CategoryEncoding - Converts categorical data into numeric representations</li> <li> Hashing - Performs feature hashing for categorical variables</li> <li> HashedCrossing - Creates feature crosses using hashing</li> <li> StringLookup - Converts string inputs to integer indices</li> <li> IntegerLookup - Maps integer inputs to indexed array positions</li> <li> TextVectorization - Processes raw text into encoded representations</li> <li> ... and more</li> </ul>"},{"location":"api/layers-factory.html#custom-kdp-preprocessing-layers","title":"\ud83c\udfd7\ufe0f Custom KDP Preprocessing Layers","text":"<p>In addition to Keras layers, the <code>PreprocessorLayerFactory</code> includes several custom layers specific to the KDP framework. Here's a list of available custom layers:</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.cast_to_float32_layer","title":"<code>cast_to_float32_layer(name='cast_to_float32', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a CastToFloat32Layer layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'cast_to_float32'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the CastToFloat32Layer layer.</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.create_layer","title":"<code>create_layer(layer_class, name=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a layer using the layer class name, automatically filtering kwargs based on the layer class.</p> <p>Parameters:</p> Name Type Description Default <code>layer_class</code> <code>str | Class Object</code> <p>The name of the layer class to be created (e.g., 'Normalization', 'Rescaling') or the class object itself.</p> required <code>name</code> <code>str</code> <p>The name of the layer. Optional.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the specified layer class.</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.date_encoding_layer","title":"<code>date_encoding_layer(name='date_encoding_layer', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a DateEncodingLayer layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'date_encoding_layer'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the DateEncodingLayer layer.</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.date_parsing_layer","title":"<code>date_parsing_layer(name='date_parsing_layer', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a DateParsingLayer layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'date_parsing_layer'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the DateParsingLayer layer.</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.date_season_layer","title":"<code>date_season_layer(name='date_season_layer', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a SeasonLayer layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'date_season_layer'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the SeasonLayer layer.</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.distribution_aware_encoder","title":"<code>distribution_aware_encoder(name='distribution_aware', num_bins=1000, epsilon=1e-06, detect_periodicity=True, handle_sparsity=True, adaptive_binning=True, mixture_components=3, prefered_distribution=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a DistributionAwareEncoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'distribution_aware'</code> <code>num_bins</code> <code>int</code> <p>Number of bins for quantile encoding</p> <code>1000</code> <code>epsilon</code> <code>float</code> <p>Small value for numerical stability</p> <code>1e-06</code> <code>detect_periodicity</code> <code>bool</code> <p>Whether to detect and handle periodic patterns</p> <code>True</code> <code>handle_sparsity</code> <code>bool</code> <p>Whether to handle sparse data specially</p> <code>True</code> <code>adaptive_binning</code> <code>bool</code> <p>Whether to use adaptive binning</p> <code>True</code> <code>mixture_components</code> <code>int</code> <p>Number of components for mixture modeling</p> <code>3</code> <code>specified_distribution</code> <code>DistributionType</code> <p>Optional specific distribution type to use</p> required <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>DistributionAwareEncoder layer</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.distribution_transform_layer","title":"<code>distribution_transform_layer(name='distribution_transform', transform_type='none', lambda_param=0.0, epsilon=1e-10, min_value=0.0, max_value=1.0, clip_values=True, auto_candidates=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a DistributionTransformLayer layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'distribution_transform'</code> <code>transform_type</code> <code>str</code> <p>Type of transformation to apply</p> <code>'none'</code> <code>lambda_param</code> <code>float</code> <p>Parameter for parameterized transformations</p> <code>0.0</code> <code>epsilon</code> <code>float</code> <p>Small value for numerical stability</p> <code>1e-10</code> <code>min_value</code> <code>float</code> <p>Minimum value for min-max scaling</p> <code>0.0</code> <code>max_value</code> <code>float</code> <p>Maximum value for min-max scaling</p> <code>1.0</code> <code>clip_values</code> <code>bool</code> <p>Whether to clip values to the specified range</p> <code>True</code> <code>auto_candidates</code> <code>list[str]</code> <p>List of transformations to consider in auto mode</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>DistributionTransformLayer layer</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.gated_linear_unit_layer","title":"<code>gated_linear_unit_layer(units, name='gated_linear_unit', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a GatedLinearUnit layer.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>Dimensionality of the output space</p> required <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'gated_linear_unit'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to pass to the layer</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GatedLinearUnit</code> <code>tf.keras.layers.Layer</code> <p>A GatedLinearUnit layer instance</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.gated_residual_network_layer","title":"<code>gated_residual_network_layer(units, dropout_rate=0.2, name='gated_residual_network', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a GatedResidualNetwork layer.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>Dimensionality of the output space</p> required <code>dropout_rate</code> <code>float</code> <p>Fraction of the input units to drop</p> <code>0.2</code> <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'gated_residual_network'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to pass to the layer</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GatedResidualNetwork</code> <code>tf.keras.layers.Layer</code> <p>A GatedResidualNetwork layer instance</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.global_numerical_embedding_layer","title":"<code>global_numerical_embedding_layer(global_embedding_dim=8, global_mlp_hidden_units=16, global_num_bins=10, global_init_min=-3.0, global_init_max=3.0, global_dropout_rate=0.1, global_use_batch_norm=True, global_pooling='average', name='global_numerical_embedding', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a GlobalNumericalEmbedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>global_embedding_dim</code> <code>int</code> <p>Dimension of the final global embedding</p> <code>8</code> <code>global_mlp_hidden_units</code> <code>int</code> <p>Number of hidden units in the global MLP</p> <code>16</code> <code>global_num_bins</code> <code>int</code> <p>Number of bins for discretization</p> <code>10</code> <code>global_init_min</code> <code>float</code> <p>Minimum value for initialization</p> <code>-3.0</code> <code>global_init_max</code> <code>float</code> <p>Maximum value for initialization</p> <code>3.0</code> <code>global_dropout_rate</code> <code>float</code> <p>Dropout rate for regularization</p> <code>0.1</code> <code>global_use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization</p> <code>True</code> <code>global_pooling</code> <code>str</code> <p>Pooling method to use (\"average\" or \"max\")</p> <code>'average'</code> <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'global_numerical_embedding'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to pass to the layer</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GlobalNumericalEmbedding</code> <code>tf.keras.layers.Layer</code> <p>A GlobalNumericalEmbedding layer instance</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.multi_resolution_attention_layer","title":"<code>multi_resolution_attention_layer(num_heads, d_model, embedding_dim=32, name='multi_resolution_attention', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a MultiResolutionTabularAttention layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>d_model</code> <code>int</code> <p>Dimensionality of the attention model</p> required <code>embedding_dim</code> <code>int</code> <p>Dimension for categorical embeddings</p> <code>32</code> <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'multi_resolution_attention'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to pass to the layer</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>MultiResolutionTabularAttention</code> <code>tf.keras.layers.Layer</code> <p>A MultiResolutionTabularAttention layer instance</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.numerical_embedding_layer","title":"<code>numerical_embedding_layer(embedding_dim=8, mlp_hidden_units=16, num_bins=10, init_min=-3.0, init_max=3.0, dropout_rate=0.1, use_batch_norm=True, name='numerical_embedding', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a NumericalEmbedding layer.</p> <p>Parameters:</p> Name Type Description Default <code>embedding_dim</code> <code>int</code> <p>Dimension of the output embedding</p> <code>8</code> <code>mlp_hidden_units</code> <code>int</code> <p>Number of hidden units in the MLP</p> <code>16</code> <code>num_bins</code> <code>int</code> <p>Number of bins for discretization</p> <code>10</code> <code>init_min</code> <code>float</code> <p>Minimum value for initialization</p> <code>-3.0</code> <code>init_max</code> <code>float</code> <p>Maximum value for initialization</p> <code>3.0</code> <code>dropout_rate</code> <code>float</code> <p>Dropout rate for regularization</p> <code>0.1</code> <code>use_batch_norm</code> <code>bool</code> <p>Whether to use batch normalization</p> <code>True</code> <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'numerical_embedding'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to pass to the layer</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>NumericalEmbedding</code> <code>tf.keras.layers.Layer</code> <p>A NumericalEmbedding layer instance</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.tabular_attention_layer","title":"<code>tabular_attention_layer(num_heads, d_model, name='tabular_attention', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a TabularAttention layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>d_model</code> <code>int</code> <p>Dimensionality of the attention model</p> required <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'tabular_attention'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to pass to the layer</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>TabularAttention</code> <code>tf.keras.layers.Layer</code> <p>A TabularAttention layer instance</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.text_preprocessing_layer","title":"<code>text_preprocessing_layer(name='text_preprocessing', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a TextPreprocessingLayer layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'text_preprocessing'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the TextPreprocessingLayer layer.</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.transformer_block_layer","title":"<code>transformer_block_layer(name='transformer', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a TransformerBlock layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'transformer'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the TransformerBlock layer.</p>"},{"location":"api/layers-factory.html#kdp.layers_factory.PreprocessorLayerFactory.variable_selection_layer","title":"<code>variable_selection_layer(nr_features=None, units=16, dropout_rate=0.2, name='variable_selection', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a VariableSelection layer.</p> <p>Parameters:</p> Name Type Description Default <code>nr_features</code> <code>int</code> <p>Number of input features</p> <code>None</code> <code>units</code> <code>int</code> <p>Dimensionality of the output space</p> <code>16</code> <code>dropout_rate</code> <code>float</code> <p>Fraction of the input units to drop</p> <code>0.2</code> <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'variable_selection'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to pass to the layer</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>VariableSelection</code> <code>tf.keras.layers.Layer</code> <p>A VariableSelection layer instance</p>"},{"location":"contributing/overview.html","title":"\ud83d\udcbb Contributing: Join the Preprocessing Revolution! \ud83d\udee0\ufe0f","text":"Help us build the future of data preprocessing! <p>We're excited to welcome new contributors to KDP. This guide will help you get started on your contribution journey.</p>"},{"location":"contributing/overview.html#contribution-process-overview","title":"\ud83c\udfc1 Contribution Process Overview","text":"1 Getting Started <ul> <li>Fork the Repository: Visit our GitHub page, fork the repository, and clone it to your local machine.</li> <li>Set Up Your Environment: Make sure you have TensorFlow, Loguru, and all necessary dependencies installed.</li> <li>Install Pre-commit Hook: This ensures code quality before commits.</li> </ul> <pre><code># Install pre-commit hook\nconda install -c conda-forge pre-commit\n\n# Go to the root folder of the repository and run\npre-commit install\n</code></pre> 2 Making Changes <ul> <li>Create a Branch: Always work in a dedicated branch for your changes.</li> <li>Follow Coding Standards: Maintain the project's coding style and conventions.</li> <li>Write Tests: All new features must include tests.</li> <li>Use Standardized Commit Messages: Follow the format below.</li> </ul> <pre><code>{LABEL}(KDP): {message}\n\n# Examples:\nfeat(KDP): Add distribution-aware encoding\nfix(KDP): Resolve memory leak in feature selection\n</code></pre> 3 Submitting Your Work <ul> <li>Create Small MRs: Keep merge requests under 400 lines for easier review.</li> <li>Request Code Review: All code must be reviewed before merging.</li> <li>Address Feedback: Resolve all comments and ensure CI checks pass.</li> <li>Tests Must Pass: NO TESTS = NO MERGE \ud83d\udea8</li> </ul>"},{"location":"contributing/overview.html#feature-requests-issues","title":"\ud83d\udca1 Feature Requests &amp; Issues","text":"Have ideas or found a bug? <p>We welcome your input! Please use our GitHub issues page to:</p> <ul> <li>Report bugs or unexpected behavior</li> <li>Suggest new features or improvements</li> <li>Discuss implementation approaches</li> </ul> Open an Issue"},{"location":"contributing/overview.html#commit-message-guidelines","title":"\ud83d\udcdd Commit Message Guidelines","text":"\ud83c\udff7\ufe0f Label Types Label Usage Version Impact <code>break</code> Changes that break backward compatibility major <code>feat</code> New backward-compatible features minor <code>fix</code> Bug fixes patch <code>docs</code> Documentation changes patch <code>style</code> Code style changes (formatting, etc.) patch <code>refactor</code> Code changes that neither fix bugs nor add features patch <code>perf</code> Performance improvements patch <code>test</code> Adding or updating tests minor <code>build</code> Build system or dependency changes patch <code>ci</code> CI configuration changes minor"},{"location":"contributing/overview.html#merge-request-process","title":"\ud83d\udd04 Merge Request Process","text":"Creating Effective Merge Requests <p>Merge requests are the heart of our collaborative development process:</p> <ul> <li>Create your MR early - even as a work in progress</li> <li>Use the same naming convention as commits: <code>{LABEL}(KDP): {message}</code></li> <li>Break large features into smaller, focused MRs</li> <li>Include relevant tests for your changes</li> <li>Ensure all CI checks pass before requesting review</li> <li>Address all feedback before merging</li> </ul> \ud83d\udca1 <p>Merge requests generate our changelog automatically, so clear and descriptive messages help everyone understand your contributions!</p>"},{"location":"contributing/development/auto-documentation.html","title":"\ud83d\udd04 Automatic Documentation Generation","text":"<p>KDP includes tools to automatically generate documentation from code docstrings and visualize model architectures for different configurations. This ensures that the documentation remains up-to-date with the codebase.</p>"},{"location":"contributing/development/auto-documentation.html#documentation-tools","title":"\ud83d\udee0\ufe0f Documentation Tools","text":"<p>The following tools are included in the <code>scripts/</code> directory:</p> <ol> <li><code>generate_docstring_docs.py</code>: Extracts docstrings from classes and functions in the codebase and converts them to Markdown documentation.</li> <li><code>generate_model_diagrams.py</code>: Creates visualizations of model diagrams for different feature types and preprocessing configurations.</li> </ol>"},{"location":"contributing/development/auto-documentation.html#generating-documentation","title":"\ud83d\ude80 Generating Documentation","text":"<p>You can generate the documentation content using the Makefile target:</p> <pre><code>make generate_doc_content\n</code></pre> <p>This will: - Extract API documentation from docstrings and save it to <code>docs/generated/api/</code> - Generate model diagrams and save them to <code>docs/features/imgs/models/</code> - Create an API index at <code>docs/generated/api_index.md</code></p>"},{"location":"contributing/development/auto-documentation.html#model-diagram-types","title":"\ud83c\udfd7\ufe0f Model Diagram Types","text":"<p>The script generates visualizations for a variety of model configurations, including:</p> <ul> <li>Basic feature types (numerical, categorical, text, date, passthrough)</li> <li>Feature combinations and cross-features</li> <li>Different output modes (CONCAT and DICT)</li> <li>Advanced features like tabular attention and transformer blocks</li> <li>Distribution-aware encoding and numerical embeddings</li> </ul> <p>Each diagram shows: - The full TensorFlow model architecture - Input and output shapes - Layer connections and data flow</p>"},{"location":"contributing/development/auto-documentation.html#api-documentation-from-docstrings","title":"\ud83d\udcda API Documentation from Docstrings","text":"<p>The docstring extraction process: 1. Scans key modules in the codebase 2. Extracts docstrings from classes and their methods 3. Converts them to Markdown format 4. Organizes them into a structured API reference</p>"},{"location":"contributing/development/auto-documentation.html#integration-with-mkdocs","title":"\ud83d\udd04 Integration with MkDocs","text":"<p>The generated documentation is automatically included when building the MkDocs site:</p> <pre><code>make serve_doc      # Test documentation locally\n</code></pre> <p>or</p> <pre><code>make deploy_doc     # Deploy to GitHub Pages\n</code></pre>"},{"location":"contributing/development/auto-documentation.html#documenting-your-code","title":"\ud83d\udcdd Documenting Your Code","text":"<p>To ensure your code is properly included in the automatic documentation:</p> <ol> <li>Use Google-style docstrings for all classes and methods:</li> </ol> <pre><code>def my_function(param1, param2):\n    \"\"\"\n    One-line description of function.\n\n    More detailed description if needed.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2\n\n    Returns:\n        Description of the return value\n\n    Examples:\n        ```python\n        result = my_function(\"example\", 123)\n        ```\n    \"\"\"\n    # Function implementation\n</code></pre> <ol> <li> <p>Document parameters and return values to provide clear usage instructions.</p> </li> <li> <p>Include examples to demonstrate usage where appropriate.</p> </li> </ol>"},{"location":"contributing/development/auto-documentation.html#dynamic-preprocessing-pipeline","title":"\ud83d\udd04 Dynamic Preprocessing Pipeline","text":"<p>The <code>DynamicPreprocessingPipeline</code> class provides a flexible way to build preprocessing pipelines with optimized execution flow:</p> <pre><code>class DynamicPreprocessingPipeline:\n    \"\"\"\n    Dynamically initializes and manages a sequence of Keras preprocessing layers, with selective retention of outputs\n    based on dependencies among layers, and supports streaming data through the pipeline.\n    \"\"\"\n</code></pre> <p>This class analyzes dependencies between layers and ensures that each layer receives the outputs it needs from previous layers.</p>"},{"location":"contributing/development/auto-documentation.html#cleaning-up-old-diagrams","title":"\ud83e\uddf9 Cleaning Up Old Diagrams","text":"<p>When updating the model diagram generation process, you may need to clean up old diagrams or directories. Use the Makefile target:</p> <pre><code>make clean_old_diagrams\n</code></pre> <p>This will: - Remove obsolete diagram directories - Clean up unused diagram files</p> <p>This target is also automatically included when running <code>make clean</code>.</p>"},{"location":"contributing/development/images-organization.html","title":"\ud83d\uddbc\ufe0f Documentation Image Organization","text":"<p>A guide to working with images in the KDP documentation</p>"},{"location":"contributing/development/images-organization.html#overview","title":"Overview","text":"<p>KDP documentation uses a section-specific image organization pattern where each documentation section has its own <code>imgs/</code> directory containing relevant images. This approach provides better organization and makes it clear which images are used in each section.</p>"},{"location":"contributing/development/images-organization.html#directory-structure","title":"\ud83d\udcc1 Directory Structure","text":"<p>Each major documentation section has its own <code>imgs/</code> directory:</p> <pre><code>docs/\n\u251c\u2500\u2500 features/\n\u2502   \u251c\u2500\u2500 imgs/             # Feature-specific images\n\u2502   \u2502   \u2514\u2500\u2500 models/       # Generated model diagrams\n\u2502   \u251c\u2500\u2500 overview.md\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 advanced/\n\u2502   \u251c\u2500\u2500 imgs/             # Advanced feature images\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 getting-started/\n\u2502   \u251c\u2500\u2500 imgs/             # Getting started images\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"contributing/development/images-organization.html#image-organization-script","title":"\ud83d\udd04 Image Organization Script","text":"<p>The <code>scripts/organize_docs_images.sh</code> script handles image organization:</p> <ol> <li>It categorizes images into common, feature-specific, and advanced types</li> <li>It distributes these images to appropriate section folders</li> <li>It ensures generated model diagrams are correctly placed in the <code>features/imgs/models/</code> directory</li> </ol> <p>This script runs automatically during documentation generation:</p> <pre><code>make generate_doc_content\n</code></pre>"},{"location":"contributing/development/images-organization.html#image-categories","title":"\ud83c\udfa8 Image Categories","text":"<p>Images are organized into three main categories:</p> <ol> <li>Common Images (distributed to all sections):</li> <li><code>Model_Architecture.png</code> - Main architecture diagram</li> <li><code>features_stats.png</code> - Feature statistics visualization</li> <li><code>time_vs_nr_data.png</code> - Performance comparison chart</li> <li><code>time_vs_nr_features.png</code> - Feature scaling chart</li> <li> <p><code>kdp_logo.png</code> - Project logo</p> </li> <li> <p>Feature-Specific Images (in <code>features/imgs/</code> and <code>getting-started/imgs/</code>):</p> </li> <li><code>cat_feature_pipeline.png</code> - Categorical feature processing pipeline</li> <li><code>numerical_example_model.png</code> - Example numerical feature model</li> <li> <p>And others related to core feature types</p> </li> <li> <p>Advanced Feature Images (in <code>advanced/imgs/</code>, <code>optimization/imgs/</code>, and <code>examples/imgs/</code>):</p> </li> <li><code>TransformerBlockAllFeatures.png</code> - Transformer block architecture</li> <li><code>numerical_example_model_with_distribution_aware.png</code> - Distribution-aware encoding</li> <li> <p>And others related to advanced features</p> </li> <li> <p>Model Diagrams (in <code>features/imgs/models/</code>):</p> </li> <li>Automatically generated diagrams showing different model architectures</li> <li>Created by the <code>generate_model_diagrams.py</code> script</li> </ol>"},{"location":"contributing/development/images-organization.html#adding-new-images","title":"\ud83d\udd8a\ufe0f Adding New Images","text":"<p>When adding new images to the documentation:</p> <ol> <li>Place the image in the appropriate section's <code>imgs/</code> directory</li> <li>Reference it in the Markdown using a relative path: <code>![Description](imgs/image_name.png)</code></li> <li>Run <code>make generate_doc_content</code> to ensure proper organization</li> </ol>"},{"location":"contributing/development/images-organization.html#best-practices","title":"\ud83d\udccb Best Practices","text":"<ol> <li>Use Descriptive Filenames:</li> <li>Choose clear, descriptive names: <code>categorical_processing_flow.png</code> not <code>img1.png</code></li> <li> <p>Use snake_case for filenames</p> </li> <li> <p>Optimize Images:</p> </li> <li>Compress images to reduce file size</li> <li>Use PNG for diagrams and screenshots</li> <li> <p>Keep dimensions reasonable (800-1200px width for most images)</p> </li> <li> <p>Include Alt Text:</p> </li> <li> <p>Always provide descriptive alt text: <code>![Distribution-aware encoding architecture](imgs/distribution_aware.png)</code></p> </li> <li> <p>Keep Images Relevant:</p> </li> <li>Only include images that add value to the documentation</li> <li>Remove unused images using <code>make clean_old_diagrams</code></li> </ol>"},{"location":"contributing/development/images-organization.html#finding-unused-images","title":"\ud83d\udd0d Finding Unused Images","text":"<p>To identify potentially unused images:</p> <pre><code>make identify_unused_diagrams\n</code></pre> <p>This will generate an <code>unused_diagrams_report.txt</code> file listing images that may not be referenced in the documentation.</p>"},{"location":"contributing/development/images-organization.html#related-topics","title":"\ud83e\udd1d Related Topics","text":"<ul> <li>Documentation Generation</li> <li>Contributing Guide</li> </ul>"},{"location":"examples/basic-examples.html","title":"Example usages","text":"<p>Let us go trough some different examples of case scenarios. To get a feeling how one could use the library to their advantage with ease and comfort.</p>"},{"location":"examples/basic-examples.html#example-1-numerical-features","title":"Example 1: Numerical features","text":"<pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom kdp.processor import PreprocessingModel, OutputModeOptions\n\n# Define features\nfeatures = {\n    # 1. Basic float feature (no preprocessing)\n    \"basic_float\": NumericalFeature(\n        name=\"basic_float\",\n        feature_type=FeatureType.FLOAT\n    ),\n\n    # 2. Basic float feature (no preprocessing)\n    \"basic_float2\": NumericalFeature(\n        name=\"basic_float2\",\n        feature_type=FeatureType.FLOAT\n    ),\n\n    # 3. Normalized float feature\n    \"normalized_float\": NumericalFeature(\n        name=\"normalized_float\",\n        feature_type=FeatureType.FLOAT_NORMALIZED\n    ),\n\n    # 4. Rescaled float feature\n    \"rescaled_float\": NumericalFeature(\n        name=\"rescaled_float\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        scale=2.0  # Optional scale parameter\n    ),\n\n    # 5. Discretized float feature\n    \"discretized_float\": NumericalFeature(\n        name=\"discretized_float\",\n        feature_type=FeatureType.FLOAT_DISCRETIZED,\n        bin_boundaries=[0.0, 1.0, 2.0]  # Required for discretization\n    ),\n\n    # 6. Custom preprocessing pipeline\n    \"custom_float\": NumericalFeature(\n        name=\"custom_float\",\n        feature_type=FeatureType.FLOAT,\n        preprocessors=[\n            tf.keras.layers.Rescaling,\n            tf.keras.layers.Normalization,\n        ],\n        # Additional kwargs for the preprocessors\n        bin_boundaries=[0.0, 1.0, 2.0],\n        mean=0.0,\n        variance=1.0,\n        scale=4.0  # Added required scale parameter for Rescaling layer\n    ),\n}\n\n# Define cross-feature between 2 arbitrary features, though tabular attention would be more useful for feature crossings\nfeature_crosses = [(\"normalized_float\", \"rescaled_float\", 10)] # 10 is the number of bins to hash into\n\n# Now we can create a preprocessing model with the features\nppr = PreprocessingModel(\n    path_data=\"sample_data.csv\",\n    features_specs=features,\n    feature_crosses=feature_crosses,\n    features_stats_path=\"features_stats.json\",\n    overwrite_stats=True,\n    output_mode=OutputModeOptions.CONCAT,\n\n    # Add feature selection to get the most important features\n    feature_selection_placement=\"numeric\", # Choose between (all_features|numeric|categorical)\n    feature_selection_units=32,\n    feature_selection_dropout=0.10,\n\n    # Add tabular attention to check for feature interactions\n    tabular_attention=True,\n    tabular_attention_placement=\"all_features\",  # Choose between (none|numeric|categorical|all_features|multi_resolution)\n    tabular_attention_heads=3,                   # Number of attention heads\n    tabular_attention_dim=32,                    # Attention dimension\n    tabular_attention_dropout=0.1,               # Attention dropout rate\n    tabular_attention_embedding_dim=16,          # Embedding dimension\n)\n\n# Build the preprocessor\nresult = ppr.build_preprocessor()\n\n# Transform data using direct model prediction\ntransformed_data = ppr.model.predict(test_batch)\n\n# Get feature importances\nfeature_importances = ppr.get_feature_importances()\n</code></pre>"},{"location":"examples/basic-examples.html#visualize-the-model-architecture","title":"Visualize the model architecture","text":"<p>You can visualize the model architecture to understand how the features are processed:</p> <pre><code>ppr.plot_model(\"numerical_example_model.png\")\n</code></pre> <p></p>"},{"location":"examples/basic-examples.html#example-2-categorical-features","title":"Example 2: Categorical features","text":"<pre><code>from kdp.features import CategoricalFeature, FeatureType, CategoryEncodingOptions\nfrom kdp.processor import PreprocessingModel, OutputModeOptions\n\n# Define features\nfeatures = {\n    # 1. Basic string categorical feature with embedding\n    \"basic_category\": CategoricalFeature(\n        name=\"basic_category\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.EMBEDDING,\n        embedding_size=8  # Custom embedding size\n    ),\n\n    # 2. Basic integer categorical feature with one-hot encoding\n    \"basic_int_category\": CategoricalFeature(\n        name=\"basic_int_category\",\n        feature_type=FeatureType.INTEGER_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.ONE_HOT_ENCODING\n    ),\n\n    # 3. High cardinality categorical feature with embedding\n    \"high_card_category\": CategoricalFeature(\n        name=\"high_card_category\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.EMBEDDING,\n        # embedding size will be automatically determined based on cardinality\n    ),\n\n    # 4. Binary categorical feature with one-hot encoding\n    \"binary_category\": CategoricalFeature(\n        name=\"binary_category\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.ONE_HOT_ENCODING\n    ),\n}\n\nfeature_crosses = [(\"basic_category\", \"basic_int_category\", 8)]  # Using existing features\n\n# Now we can create a preprocessing model with the features\nppr = PreprocessingModel(\n    path_data=\"sample_data.csv\",\n    features_specs=features,\n    feature_crosses=feature_crosses,\n    features_stats_path=\"features_stats.json\",\n    overwrite_stats=True,\n    output_mode=OutputModeOptions.CONCAT,\n\n    # Add feature selection\n    feature_selection_placement=\"categorical\",\n    feature_selection_units=32,\n    feature_selection_dropout=0.1,\n\n    # Add tabular attention\n    tabular_attention=True,\n    tabular_attention_placement=\"categorical\",  # Apply attention only to categorical features\n    tabular_attention_heads=4,\n    tabular_attention_dim=32,\n    tabular_attention_dropout=0.1,\n    tabular_attention_embedding_dim=16,\n\n    # Add transformer blocks to make it learn sophisticated feature interactions\n    transfo_nr_blocks=2,\n    transfo_nr_heads=4,\n    transfo_ff_units=32,\n    transfo_dropout_rate=0.1,\n    transfo_placement=\"categorical\",  # Apply transformer only to categorical features\n)\n\n# Build the preprocessor\nresult = ppr.build_preprocessor()\n\n# Transform data using direct model prediction\ntransformed_data = ppr.model.predict(test_batch)\n\n# Get feature importances\nfeature_importances = ppr.get_feature_importances()\n</code></pre> <p>You can visualize the model architecture:</p> <pre><code>ppr.plot_model(\"categorical_example_model.png\")\n</code></pre> <p></p>"},{"location":"examples/basic-examples.html#example-3-multi-feature-model","title":"Example 3: Multi-feature model","text":"<pre><code>from kdp.features import NumericalFeature, CategoricalFeature, TextFeature, DateFeature, FeatureType\nfrom kdp.processor import PreprocessingModel, OutputModeOptions\n\n# Define features\nfeatures = {\n    # Numerical features\n    \"price\": NumericalFeature(\n        name=\"price\",\n        feature_type=FeatureType.FLOAT_NORMALIZED\n    ),\n    \"quantity\": NumericalFeature(\n        name=\"quantity\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        scale=1.0\n    ),\n\n    # Categorical features\n    \"category\": CategoricalFeature(\n        name=\"category\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_size=32\n    ),\n    \"brand\": CategoricalFeature(\n        name=\"brand\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_size=16\n    ),\n\n    # Text features\n    \"description\": TextFeature(\n        name=\"description\",\n        feature_type=FeatureType.TEXT,\n        max_tokens=100\n    ),\n    \"title\": TextFeature(\n        name=\"title\",\n        feature_type=FeatureType.TEXT,\n        max_tokens=50, # max number of tokens to keep\n\n    ),\n\n    # Date features\n    \"sale_date\": DateFeature(\n        name=\"sale_date\",\n        feature_type=FeatureType.DATE,\n        add_season=True, # adds one-hot season indicator (summer, winter, etc) defaults to False\n    )\n}\n\n# Create preprocessor with both transformer blocks and attention\nppr = PreprocessingModel(\n    path_data=\"sample_data.csv\",\n    features_stats_path=\"features_stats.json\",\n    overwrite_stats=True,             # Force stats generation, recommended to be set to True\n    features_specs=features,\n    output_mode=OutputModeOptions.CONCAT,\n\n    # Transformer block configuration\n    transfo_placement=\"all_features\",  # Choose between (categorical|all_features)\n    transfo_nr_blocks=2,              # Number of transformer blocks\n    transfo_nr_heads=4,               # Number of attention heads in transformer\n    transfo_ff_units=64,              # Feed-forward units in transformer\n    transfo_dropout_rate=0.1,         # Dropout rate for transformer\n\n    # Tabular attention configuration\n    tabular_attention=True,\n    tabular_attention_placement=\"all_features\",  # Choose between (none|numeric|categorical|all_features| multi_resolution)\n    tabular_attention_heads=3,                   # Number of attention heads\n    tabular_attention_dim=32,                    # Attention dimension\n    tabular_attention_dropout=0.1,               # Attention dropout rate\n    tabular_attention_embedding_dim=16,          # Embedding dimension\n\n    # Feature selection configuration\n    feature_selection_placement=\"all_features\", # Choose between (all_features|numeric|categorical)\n    feature_selection_units=32,\n    feature_selection_dropout=0.15,\n)\n\n# Build the preprocessor\nresult = ppr.build_preprocessor()\n</code></pre> <p>You can visualize the model architecture:</p> <pre><code>ppr.plot_model(\"complex_model.png\")\n</code></pre> <p></p>"},{"location":"examples/basic-examples.html#example-4-numerical-features-with-distribution-aware-encoder","title":"Example 4: Numerical features with distribution aware encoder","text":"<p>Normally the distribution aware encoder works well in automatic mode, once use_distribution_aware=True is set. However we can also manually set the prefered distribution for each numerical feature if we would like to.</p> <pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom kdp.processor import PreprocessingModel, OutputModeOptions\nfrom kdp.layers.distribution_aware_encoder_layer import DistributionAwareEncoder\n\n\n# Define features\nfeatures = {\n    # 1. Default automatic distribution detection\n    \"basic_float\": NumericalFeature(\n        name=\"basic_float\",\n        feature_type=FeatureType.FLOAT,\n    ),\n\n    # 2. Manually setting a gamma distribution\n    \"rescaled_float\": NumericalFeature(\n        name=\"rescaled_float\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        scale=2.0,\n        prefered_distribution=\"gamma\"\n    ),\n    # 3. Custom preprocessing pipeline with a custom set normal distribution\n    \"custom_float\": NumericalFeature(\n        name=\"custom_float\",\n        feature_type=FeatureType.FLOAT,\n        preprocessors=[\n            tf.keras.layers.Rescaling,\n            tf.keras.layers.Normalization,\n            DistributionAwareEncoder,\n        ],\n        bin_boundaries=[0.0, 1.0, 2.0],\n        mean=0.0,\n        variance=1.0,\n        scale=4.0,\n        prefered_distribution=\"normal\"\n    ),\n}\n\n# Now we can create a preprocessing model with the features\nppr = PreprocessingModel(\n    path_data=\"sample_data.csv\",\n    features_specs=features,\n    features_stats_path=\"features_stats.json\",\n    overwrite_stats=True,\n    output_mode=OutputModeOptions.CONCAT,\n\n    # Add feature selection to get the most important features\n    feature_selection_placement=\"numeric\", # Choose between (all_features|numeric|categorical)\n\n    # Add tabular attention to check for feature interactions\n    tabular_attention=True,\n\n    # Add distribution aware encoder\n    use_distribution_aware=True\n)\n\n# Build the preprocessor\nresult = ppr.build_preprocessor()\n\n# Transform data using direct model prediction\ntransformed_data = ppr.model.predict(test_batch)\n\n# Get feature importances\nfeature_importances = ppr.get_feature_importances()\n</code></pre> <pre><code>ppr.plot_model(\"model_with_distribution_aware.png\")\n</code></pre> <p></p>"},{"location":"examples/basic-examples.html#example-5-numerical-features-with-numerical-embedding","title":"Example 5: Numerical features with numerical embedding","text":"<p>Numerical embedding is a technique that allows us to embed numerical features into a higher dimensional space. This can be useful for capturing non-linear relationships within/between numerical feature/s.</p> <pre><code>from kdp.features import NumericalFeature, FeatureType\nfrom kdp.processor import PreprocessingModel, OutputModeOptions\n\n\n# Define features\nfeatures = {\n    \"basic_float\": NumericalFeature(\n        name=\"basic_float\",\n        feature_type=FeatureType.FLOAT,\n    ),\n\n    \"rescaled_float\": NumericalFeature(\n        name=\"rescaled_float\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        scale=2.0,\n    ),\n\n    \"custom_float\": NumericalFeature(\n        name=\"custom_float\",\n        feature_type=FeatureType.FLOAT,\n        preprocessors=[\n            tf.keras.layers.Rescaling,\n            tf.keras.layers.Normalization,\n            DistributionAwareEncoder,\n        ],\n    ),\n}\n\n# Now we can create a preprocessing model with the features\nppr = PreprocessingModel(\n    path_data=\"sample_data.csv\",\n    features_specs=features,\n    features_stats_path=\"features_stats.json\",\n    overwrite_stats=True,\n\n    # Add numerical embedding\n    # Use advanced numerical embedding for individual features\n    use_advanced_numerical_embedding=True,\n    # Use global numerical embedding for all features\n    use_global_numerical_embedding=True,\n\n    output_mode=OutputModeOptions.CONCAT,\n)\n\n# Build the preprocessor\nresult = ppr.build_preprocessor()\n\n# Transform data using direct model prediction\ntransformed_data = ppr.model.predict(test_batch)\n\n# Get feature importances\nfeature_importances = ppr.get_feature_importances()\n</code></pre> <pre><code>ppr.plot_model(\"model_with_advanced_numerical_embedding.png\")\n</code></pre> <p></p>"},{"location":"examples/categorical-hashing-example.html","title":"Categorical Feature Hashing Example","text":"<p>This example demonstrates how to use feature hashing for categorical variables in the KDP library.</p>"},{"location":"examples/categorical-hashing-example.html#what-is-categorical-feature-hashing","title":"What is Categorical Feature Hashing?","text":"<p>Feature hashing (also known as the \"hashing trick\") is a technique used to transform high-cardinality categorical features into a fixed-size vector representation. It's particularly useful for:</p> <ul> <li>Handling categorical features with very large numbers of unique values</li> <li>Dealing with previously unseen categories at inference time</li> <li>Reducing memory usage for high-cardinality features</li> </ul>"},{"location":"examples/categorical-hashing-example.html#when-to-use-hashing-vs-embeddings-or-one-hot-encoding","title":"When to Use Hashing vs. Embeddings or One-Hot Encoding","text":"<ul> <li>One-Hot Encoding: Best for low-cardinality features (typically &lt;10 categories)</li> <li>Embeddings: Good for medium-cardinality features where the relationships between categories are important</li> <li>Hashing: Ideal for high-cardinality features (hundreds or thousands of unique values)</li> </ul>"},{"location":"examples/categorical-hashing-example.html#basic-example","title":"Basic Example","text":"<pre><code>from kdp.features import CategoricalFeature, FeatureType, CategoryEncodingOptions\nfrom kdp.processor import PreprocessingModel\n\n# Define a categorical feature with hashing\nfeatures = {\n    \"high_cardinality_feature\": CategoricalFeature(\n        name=\"high_cardinality_feature\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.HASHING,\n        hash_bucket_size=1024  # Number of hash buckets\n    )\n}\n\n# Create a preprocessing model with the features\nmodel = PreprocessingModel(features_specs=features)\n</code></pre>"},{"location":"examples/categorical-hashing-example.html#advanced-hashing-options","title":"Advanced Hashing Options","text":""},{"location":"examples/categorical-hashing-example.html#hash-with-embeddings","title":"Hash with Embeddings","text":"<p>You can combine hashing with embeddings to reduce dimensionality further:</p> <pre><code>from kdp.features import CategoricalFeature, FeatureType, CategoryEncodingOptions\n\nfeatures = {\n    \"hashed_with_embedding\": CategoricalFeature(\n        name=\"hashed_with_embedding\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.HASHING,\n        hash_bucket_size=512,     # Number of hash buckets\n        hash_with_embedding=True, # Enable embedding layer after hashing\n        embedding_size=16         # Size of the embedding vectors\n    )\n}\n</code></pre>"},{"location":"examples/categorical-hashing-example.html#custom-hash-salt","title":"Custom Hash Salt","text":"<p>Adding a salt value to the hash function can help prevent collisions between different features:</p> <pre><code>features = {\n    \"product_id\": CategoricalFeature(\n        name=\"product_id\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.HASHING,\n        hash_bucket_size=2048,\n        salt=42  # Custom salt value for hashing\n    )\n}\n</code></pre>"},{"location":"examples/categorical-hashing-example.html#comparison-of-different-encoding-methods","title":"Comparison of Different Encoding Methods","text":"<pre><code>features = {\n    # Small cardinality - one hot encoding\n    \"product_category\": CategoricalFeature(\n        name=\"product_category\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.ONE_HOT_ENCODING\n    ),\n\n    # Medium cardinality - embeddings\n    \"store_id\": CategoricalFeature(\n        name=\"store_id\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.EMBEDDING,\n        embedding_size=8\n    ),\n\n    # High cardinality - hashing\n    \"customer_id\": CategoricalFeature(\n        name=\"customer_id\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.HASHING,\n        hash_bucket_size=1024\n    ),\n\n    # Very high cardinality - hashing with embedding\n    \"product_id\": CategoricalFeature(\n        name=\"product_id\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.HASHING,\n        hash_bucket_size=2048,\n        hash_with_embedding=True,\n        embedding_size=16\n    )\n}\n</code></pre>"},{"location":"examples/categorical-hashing-example.html#automatic-configuration-with-modeladvisor","title":"Automatic Configuration with ModelAdvisor","text":"<p>KDP's <code>ModelAdvisor</code> can automatically determine the best encoding strategy for each feature based on data statistics:</p> <pre><code>from kdp.stats import DatasetStatistics\nfrom kdp.model_advisor import recommend_model_configuration\n\n# First, analyze your dataset\ndataset_stats = DatasetStatistics(\"e_commerce_data.csv\")\ndataset_stats.compute_statistics()\n\n# Get recommendations from the ModelAdvisor\nrecommendations = recommend_model_configuration(dataset_stats.features_stats)\n\n# Print feature-specific recommendations\nfor feature, config in recommendations[\"features\"].items():\n    if \"HASHING\" in config.get(\"preprocessing\", []):\n        print(f\"Feature '{feature}' recommended for hashing:\")\n        print(f\"  - Hash bucket size: {config['config'].get('hash_bucket_size')}\")\n        print(f\"  - Use embeddings: {config['config'].get('hash_with_embedding', False)}\")\n        if config['config'].get('hash_with_embedding'):\n            print(f\"  - Embedding size: {config['config'].get('embedding_size')}\")\n        print(f\"  - Salt value: {config['config'].get('salt')}\")\n        print(f\"  - Notes: {', '.join(config.get('notes', []))}\")\n        print()\n\n# Generate ready-to-use code\nprint(\"Generated code snippet:\")\nprint(recommendations[\"code_snippet\"])\n</code></pre> <p>The ModelAdvisor uses these heuristics for categorical features: - For features with &lt;50 unique values: ONE_HOT_ENCODING - For features with 50-1000 unique values: EMBEDDING - For features with &gt;1000 unique values: HASHING - For features with &gt;10,000 unique values: HASHING with embeddings</p> <p>It also automatically determines: - The appropriate hash bucket size based on cardinality - Whether to add salt values to prevent collisions - Embedding dimensions when using hash_with_embedding=True</p>"},{"location":"examples/categorical-hashing-example.html#choosing-the-right-hash-bucket-size","title":"Choosing the Right Hash Bucket Size","text":"<p>The number of hash buckets is a critical parameter that affects model performance:</p> <ul> <li>Too few buckets: Many categories will hash to the same bucket (high collision rate)</li> <li>Too many buckets: Sparse representation that might not generalize well</li> </ul> <p>A good rule of thumb is to use a bucket size that is 2-4 times the number of unique categories in your data.</p>"},{"location":"examples/categorical-hashing-example.html#handling-hash-collisions","title":"Handling Hash Collisions","text":"<p>Hash collisions occur when different category values hash to the same bucket. There are two common strategies to mitigate this:</p> <ol> <li>Increase bucket size: Use more buckets to reduce collision probability</li> <li>Multi-hashing: Apply multiple hash functions and use all outputs:</li> </ol> <pre><code># Example of using multi-hash technique (available in advanced settings)\nfeatures = {\n    \"complex_id\": CategoricalFeature(\n        name=\"complex_id\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        category_encoding=CategoryEncodingOptions.HASHING,\n        hash_bucket_size=1024,\n        hash_with_embedding=True,\n        multi_hash=True,  # Enable multiple hash functions\n        num_hash_functions=3  # Number of hash functions to use\n    )\n}\n</code></pre>"},{"location":"examples/categorical-hashing-example.html#performance-considerations","title":"Performance Considerations","text":"<p>Hashing is computationally efficient compared to maintaining a large vocabulary mapping, especially when:</p> <ul> <li>You have a very large number of unique categories</li> <li>New categories appear frequently in production</li> <li>Memory is constrained</li> </ul> <p>Feature hashing trades off a small amount of accuracy (due to potential collisions) for significant efficiency gains with very high-cardinality features.</p>"},{"location":"examples/categorical-hashing-example.html#complete-end-to-end-example","title":"Complete End-to-End Example","text":"<p>Here's a complete example showing how to use feature hashing for e-commerce product data:</p> <p>```python import pandas as pd from kdp.features import CategoricalFeature, FeatureType, CategoryEncodingOptions, NumericalFeature from kdp.processor import PreprocessingModel</p>"},{"location":"examples/categorical-hashing-example.html#create-sample-e-commerce-data","title":"Create sample e-commerce data","text":"<p>data = {     \"product_id\": [f\"p{i}\" for i in range(1000)],  # High cardinality     \"category\": [\"electronics\", \"clothing\", \"books\", \"home\"] * 250,  # Low cardinality     \"store_id\": [f\"store_{i % 100}\" for i in range(1000)],  # Medium cardinality     \"user_id\": [f\"user_{i % 10000}\" for i in range(1000)],  # Very high cardinality     \"price\": [i * 0.1 for i in range(1000)]  # Numerical } df = pd.DataFrame(data) df.to_csv(\"ecommerce.csv\", index=False)</p>"},{"location":"examples/categorical-hashing-example.html#define-features-with-appropriate-encoding-strategies","title":"Define features with appropriate encoding strategies","text":"<p>features = {     # Low cardinality - one hot encoding     \"category\": CategoricalFeature(         name=\"category\",         feature_type=FeatureType.STRING_CATEGORICAL,         category_encoding=CategoryEncodingOptions.ONE_HOT_ENCODING     ),</p> <pre><code># Medium cardinality - embedding\n\"store_id\": CategoricalFeature(\n    name=\"store_id\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    category_encoding=CategoryEncodingOptions.EMBEDDING,\n    embedding_size=8\n),\n\n# High cardinality - hashing\n\"product_id\": CategoricalFeature(\n    name=\"product_id\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    category_encoding=CategoryEncodingOptions.HASHING,\n    hash_bucket_size=2048,\n    salt=1  # Use different salt values for different features\n),\n\n# Very high cardinality - hashing with embedding\n\"user_id\": CategoricalFeature(\n    name=\"user_id\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    category_encoding=CategoryEncodingOptions.HASHING,\n    hash_bucket_size=4096,\n    hash_with_embedding=True,\n    embedding_size=16,\n    salt=2  # Different salt to avoid collisions with product_id\n),\n\n# Numerical feature\n\"price\": NumericalFeature(\n    name=\"price\",\n    feature_type=FeatureType.FLOAT_NORMALIZED\n)\n</code></pre> <p>}</p>"},{"location":"examples/categorical-hashing-example.html#create-and-build-the-model","title":"Create and build the model","text":"<p>model = PreprocessingModel(     path_data=\"ecommerce.csv\",     features_specs=features,     output_mode=\"CONCAT\" )</p>"},{"location":"examples/categorical-hashing-example.html#build-the-preprocessor","title":"Build the preprocessor","text":"<p>preprocessor = model.build_preprocessor()</p>"},{"location":"examples/categorical-hashing-example.html#use-the-preprocessor-for-inference","title":"Use the preprocessor for inference","text":"<p>input_data = {     \"category\": [\"electronics\"],     \"store_id\": [\"store_42\"],     \"product_id\": [\"p999\"],  # Known product     \"user_id\": [\"user_new\"],  # New user, not seen in training     \"price\": [99.99] }</p>"},{"location":"examples/categorical-hashing-example.html#process-the-data-note-how-hashing-handles-both-known-and-unknown-values","title":"Process the data - note how hashing handles both known and unknown values","text":"<p>processed = preprocessor(input_data) print(\"Output shape:\", processed.shape)</p>"},{"location":"examples/complex-examples.html","title":"\ud83d\ude80 Real-World KDP Examples","text":""},{"location":"examples/complex-examples.html#quick-overview","title":"\ud83d\udccb Quick Overview","text":"<p>Ready to see KDP in action? This guide showcases complete, practical examples that demonstrate how to use KDP's advanced features for real-world scenarios. Each example includes detailed explanations, code snippets, and visualization of the resulting preprocessing pipeline.</p>"},{"location":"examples/complex-examples.html#e-commerce-product-analytics","title":"\ud83d\udcbc E-commerce Product Analytics","text":"<p>This example demonstrates how to preprocess product data for an e-commerce recommendation system, combining numerical, categorical, text, and date features with advanced KDP capabilities.</p>"},{"location":"examples/complex-examples.html#1-setting-up-features","title":"1\ufe0f\u20e3 Setting Up Features","text":"<pre><code>import pandas as pd\nimport tensorflow as tf\nfrom kdp.features import (\n    NumericalFeature, CategoricalFeature, TextFeature, DateFeature, FeatureType\n)\nfrom kdp.processor import PreprocessingModel, OutputModeOptions\n\n# Define features with specialized processing for each type\nfeatures = {\n    # Numerical features with different processing strategies\n    \"price\": NumericalFeature(\n        name=\"price\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        use_embedding=True,                  # Create richer representation\n        embedding_dim=8                      # Size of embedding\n    ),\n    \"quantity\": NumericalFeature(\n        name=\"quantity\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        preferred_distribution=\"poisson\"     # Handle count data appropriately\n    ),\n\n    # Categorical features with semantic embeddings\n    \"category\": CategoricalFeature(\n        name=\"category\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_dim=32,                    # Larger embeddings for complex categories\n        max_vocabulary_size=1000            # Limit vocabulary size\n    ),\n    \"brand\": CategoricalFeature(\n        name=\"brand\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_dim=16\n    ),\n\n    # Text features with different token limits\n    \"description\": TextFeature(\n        name=\"description\",\n        feature_type=FeatureType.TEXT,\n        max_tokens=100,                      # Longer limit for descriptions\n        output_mode=\"embedding\"              # Use text embeddings\n    ),\n    \"title\": TextFeature(\n        name=\"title\",\n        feature_type=FeatureType.TEXT,\n        max_tokens=50                        # Shorter limit for titles\n    ),\n\n    # Date features with seasonality\n    \"sale_date\": DateFeature(\n        name=\"sale_date\",\n        feature_type=FeatureType.DATE,\n        add_season=True,                     # Add seasonal indicators\n        add_day_of_week=True                 # Add day of week information\n    )\n}\n</code></pre>"},{"location":"examples/complex-examples.html#2-creating-the-advanced-preprocessor","title":"2\ufe0f\u20e3 Creating the Advanced Preprocessor","text":"<pre><code># Create a comprehensive preprocessor with multiple advanced features\npreprocessor = PreprocessingModel(\n    path_data=\"ecommerce_data.csv\",\n    features_specs=features,\n    output_mode=OutputModeOptions.CONCAT,\n\n    # Enable distribution-aware processing\n    use_distribution_aware=True,\n    distribution_aware_bins=1000,\n\n    # Enable transformer blocks for complex interactions\n    transfo_nr_blocks=2,\n    transfo_nr_heads=4,\n    transfo_ff_units=64,\n    transfo_dropout_rate=0.1,\n    transfo_placement=\"all_features\",\n\n    # Enable tabular attention for feature relationships\n    tabular_attention=True,\n    tabular_attention_placement=\"multi_resolution\",\n    tabular_attention_heads=3,\n    tabular_attention_dim=32,\n\n    # Enable feature selection to focus on what matters\n    feature_selection_placement=\"all_features\",\n    feature_selection_units=32,\n\n    # Enable caching for better performance\n    enable_caching=True\n)\n\n# Build the preprocessor\nresult = preprocessor.build_preprocessor()\nmodel = result[\"model\"]\n</code></pre>"},{"location":"examples/complex-examples.html#3-using-the-preprocessor","title":"3\ufe0f\u20e3 Using the Preprocessor","text":"<pre><code># Generate predictions on test data\ntest_batch = tf.data.Dataset.from_tensor_slices(dict(test_df)).batch(32)\nprocessed_features = model.predict(test_batch)\n\n# Analyze feature importance\nfeature_importances = preprocessor.get_feature_importances()\nprint(\"Top features:\", sorted(\n    feature_importances.items(),\n    key=lambda x: x[1],\n    reverse=True\n)[:3])\n\n# Visualize the model architecture\npreprocessor.plot_model(\"ecommerce_model.png\")\n</code></pre>"},{"location":"examples/complex-examples.html#financial-time-series-analysis","title":"\ud83d\udcca Financial Time Series Analysis","text":"<p>This example demonstrates using KDP to preprocess financial time series data for forecasting, focusing on distribution-aware encoding and feature interactions.</p>"},{"location":"examples/complex-examples.html#setting-up-features","title":"Setting Up Features","text":"<pre><code># Define financial features\nfeatures = {\n    # Price data with custom distribution handling\n    \"close_price\": NumericalFeature(\n        name=\"close_price\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        use_embedding=True,\n        embedding_dim=16,\n        preferred_distribution=\"heavy_tailed\"  # Handle market data distributions\n    ),\n    \"volume\": NumericalFeature(\n        name=\"volume\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        preferred_distribution=\"log_normal\"    # Common for volume data\n    ),\n\n    # Technical indicators\n    \"rsi\": NumericalFeature(\n        name=\"rsi\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,  # RSI is already 0-100\n        preferred_distribution=\"normal\"\n    ),\n    \"macd\": NumericalFeature(\n        name=\"macd\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        preferred_distribution=\"normal\"\n    ),\n\n    # Categorical market data\n    \"market_regime\": CategoricalFeature(\n        name=\"market_regime\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_dim=8\n    ),\n\n    # Date information with market-specific features\n    \"date\": DateFeature(\n        name=\"date\",\n        feature_type=FeatureType.DATE,\n        add_day_of_week=True,  # Markets behave differently on different days\n        add_month=True         # Capture seasonal effects\n    )\n}\n\n# Create financial preprocessor\nfinancial_preprocessor = PreprocessingModel(\n    path_data=\"market_data.csv\",\n    features_specs=features,\n\n    # Enable advanced numerical embeddings for better pattern detection\n    use_numerical_embedding=True,\n    numerical_embedding_dim=32,\n\n    # Enable distribution-aware encoding for market data\n    use_distribution_aware=True,\n\n    # Enable feature selection to identify important signals\n    feature_selection_placement=\"ALL_FEATURES\",\n\n    # Financial time series benefits from attention mechanisms\n    tabular_attention=True,\n    tabular_attention_heads=8\n)\n\n# Build the preprocessor\nfinancial_result = financial_preprocessor.build_preprocessor()\n</code></pre>"},{"location":"examples/complex-examples.html#user-behavior-analysis","title":"\ud83d\udc65 User Behavior Analysis","text":"<p>This example shows how to preprocess user behavior data for churn prediction or personalization algorithms.</p>"},{"location":"examples/complex-examples.html#setting-up-features_1","title":"Setting Up Features","text":"<pre><code># Define user behavior features\nfeatures = {\n    # User demographics\n    \"age\": NumericalFeature(\n        name=\"age\",\n        feature_type=FeatureType.FLOAT_NORMALIZED\n    ),\n    \"gender\": CategoricalFeature(\n        name=\"gender\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_dim=4\n    ),\n    \"location\": CategoricalFeature(\n        name=\"location\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_dim=16,\n        max_vocabulary_size=500  # Limit to top locations\n    ),\n\n    # Behavioral metrics\n    \"days_since_last_login\": NumericalFeature(\n        name=\"days_since_last_login\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        preferred_distribution=\"exponential\"  # Often follows exponential distribution\n    ),\n    \"total_purchases\": NumericalFeature(\n        name=\"total_purchases\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        preferred_distribution=\"zero_inflated\"  # Many users have zero purchases\n    ),\n    \"average_session_time\": NumericalFeature(\n        name=\"average_session_time\",\n        feature_type=FeatureType.FLOAT_RESCALED\n    ),\n\n    # Categorical engagement data\n    \"subscription_tier\": CategoricalFeature(\n        name=\"subscription_tier\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_dim=8\n    ),\n\n    # Textual data\n    \"user_feedback\": TextFeature(\n        name=\"user_feedback\",\n        feature_type=FeatureType.TEXT,\n        max_tokens=150\n    ),\n\n    # Time-based features\n    \"account_creation_date\": DateFeature(\n        name=\"account_creation_date\",\n        feature_type=FeatureType.DATE,\n        add_season=True\n    )\n}\n\n# Create user behavior preprocessor\nuser_preprocessor = PreprocessingModel(\n    path_data=\"user_data.csv\",\n    features_specs=features,\n\n    # Enable mixture of experts for specialized handling\n    use_feature_moe=True,\n    feature_moe_num_experts=5,\n\n    # Enable feature selection to identify churn indicators\n    feature_selection_placement=\"ALL_FEATURES\",\n\n    # Enable distribution-aware encoding\n    use_distribution_aware=True,\n\n    # User behavior data benefits from transformer blocks\n    transfo_nr_blocks=2,\n    transfo_nr_heads=4\n)\n\n# Build the preprocessor\nuser_result = user_preprocessor.build_preprocessor()\n</code></pre>"},{"location":"examples/complex-examples.html#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"<ol> <li>Start Simple, Then Add Complexity</li> <li>Begin with basic feature definitions</li> <li>Add advanced features one at a time</li> <li> <p>Test each addition's impact</p> </li> <li> <p>Monitor Resource Usage</p> </li> <li>Complex models can be memory-intensive</li> <li>Use batch processing for large datasets</li> <li> <p>Monitor preprocessing time and adjust accordingly</p> </li> <li> <p>Balance Preprocessing Complexity</p> </li> <li>More complex preprocessing isn't always better</li> <li>Focus on features that provide the most value</li> <li> <p>Use <code>get_feature_importances()</code> to identify key features</p> </li> <li> <p>Save Your Preprocessing Pipeline <pre><code># Save the complete pipeline for reuse\npreprocessor.save_model(\"my_preprocessor.keras\")\n\n# Load it when needed\nfrom kdp import PreprocessingModel\nloaded_preprocessor = PreprocessingModel.load_model(\"my_preprocessor.keras\")\n</code></pre></p> </li> </ol>"},{"location":"examples/complex-examples.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"<ul> <li>Quick Start Guide</li> <li>Feature Processing Guide</li> <li>Advanced Numerical Embeddings</li> <li>Distribution-Aware Encoding</li> </ul>"},{"location":"features/categorical-features.html","title":"\ud83c\udff7\ufe0f Categorical Features","text":"Categorical Features in KDP <p>Learn how to effectively represent categories, leverage embeddings, and handle high-cardinality data.</p>"},{"location":"features/categorical-features.html#overview","title":"\ud83d\udccb Overview","text":"<p>Categorical features represent data that belongs to a finite set of possible values or categories. KDP provides advanced techniques for handling categorical data, from simple encoding to neural embeddings that capture semantic relationships between categories.</p>"},{"location":"features/categorical-features.html#types-of-categorical-features","title":"\ud83d\ude80 Types of Categorical Features","text":"Feature Type Best For Example When to Use <code>STRING_CATEGORICAL</code> Text categories product_type: \"shirt\", \"pants\", \"shoes\" When categories are text strings <code>INTEGER_CATEGORICAL</code> Numeric categories education_level: 1, 2, 3, 4 When categories are already represented as integers <code>STRING_HASHED</code> High-cardinality sets user_id: \"user_12345\", \"user_67890\" When there are too many unique categories (&gt;10K) <code>MULTI_CATEGORICAL</code> Multiple categories per sample interests: [\"sports\", \"music\", \"travel\"] When each sample can belong to multiple categories"},{"location":"features/categorical-features.html#basic-usage","title":"\ud83d\udcdd Basic Usage","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Simple categorical features\nfeatures = {\n    \"product_category\": FeatureType.STRING_CATEGORICAL,\n    \"store_id\": FeatureType.INTEGER_CATEGORICAL,\n    \"tags\": FeatureType.MULTI_CATEGORICAL\n}\n\npreprocessor = PreprocessingModel(\n    path_data=\"product_data.csv\",\n    features_specs=features\n)\n</code></pre>"},{"location":"features/categorical-features.html#advanced-configuration","title":"\ud83e\udde0 Advanced Configuration","text":"<p>For more control over categorical processing, use the detailed configuration:</p> <pre><code>from kdp import PreprocessingModel, FeatureType, CategoricalFeature\n\n# Detailed configuration\nfeatures = {\n    # Basic configuration\n    \"product_type\": FeatureType.STRING_CATEGORICAL,\n\n    # Full configuration with explicit CategoricalFeature\n    \"store_location\": CategoricalFeature(\n        name=\"store_location\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_dim=16,                  # Size of embedding vector\n        hash_bucket_size=1000,             # For hashed features\n        vocabulary_size=250,               # Limit vocabulary size\n        use_embedding=True,                # Use neural embeddings\n        unknown_token=\"&lt;UNK&gt;\",             # Token for out-of-vocabulary values\n        oov_buckets=10,                    # Out-of-vocabulary buckets\n        multi_hot=False                    # For single category per sample\n    ),\n\n    # High-cardinality feature using hashing\n    \"product_id\": CategoricalFeature(\n        name=\"product_id\",\n        feature_type=FeatureType.STRING_HASHED,\n        hash_bucket_size=5000\n    ),\n\n    # Multi-categorical feature with separator\n    \"product_tags\": CategoricalFeature(\n        name=\"product_tags\",\n        feature_type=FeatureType.MULTI_CATEGORICAL,\n        separator=\",\",                     # How values are separated in data\n        multi_hot=True                     # Enable multi-hot encoding\n    )\n}\n\npreprocessor = PreprocessingModel(\n    path_data=\"product_data.csv\",\n    features_specs=features\n)\n</code></pre>"},{"location":"features/categorical-features.html#key-configuration-parameters","title":"\u2699\ufe0f Key Configuration Parameters","text":"Parameter Description Default Notes <code>embedding_dim</code> Size of embedding vectors 8 Higher values capture more complex relationships (8-128) <code>hash_bucket_size</code> Number of hash buckets for hashed features 1000 Larger values reduce collisions but increase dimensionality <code>salt</code> Salt value for hash function None Custom salt to make hash values unique across features <code>hash_with_embedding</code> Apply embedding after hashing False Combines hashing with embeddings for large vocabularies <code>vocabulary_size</code> Maximum number of categories to keep None None uses all categories, otherwise keeps top N by frequency <code>use_embedding</code> Enable neural embeddings vs. one-hot encoding True Neural embeddings improve performance for most models <code>separator</code> Character that separates values in multi-categorical features \",\" Only used for <code>MULTI_CATEGORICAL</code> features <code>oov_buckets</code> Number of buckets for out-of-vocabulary values 1 Higher values help handle new categories in production"},{"location":"features/categorical-features.html#powerful-features","title":"\ud83d\udca1 Powerful Features","text":"\ud83e\uddff Embedding Visualizations <p>KDP's categorical embeddings can be visualized to see relationships between categories:</p> <pre><code># Train the preprocessor\npreprocessor.fit()\nresult = preprocessor.build_preprocessor()\n\n# Extract embeddings for visualization\nembeddings = preprocessor.get_feature_embeddings(\"product_category\")\n\n# Visualize with t-SNE or UMAP\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\ntsne = TSNE(n_components=2)\nembeddings_2d = tsne.fit_transform(embeddings)\n\nplt.figure(figsize=(10, 8))\nplt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\nplt.title(\"Category Embedding Visualization\")\nplt.show()\n</code></pre> \ud83c\udf0d Handling High-Cardinality <p>KDP provides multiple strategies for dealing with features that have many unique values:</p> <pre><code># Method 1: Limit vocabulary size (keeps most frequent)\nuser_id_limited = CategoricalFeature(\n    name=\"user_id\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    vocabulary_size=10000  # Keep top 10K users\n)\n\n# Method 2: Hash features to buckets (fast, fixed memory)\nuser_id_hashed = CategoricalFeature(\n    name=\"user_id\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    category_encoding=CategoryEncodingOptions.HASHING,\n    hash_bucket_size=5000  # Hash into 5K buckets\n)\n\n# Method 3: Hash with embeddings (best balance)\nuser_id_hash_embed = CategoricalFeature(\n    name=\"user_id\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    category_encoding=CategoryEncodingOptions.HASHING,\n    hash_bucket_size=2048,\n    hash_with_embedding=True,\n    embedding_size=16\n)\n</code></pre> \ud83e\uddee Feature Hashing <p>Feature hashing transforms categorical values into a fixed-size vector representation, ideal for very high-cardinality features. It's now fully integrated with the ModelAdvisor for automatic configuration:</p> <pre><code># Basic feature hashing\nproduct_id = CategoricalFeature(\n    name=\"product_id\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    category_encoding=CategoryEncodingOptions.HASHING,\n    hash_bucket_size=1024  # Number of hash buckets\n)\n\n# Advanced feature hashing with custom salt\n# The salt ensures different features use different hash spaces\nsession_id = CategoricalFeature(\n    name=\"session_id\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    category_encoding=CategoryEncodingOptions.HASHING,\n    hash_bucket_size=2048,\n    salt=42  # Custom salt value\n)\n\n# Feature hashing followed by embedding\nuser_id = CategoricalFeature(\n    name=\"user_id\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    category_encoding=CategoryEncodingOptions.HASHING,\n    hash_bucket_size=2048,\n    hash_with_embedding=True,\n    embedding_size=16  # Embedding dimension after hashing\n)\n</code></pre> \ud83e\udd16 Auto-configuration with ModelAdvisor <p>KDP's ModelAdvisor now intelligently recommends hashing for high-cardinality features:</p> <pre><code>from kdp.model_advisor import recommend_model_configuration\nfrom kdp.stats import DatasetStatistics\n\n# Analyze dataset statistics\nstats_calculator = DatasetStatistics(\"high_cardinality_data.csv\")\nstats_calculator.compute_statistics()\n\n# Get recommendations from ModelAdvisor\nrecommendations = recommend_model_configuration(stats_calculator.features_stats)\n\n# The recommendations will include HASHING for high-cardinality features\n# Example output:\n'''\n{\n  \"features\": {\n    \"user_id\": {\n      \"feature_type\": \"CategoricalFeature\",\n      \"preprocessing\": [\"HASHING\"],\n      \"config\": {\n        \"category_encoding\": \"HASHING\",\n        \"hash_bucket_size\": 2048,\n        \"hash_with_embedding\": true,\n        \"embedding_size\": 16\n      },\n      \"notes\": [\"High cardinality feature (10K+ values)\", \"Using hashing for efficiency\"]\n    },\n    ...\n  }\n}\n'''\n\n# Generate code from recommendations\ncode_snippet = recommendations[\"code_snippet\"]\nprint(code_snippet)\n</code></pre> \ud83d\udd0d Choosing Between Encoding Options <p>KDP offers multiple encoding options for categorical features. Here's how to choose:</p> Encoding Vocabulary Size Memory Usage New Categories Semantic Information One-Hot Encoding Small (&lt; 50) High \u274c Requires retraining \u274c No relationship capture Embeddings Medium (50-10K) Medium \u26a0\ufe0f Limited by OOV handling \u2705 Captures relationships Hashing Very Large (10K+) Low (fixed) \u2705 Handles new values \u274c No relationship capture Hashing with Embeddings Very Large (10K+) Low-Medium \u2705 Handles new values \u2705 Some relationship capture <p>The ModelAdvisor analyzes your data and automatically recommends the optimal encoding based on these criteria.</p> <pre><code># End-to-end example with automatic encoding selection\nfrom kdp.features import CategoricalFeature, FeatureType, CategoryEncodingOptions\nfrom kdp.processor import PreprocessingModel\nfrom kdp.stats import DatasetStatistics\nfrom kdp.model_advisor import recommend_model_configuration\n\n# 1. Analyze dataset\nstats = DatasetStatistics(\"product_dataset.csv\")\nstats.compute_statistics()\n\n# 2. Get recommendations\nrecommendations = recommend_model_configuration(stats.features_stats)\n\n# 3. Create preprocessing model using recommended config\nfeatures = {}\nfor name, feature_rec in recommendations[\"features\"].items():\n    if feature_rec[\"feature_type\"] == \"CategoricalFeature\":\n        # Extract configuration for this categorical feature\n        config = feature_rec[\"config\"]\n        features[name] = CategoricalFeature(\n            name=name,\n            feature_type=getattr(FeatureType, config.get(\"feature_type\", \"STRING_CATEGORICAL\")),\n            category_encoding=getattr(CategoryEncodingOptions, config.get(\"category_encoding\", \"EMBEDDING\")),\n            hash_bucket_size=config.get(\"hash_bucket_size\"),\n            hash_with_embedding=config.get(\"hash_with_embedding\", False),\n            embedding_size=config.get(\"embedding_size\"),\n            salt=config.get(\"salt\")\n        )\n\n# 4. Create and build the preprocessing model\nmodel = PreprocessingModel(\n    path_data=\"product_dataset.csv\",\n    features_specs=features\n)\npreprocessor = model.build_preprocessor()\n</code></pre>"},{"location":"features/categorical-features.html#real-world-examples","title":"\ud83d\udd27 Real-World Examples","text":"E-commerce Product Categorization <pre><code># E-commerce features with hierarchical categories\npreprocessor = PreprocessingModel(\n    path_data=\"products.csv\",\n    features_specs={\n        # Main category, subcategory, and detailed category\n        \"main_category\": FeatureType.STRING_CATEGORICAL,\n        \"subcategory\": FeatureType.STRING_CATEGORICAL,\n        \"detailed_category\": FeatureType.STRING_CATEGORICAL,\n\n        # Product attributes as multi-categories\n        \"product_features\": CategoricalFeature(\n            name=\"product_features\",\n            feature_type=FeatureType.MULTI_CATEGORICAL,\n            separator=\"|\",\n            multi_hot=True\n        ),\n\n        # Brand as a high-cardinality feature\n        \"brand\": CategoricalFeature(\n            name=\"brand\",\n            feature_type=FeatureType.STRING_CATEGORICAL,\n            embedding_dim=16,\n            vocabulary_size=1000  # Top 1000 brands\n        )\n    }\n)\n</code></pre> Content Recommendation System <pre><code># Content recommendation with user and item features\npreprocessor = PreprocessingModel(\n    path_data=\"interaction_data.csv\",\n    features_specs={\n        # User features\n        \"user_id\": CategoricalFeature(\n            name=\"user_id\",\n            feature_type=FeatureType.STRING_HASHED,\n            hash_bucket_size=10000\n        ),\n        \"user_interests\": CategoricalFeature(\n            name=\"user_interests\",\n            feature_type=FeatureType.MULTI_CATEGORICAL,\n            embedding_dim=32,\n            separator=\",\"\n        ),\n\n        # Content features\n        \"content_id\": CategoricalFeature(\n            name=\"content_id\",\n            feature_type=FeatureType.STRING_HASHED,\n            hash_bucket_size=5000\n        ),\n        \"content_tags\": CategoricalFeature(\n            name=\"content_tags\",\n            feature_type=FeatureType.MULTI_CATEGORICAL,\n            embedding_dim=24,\n            separator=\"|\"\n        ),\n        \"content_type\": FeatureType.STRING_CATEGORICAL\n    }\n)\n</code></pre>"},{"location":"features/categorical-features.html#pro-tips","title":"\ud83d\udc8e Pro Tips","text":"\ud83d\udd0d Choose Embedding Dimensions Wisely <p>For simple categories with few values (2-10), use 4-8 dimensions. For complex categories with many values (100+), use 16-64 dimensions. The more complex the relationships between categories, the higher dimensions you need.</p> \u26a1 Pre-train Embeddings <p>KDP allows you to initialize embeddings with pre-trained vectors for faster convergence:</p> <pre><code># Create initial embeddings dictionary\npretrained = {\n    \"sports\": [0.1, 0.2, 0.3, 0.4],\n    \"music\": [0.5, 0.6, 0.7, 0.8]\n}\n\n# Use pre-trained embeddings\ncategory_feature = CategoricalFeature(\n    name=\"interest\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    embedding_dim=4,\n    pretrained_embeddings=pretrained\n)\n</code></pre> \ud83c\udf00 Combine Multiple Encoding Strategies <p>For critical features, consider using both embeddings and one-hot encoding in parallel:</p> <pre><code># Main feature with embedding\nfeatures[\"product_type\"] = CategoricalFeature(\n    name=\"product_type\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    use_embedding=True\n)\n\n# Same feature with one-hot encoding\nfeatures[\"product_type_onehot\"] = CategoricalFeature(\n    name=\"product_type\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    use_embedding=False\n)\n</code></pre> \ud83d\udd04 Handling Unknown Categories <p>Configure how KDP handles previously unseen categories in production:</p> <pre><code>feature = CategoricalFeature(\n    name=\"store_type\",\n    feature_type=FeatureType.STRING_CATEGORICAL,\n    unknown_token=\"&lt;NEW_STORE&gt;\",  # Custom token\n    oov_buckets=5                 # Use 5 different embeddings\n)\n</code></pre>"},{"location":"features/categorical-features.html#understanding-categorical-embeddings","title":"\ud83d\udcca Understanding Categorical Embeddings","text":"graph TD       A[Raw Category Data] --&gt;|Vocabulary Creation| B[Category Vocabulary]       B --&gt;|Lookup| C[Integer Indices]       C --&gt;|Embedding Layer| D[Dense Vectors]        style A fill:#f9f9f9,stroke:#ccc,stroke-width:2px       style B fill:#e1f5fe,stroke:#4fc3f7,stroke-width:2px       style C fill:#e8f5e9,stroke:#66bb6a,stroke-width:2px       style D fill:#f3e5f5,stroke:#ce93d8,stroke-width:2px    <p>Categorical embeddings transform categorical values into dense vector representations that capture semantic relationships between categories.</p>"},{"location":"features/categorical-features.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"\ud83d\udd22 Numerical Features \ud83d\udcdd Text Features \ud83e\udde0 Advanced Embedding Techniques \ud83d\udcca Categorical Encoding Examples \u2190 Numerical Features Text Features \u2192"},{"location":"features/cross-features.html","title":"\u2795 Cross Features","text":"Cross Features in KDP <p>Capture powerful interactions between features to uncover hidden patterns in your data.</p>"},{"location":"features/cross-features.html#overview","title":"\ud83d\udccb Overview","text":"<p>Cross features model the interactions between input features, unlocking patterns that individual features alone might miss. They're especially powerful for capturing relationships like \"product category \u00d7 user location\" or \"day of week \u00d7 hour of day\" that drive important outcomes in your data.</p> \ud83d\udd17 Feature Interaction <p>Capture complex relationships between features</p> \ud83c\udfaf Pattern Discovery <p>Uncover hidden correlations in your data</p> \u26a1 Efficient Processing <p>Optimized for large-scale feature crosses</p> \ud83e\udde0 Smart Embeddings <p>Learn meaningful feature combinations</p>"},{"location":"features/cross-features.html#how-cross-features-work","title":"\ud83e\udde0 How Cross Features Work","text":"<p>KDP's cross features combine input features through a sophisticated embedding process, creating rich representations of feature interactions.</p> \ud83d\udd04 Feature Combination <p>Merging values from different features</p> \ud83d\udcca Vocabulary Creation <p>Building a vocabulary of meaningful combinations</p> \ud83e\uddee Embedding Generation <p>Creating dense representations of combined features</p> \ud83d\udd0d Pattern Discovery <p>Finding non-linear relationships between features</p>"},{"location":"features/cross-features.html#basic-usage","title":"\ud83d\udcdd Basic Usage","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Define your features\nfeatures = {\n    \"product_category\": FeatureType.STRING_CATEGORICAL,\n    \"user_country\": FeatureType.STRING_CATEGORICAL,\n    \"age_group\": FeatureType.STRING_CATEGORICAL\n}\n\n# Create a preprocessor with cross features\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",\n    features_specs=features,\n\n    # Define crosses as (feature1, feature2, embedding_dim)\n    feature_crosses=[\n        (\"product_category\", \"user_country\", 32),  # Cross with 32-dim embedding\n        (\"age_group\", \"user_country\", 16)          # Cross with 16-dim embedding\n    ]\n)\n</code></pre>"},{"location":"features/cross-features.html#key-configuration-parameters","title":"\u2699\ufe0f Key Configuration Parameters","text":"Parameter Description Default Suggested Range <code>feature1</code> First feature to cross - Any feature name <code>feature2</code> Second feature to cross - Any feature name <code>embedding_dim</code> Dimensionality of cross embedding 16 8-64 <code>hash_bucket_size</code> Size of hash space for combinations 10000 1000-100000 <code>use_attention</code> Apply attention to cross embeddings False Boolean"},{"location":"features/cross-features.html#cross-feature-types","title":"\ud83d\udee0\ufe0f Cross Feature Types","text":"Categorical \u00d7 Categorical <p>The most common type, capturing relationships between discrete features:</p> <pre><code># Creating categorical crosses\npreprocessor = PreprocessingModel(\n    features_specs={\n        \"product_category\": FeatureType.STRING_CATEGORICAL,\n        \"user_country\": FeatureType.STRING_CATEGORICAL\n    },\n    feature_crosses=[\n        (\"product_category\", \"user_country\", 32)\n    ]\n)\n</code></pre> Categorical \u00d7 Numerical <p>Capture how numerical relationships change across categories:</p> <pre><code># Creating categorical \u00d7 numerical crosses\npreprocessor = PreprocessingModel(\n    features_specs={\n        \"product_category\": FeatureType.STRING_CATEGORICAL,\n        \"price\": FeatureType.FLOAT_RESCALED\n    },\n    feature_crosses=[\n        (\"product_category\", \"price\", 32)\n    ]\n)\n</code></pre> Date Component Crosses <p>Useful for temporal patterns that depend on multiple time components:</p> <pre><code># Creating date component crosses\nfrom kdp.features import DateFeature\n\npreprocessor = PreprocessingModel(\n    features_specs={\n        \"transaction_time\": DateFeature(\n            name=\"transaction_time\",\n            add_day_of_week=True,\n            add_hour=True\n        )\n    },\n    # Cross day of week with hour of day\n    feature_crosses=[\n        (\"transaction_time_day_of_week\", \"transaction_time_hour\", 16)\n    ]\n)\n</code></pre> Multiple Crosses <p>Combine multiple cross features to capture complex interactions:</p> <pre><code># Creating multiple crosses\npreprocessor = PreprocessingModel(\n    features_specs={\n        \"product_category\": FeatureType.STRING_CATEGORICAL,\n        \"user_country\": FeatureType.STRING_CATEGORICAL,\n        \"device_type\": FeatureType.STRING_CATEGORICAL,\n        \"user_age\": FeatureType.FLOAT_NORMALIZED\n    },\n    # Define multiple crosses to capture different interactions\n    feature_crosses=[\n        (\"product_category\", \"user_country\", 32),\n        (\"device_type\", \"user_country\", 16),\n        (\"product_category\", \"user_age\", 24)\n    ]\n)\n</code></pre>"},{"location":"features/cross-features.html#advanced-cross-feature-techniques","title":"\ud83d\udca1 Advanced Cross Feature Techniques","text":"\ud83d\udd0d Attention-Enhanced Crosses <p>Apply attention mechanisms to learn which interactions matter most:</p> <pre><code># Creating cross features with attention\nfrom kdp import PreprocessingModel, FeatureType\nfrom kdp.features import CrossFeature\n\npreprocessor = PreprocessingModel(\n    features_specs={\n        \"product_id\": FeatureType.STRING_CATEGORICAL,\n        \"user_id\": FeatureType.STRING_CATEGORICAL\n    },\n    feature_crosses=[\n        # Define cross with attention\n        CrossFeature(\n            feature1=\"product_id\",\n            feature2=\"user_id\",\n            embedding_dim=32,\n            use_attention=True,\n            attention_heads=4\n        )\n    ]\n)\n</code></pre> \ud83e\udde0 Multi-way Crosses <p>Create complex interactions between three or more features:</p> <pre><code># Creating multi-way crosses (3+ features)\nfrom kdp.features import CompoundFeature, CrossFeature\n\n# First create a cross of two features\nproduct_location_cross = CrossFeature(\n    name=\"product_location_cross\",\n    feature1=\"product_category\",\n    feature2=\"user_location\",\n    embedding_dim=32\n)\n\n# Then cross the result with a third feature\npreprocessor = PreprocessingModel(\n    features_specs={\n        \"product_category\": FeatureType.STRING_CATEGORICAL,\n        \"user_location\": FeatureType.STRING_CATEGORICAL,\n        \"time_of_day\": FeatureType.STRING_CATEGORICAL,\n        # Add the intermediate cross\n        \"product_location_cross\": product_location_cross\n    },\n    # Cross the intermediate with a third feature\n    feature_crosses=[\n        (\"product_location_cross\", \"time_of_day\", 48)\n    ]\n)\n</code></pre>"},{"location":"features/cross-features.html#real-world-examples","title":"\ud83d\udd27 Real-World Examples","text":"E-commerce Recommendations <pre><code># Cross features for e-commerce recommendations\nfrom kdp import PreprocessingModel, FeatureType\nfrom kdp.features import CategoricalFeature, DateFeature\n\npreprocessor = PreprocessingModel(\n    path_data=\"ecommerce_data.csv\",\n    features_specs={\n        # User features\n        \"user_segment\": FeatureType.STRING_CATEGORICAL,\n        \"user_device\": FeatureType.STRING_CATEGORICAL,\n\n        # Product features\n        \"product_category\": CategoricalFeature(\n            name=\"product_category\",\n            feature_type=FeatureType.STRING_CATEGORICAL,\n            embedding_dim=32\n        ),\n        \"product_price_range\": FeatureType.STRING_CATEGORICAL,\n\n        # Temporal features\n        \"browse_time\": DateFeature(\n            name=\"browse_time\",\n            add_day_of_week=True,\n            add_hour=True,\n            add_is_weekend=True\n        )\n    },\n\n    # Define crosses for recommendation patterns\n    feature_crosses=[\n        # User segment \u00d7 product category (what segments like what categories)\n        (\"user_segment\", \"product_category\", 48),\n\n        # Device \u00d7 price range (mobile users prefer different price points)\n        (\"user_device\", \"product_price_range\", 16),\n\n        # Temporal \u00d7 product (weekend browsing patterns)\n        (\"browse_time_is_weekend\", \"product_category\", 32),\n\n        # Time of day \u00d7 product (morning vs evening preferences)\n        (\"browse_time_hour\", \"product_category\", 32)\n    ]\n)\n</code></pre> Fraud Detection <pre><code># Cross features for fraud detection\nfrom kdp import PreprocessingModel, FeatureType\nfrom kdp.features import NumericalFeature, DateFeature\n\npreprocessor = PreprocessingModel(\n    path_data=\"transactions.csv\",\n    features_specs={\n        # Transaction features\n        \"transaction_amount\": NumericalFeature(\n            name=\"transaction_amount\",\n            feature_type=FeatureType.FLOAT_RESCALED,\n            use_distribution_aware=True\n        ),\n        \"merchant_category\": FeatureType.STRING_CATEGORICAL,\n        \"payment_method\": FeatureType.STRING_CATEGORICAL,\n\n        # User features\n        \"user_country\": FeatureType.STRING_CATEGORICAL,\n        \"account_age_days\": FeatureType.FLOAT_NORMALIZED,\n\n        # Time features\n        \"transaction_time\": DateFeature(\n            name=\"transaction_time\",\n            add_hour=True,\n            add_day_of_week=True,\n            add_is_weekend=True\n        )\n    },\n\n    # Cross features for fraud patterns\n    feature_crosses=[\n        # Country \u00d7 merchant (unusual combinations)\n        (\"user_country\", \"merchant_category\", 32),\n\n        # Payment method \u00d7 amount (unusual payment methods for large amounts)\n        (\"payment_method\", \"transaction_amount\", 24),\n\n        # Time \u00d7 amount (unusual times for large transactions)\n        (\"transaction_time_hour\", \"transaction_amount\", 24),\n\n        # Country \u00d7 time (transactions from unusual locations at odd hours)\n        (\"user_country\", \"transaction_time_hour\", 32)\n    ],\n\n    # Enable tabular attention for additional interaction discovery\n    tabular_attention=True\n)\n</code></pre>"},{"location":"features/cross-features.html#model-architecture","title":"\ud83d\udcca Model Architecture","text":"graph TD       A1[Feature 1] --&gt; C[Feature Combination]       A2[Feature 2] --&gt; C       C --&gt; D[Hash/Lookup]       D --&gt; E[Embedding Layer]       E --&gt; F[Cross Representation]        style A1 fill:#e3f2fd,stroke:#64b5f6,stroke-width:2px       style A2 fill:#e3f2fd,stroke:#64b5f6,stroke-width:2px       style C fill:#e8f5e9,stroke:#66bb6a,stroke-width:2px       style D fill:#fff8e1,stroke:#ffd54f,stroke-width:2px       style E fill:#f3e5f5,stroke:#ce93d8,stroke-width:2px       style F fill:#e8eaf6,stroke:#7986cb,stroke-width:2px    <p>KDP combines features, creates a vocabulary or hash space for combinations, and embeds these into dense representations to capture meaningful interactions.</p>"},{"location":"features/cross-features.html#pro-tips","title":"\ud83d\udc8e Pro Tips","text":"\ud83c\udfaf Choose Meaningful Crosses <p>Focus on feature pairs with likely interactions based on domain knowledge:</p> <ul> <li>Product \u00d7 location (regional preferences)</li> <li>Time \u00d7 event (temporal patterns)</li> <li>User \u00d7 item (personalization)</li> <li>Price \u00d7 category (price sensitivity)</li> </ul> \u26a0\ufe0f Beware of Sparsity <p>Crosses between high-cardinality features can create sparse combinations:</p> <ul> <li>Use embeddings (default in KDP) rather than one-hot encoding</li> <li>Consider hashing for very high cardinality crosses</li> <li>Use category_encoding=\"hashing\" for feature types with many values</li> </ul> \ud83d\udccf Cross Dimensionality <p>Choose embedding dimension based on cross importance and complexity:</p> <ul> <li>More important crosses deserve higher dimensionality</li> <li>Simple crosses: 8-16 dimensions</li> <li>Complex crosses: 32-64 dimensions</li> <li>Rule of thumb: \u2074\u221a(possible combinations)</li> </ul> \ud83d\udd04 Alternative Approaches <p>Consider other interaction modeling techniques alongside crosses:</p> <ul> <li>Enable tabular_attention=True to automatically discover interactions</li> <li>Use transformer_blocks for more sophisticated feature relationships</li> <li>Try dot-product interactions for numerical features</li> </ul>"},{"location":"features/cross-features.html#comparing-with-alternatives","title":"\ud83d\udd04 Comparing With Alternatives","text":"Approach Pros Cons When to Use Cross Features Explicit modeling of specific interactions Need to specify each interaction When you know which interactions matter Tabular Attention Automatic discovery of interactions Less control When you're unsure which interactions matter Transformer Blocks Most powerful interaction modeling Most computationally expensive For complex interaction patterns Feature MoE Adaptive feature processing Higher complexity For heterogeneous feature sets"},{"location":"features/cross-features.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"\ud83c\udff7\ufe0f Categorical Features \ud83d\udcc5 Date Features \ud83d\udc41\ufe0f Tabular Attention \ud83d\udcda Feature Cross Examples \u2190 Date Features Distribution-Aware Encoding \u2192"},{"location":"features/date-features.html","title":"\ud83d\udcc5 Date Features","text":"Date Features in KDP <p>Extract powerful patterns from temporal data like timestamps, dates, and time series.</p>"},{"location":"features/date-features.html#overview","title":"\ud83d\udccb Overview","text":"<p>Date features transform timestamps and dates into ML-ready representations that capture important temporal patterns and seasonality. KDP automatically handles date parsing and formatting, enabling your models to learn from time-based signals.</p>"},{"location":"features/date-features.html#date-processing-approaches","title":"\ud83d\ude80 Date Processing Approaches","text":"\ud83d\udcc6 Component Extraction <p>Breaking dates into day, month, year, etc.</p> \ud83d\udd04 Cyclical Encoding <p>Representing cyclic time components (hour, weekday)</p> \ud83d\udcca Temporal Distances <p>Computing time since reference points</p> \ud83d\udcc8 Seasonality Analysis <p>Capturing seasonal patterns and trends</p>"},{"location":"features/date-features.html#basic-usage","title":"\ud83d\udcdd Basic Usage","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Quick date feature definition\nfeatures = {\n    \"purchase_date\": FeatureType.DATE,     # Transaction dates\n    \"signup_date\": FeatureType.DATE,       # User signup dates\n    \"last_active\": FeatureType.DATE        # Last activity timestamps\n}\n\n# Create your preprocessor\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",\n    features_specs=features\n)\n</code></pre>"},{"location":"features/date-features.html#advanced-configuration","title":"\ud83e\udde0 Advanced Configuration","text":"<p>For more control over date processing, use the <code>DateFeature</code> class:</p> <pre><code>from kdp.features import DateFeature\n\nfeatures = {\n    # Transaction date with component extraction\n    \"transaction_date\": DateFeature(\n        name=\"transaction_date\",\n        feature_type=FeatureType.DATE,\n        add_day_of_week=True,      # Extract day of week\n        add_month=True,            # Extract month\n        add_quarter=True,          # Extract quarter\n        cyclical_encoding=True     # Use sine/cosine encoding for cyclical features\n    ),\n\n    # User signup date with time since reference\n    \"signup_date\": DateFeature(\n        name=\"signup_date\",\n        feature_type=FeatureType.DATE,\n        add_time_since_reference=True,\n        reference_date=\"2020-01-01\"  # Reference point\n    ),\n\n    # Event timestamp with hour component\n    \"event_timestamp\": DateFeature(\n        name=\"event_timestamp\",\n        feature_type=FeatureType.DATE,\n        add_hour=True,             # Extract hour\n        add_day_of_week=True,      # Extract day of week\n        add_is_weekend=True        # Add weekend indicator\n    )\n}\n</code></pre>"},{"location":"features/date-features.html#key-configuration-parameters","title":"\u2699\ufe0f Key Configuration Parameters","text":"Parameter Description Default Options <code>add_year</code> Extract year component False Boolean <code>add_month</code> Extract month component False Boolean <code>add_day</code> Extract day component False Boolean <code>add_day_of_week</code> Extract day of week False Boolean <code>add_hour</code> Extract hour component False Boolean <code>cyclical_encoding</code> Use sine/cosine encoding False Boolean <code>add_is_weekend</code> Add weekend indicator False Boolean"},{"location":"features/date-features.html#powerful-features","title":"\ud83d\udca1 Powerful Features","text":"\ud83d\udd04 Cyclical Encoding <p>Properly represent cyclical time components (like hour, day of week) using sine/cosine transformations:</p> <pre><code># Configure cyclical encoding for time components\ndate_feature = DateFeature(\n    name=\"event_time\",\n    feature_type=FeatureType.DATE,\n    add_hour=True,\n    add_day_of_week=True,\n    cyclical_encoding=True  # Enable cyclical encoding\n)\n\n# Create preprocessor with cyclical date features\npreprocessor = PreprocessingModel(\n    path_data=\"events.csv\",\n    features_specs={\"event_time\": date_feature}\n)\n</code></pre> \ud83d\udccf Time-Since Features <p>Calculate time since reference points for meaningful temporal distances:</p> <pre><code># Compute days since reference date\ndate_feature = DateFeature(\n    name=\"signup_date\",\n    feature_type=FeatureType.DATE,\n    add_time_since_reference=True,\n    reference_date=\"2020-01-01\",     # Fixed reference\n    time_since_unit=\"days\"           # Unit for calculation\n)\n\n# Compute time since multiple references\npreprocessor = PreprocessingModel(\n    path_data=\"user_data.csv\",\n    features_specs={\n        \"signup_date\": date_feature,\n        \"last_purchase\": DateFeature(\n            name=\"last_purchase\",\n            add_time_since_reference=True,\n            reference_date=\"today\",  # Dynamic reference (current date)\n            time_since_unit=\"days\"\n        )\n    }\n)\n</code></pre>"},{"location":"features/date-features.html#real-world-examples","title":"\ud83d\udd27 Real-World Examples","text":"E-commerce Purchase Analysis <pre><code># Analyze purchase patterns over time\nfrom kdp.features import DateFeature, NumericalFeature, CategoricalFeature\n\npreprocessor = PreprocessingModel(\n    path_data=\"ecommerce_data.csv\",\n    features_specs={\n        # Purchase date with rich time components\n        \"purchase_date\": DateFeature(\n            name=\"purchase_date\",\n            add_day_of_week=True,\n            add_hour=True,\n            add_month=True,\n            add_is_weekend=True,\n            cyclical_encoding=True\n        ),\n\n        # User signup date to determine user tenure\n        \"user_signup_date\": DateFeature(\n            name=\"user_signup_date\",\n            add_time_since_reference=True,\n            reference_date=\"today\",\n            time_since_unit=\"days\"\n        ),\n\n        # Additional features\n        \"product_category\": CategoricalFeature(\n            name=\"product_category\",\n            feature_type=FeatureType.STRING_CATEGORICAL\n        ),\n        \"purchase_amount\": NumericalFeature(\n            name=\"purchase_amount\",\n            feature_type=FeatureType.FLOAT_RESCALED\n        )\n    },\n\n    # Define crosses to capture time-based patterns\n    feature_crosses=[\n        (\"purchase_date_day_of_week\", \"product_category\", 16)\n    ]\n)\n</code></pre> Time Series Forecasting <pre><code># Time series feature extraction for forecasting\npreprocessor = PreprocessingModel(\n    path_data=\"sensor_readings.csv\",\n    features_specs={\n        # Timestamp with multiple components\n        \"timestamp\": DateFeature(\n            name=\"timestamp\",\n            add_year=True,\n            add_month=True,\n            add_day=True,\n            add_hour=True,\n            add_day_of_week=True,\n            cyclical_encoding=True\n        ),\n\n        # Numerical features to predict\n        \"value\": NumericalFeature(\n            name=\"value\",\n            feature_type=FeatureType.FLOAT_RESCALED,\n            use_distribution_aware=True\n        ),\n\n        # Additional context features\n        \"sensor_id\": CategoricalFeature(\n            name=\"sensor_id\",\n            feature_type=FeatureType.STRING_CATEGORICAL\n        )\n    },\n\n    # Enable tabular attention for discovering temporal patterns\n    tabular_attention=True\n)\n</code></pre>"},{"location":"features/date-features.html#pro-tips","title":"\ud83d\udc8e Pro Tips","text":"\ud83d\udd0d Date Format Handling <p>KDP automatically handles common date formats, but you can specify custom formats:</p> <pre><code># Handle custom date formats\nfrom datetime import datetime\nimport pandas as pd\n\n# Convert dates to standard format before feeding to KDP\ndef standardize_date(date_str):\n    try:\n        # Try parsing custom format\n        dt = datetime.strptime(date_str, \"%d-%b-%Y\")\n        return dt.strftime(\"%Y-%m-%d\")\n    except:\n        return date_str\n\n# Apply standardization to your data\ndata = pd.read_csv(\"custom_dates.csv\")\ndata[\"standard_date\"] = data[\"custom_date\"].apply(standardize_date)\n\n# Use standardized dates in KDP\npreprocessor = PreprocessingModel(\n    path_data=data,\n    features_specs={\"standard_date\": FeatureType.DATE}\n)\n</code></pre> \ud83e\udde0 Feature Selection <p>Use feature selection to identify important temporal patterns:</p> <pre><code># Determine which date components matter most\npreprocessor = PreprocessingModel(\n    path_data=\"events.csv\",\n    features_specs={\n        \"event_date\": DateFeature(\n            name=\"event_date\",\n            # Extract all potentially relevant components\n            add_year=True,\n            add_quarter=True,\n            add_month=True,\n            add_day=True,\n            add_day_of_week=True,\n            add_hour=True,\n            add_is_weekend=True\n        )\n    },\n    # Enable feature selection to identify important components\n    use_feature_selection=True,\n    feature_selection_strategy=\"gradient_based\"\n)\n\n# After training, check feature importance\nresult = preprocessor.build_preprocessor()\nimportance = result[\"feature_importance\"]\nprint(\"Most important date components:\", importance)\n</code></pre> \u2795 Cross Features <p>Create crosses with date components to capture context-dependent patterns:</p> <pre><code># Cross date components with categorical features\npreprocessor = PreprocessingModel(\n    path_data=\"transactions.csv\",\n    features_specs={\n        # Date with components\n        \"transaction_date\": DateFeature(\n            name=\"transaction_date\",\n            add_day_of_week=True,\n            add_hour=True,\n            add_is_weekend=True\n        ),\n\n        # Categorical context\n        \"store_location\": FeatureType.STRING_CATEGORICAL,\n        \"product_category\": FeatureType.STRING_CATEGORICAL\n    },\n\n    # Define crosses to capture contextual patterns\n    feature_crosses=[\n        # Weekend shopping differs by location\n        (\"transaction_date_is_weekend\", \"store_location\", 16),\n\n        # Day of week impacts product category popularity\n        (\"transaction_date_day_of_week\", \"product_category\", 32),\n\n        # Hour of day impacts product selections\n        (\"transaction_date_hour\", \"product_category\", 32)\n    ]\n)\n</code></pre> \ud83c\udf0d Handling Timezones <p>Standardize timezone handling for consistent date processing:</p> <pre><code># Standardize timezones before processing\nimport pandas as pd\nfrom datetime import datetime\nimport pytz\n\n# Convert timestamps to a standard timezone\ndef standardize_timezone(timestamp_str, from_tz='UTC', to_tz='America/New_York'):\n    if pd.isna(timestamp_str):\n        return None\n\n    # Parse timestamp and set timezone\n    dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))\n    if dt.tzinfo is None:\n        dt = pytz.timezone(from_tz).localize(dt)\n\n    # Convert to target timezone\n    dt = dt.astimezone(pytz.timezone(to_tz))\n    return dt.isoformat()\n\n# Apply timezone standardization\ndata = pd.read_csv(\"global_events.csv\")\ndata[\"standardized_time\"] = data[\"event_timestamp\"].apply(\n    lambda x: standardize_timezone(x, from_tz='UTC', to_tz='America/New_York')\n)\n\n# Use standardized timestamps in KDP\npreprocessor = PreprocessingModel(\n    path_data=data,\n    features_specs={\"standardized_time\": FeatureType.DATE}\n)\n</code></pre>"},{"location":"features/date-features.html#model-architecture","title":"\ud83d\udcca Model Architecture","text":"graph TD       A[Raw Date Data] --&gt;|Parsing| B[Date Components]       B --&gt;|Cyclical Encoding| C1[Sine/Cosine Components]       B --&gt;|Direct Encoding| C2[Normalized Components]       B --&gt;|Reference Distance| C3[Time Since Features]        C1 --&gt; D[Date Representation]       C2 --&gt; D       C3 --&gt; D        style A fill:#f9f9f9,stroke:#ccc,stroke-width:2px       style B fill:#e3f2fd,stroke:#64b5f6,stroke-width:2px       style C1 fill:#e8f5e9,stroke:#66bb6a,stroke-width:2px       style C2 fill:#fff8e1,stroke:#ffd54f,stroke-width:2px       style C3 fill:#f3e5f5,stroke:#ce93d8,stroke-width:2px       style D fill:#e8eaf6,stroke:#7986cb,stroke-width:2px    <p>KDP processes dates by extracting components, applying appropriate transformations, and then combining them into a unified representation that captures temporal patterns.</p>"},{"location":"features/date-features.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"\ud83d\udd22 Numerical Features \u2795 Cross Features \ud83d\udc41\ufe0f Tabular Attention \ud83c\udfaf Feature Selection \u2190 Text Features Cross Features \u2192"},{"location":"features/numerical-features.html","title":"\ud83d\udd22 Numerical Features","text":"Transform your continuous data like age, income, or prices into powerful feature representations"},{"location":"features/numerical-features.html#quick-overview","title":"\ud83d\udccb Quick Overview","text":"<p>Numerical features are the backbone of most machine learning models. KDP provides multiple ways to handle them, from simple normalization to advanced neural embeddings.</p>"},{"location":"features/numerical-features.html#types-and-use-cases","title":"\ud83c\udfaf Types and Use Cases","text":"Feature Type Best For Example Values When to Use <code>FLOAT_NORMALIZED</code> Data with clear bounds \ud83e\uddd3 Age: 18-65, \u2b50 Score: 0-100 When you know your data falls in a specific range <code>FLOAT_RESCALED</code> Unbounded, varied data \ud83d\udcb0 Income: $0-$1M+, \ud83d\udcca Revenue When data has outliers or unknown bounds <code>FLOAT_DISCRETIZED</code> Values that form groups \ud83d\udcc5 Years: 1-50, \u2b50 Ratings: 1-5 When groups of values have special meaning <code>FLOAT</code> Default normalization \ud83d\udd22 General numeric values When you want standard normalization (identical to FLOAT_NORMALIZED)"},{"location":"features/numerical-features.html#basic-usage","title":"\ud83d\ude80 Basic Usage","text":"<p>The simplest way to define numerical features is with the <code>FeatureType</code> enum:</p> <pre><code>from kdp import PreprocessingModel, FeatureType\n\n# \u2728 Quick numerical feature definition\nfeatures = {\n    \"age\": FeatureType.FLOAT_NORMALIZED,          # \ud83e\uddd3 Age gets 0-1 normalization\n    \"income\": FeatureType.FLOAT_RESCALED,         # \ud83d\udcb0 Income gets robust scaling\n    \"transaction_count\": FeatureType.FLOAT,       # \ud83d\udd22 Default normalization\n    \"rating\": FeatureType.FLOAT_DISCRETIZED       # \u2b50 Discretized into bins\n}\n\n# \ud83c\udfd7\ufe0f Create your preprocessor\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",\n    features_specs=features\n)\n</code></pre>"},{"location":"features/numerical-features.html#advanced-configuration","title":"\ud83e\udde0 Advanced Configuration","text":"<p>For more control, use the <code>NumericalFeature</code> class:</p> <pre><code>from kdp.features import NumericalFeature\n\nfeatures = {\n    # \ud83e\uddd3 Simple example with enhanced configuration\n    \"age\": NumericalFeature(\n        name=\"age\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        use_embedding=True,                 # \ud83d\udd04 Create neural embeddings\n        embedding_dim=16,                   # \ud83d\udccf Size of embedding\n        preferred_distribution=\"normal\"      # \ud83d\udcca Hint about distribution\n    ),\n\n    # \ud83d\udcb0 Financial data example\n    \"transaction_amount\": NumericalFeature(\n        name=\"transaction_amount\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        use_embedding=True,\n        embedding_dim=32,\n        preferred_distribution=\"heavy_tailed\"\n    ),\n\n    # \u23f3 Custom binning example\n    \"years_experience\": NumericalFeature(\n        name=\"years_experience\",\n        feature_type=FeatureType.FLOAT_DISCRETIZED,\n        num_bins=5                          # \ud83d\udccf Number of bins\n    )\n}\n</code></pre>"},{"location":"features/numerical-features.html#key-configuration-options","title":"\u2699\ufe0f Key Configuration Options","text":"Parameter Description Default Suggested Range <code>feature_type</code> \ud83c\udff7\ufe0f Base feature type <code>FLOAT_NORMALIZED</code> Choose from 4 types <code>use_embedding</code> \ud83e\udde0 Enable neural embeddings <code>False</code> <code>True</code>/<code>False</code> <code>embedding_dim</code> \ud83d\udccf Dimensionality of embedding 8 4-64 <code>preferred_distribution</code> \ud83d\udcca Hint about data distribution <code>None</code> \"normal\", \"log_normal\", etc. <code>num_bins</code> \ud83d\udd22 Bins for discretization 10 5-100"},{"location":"features/numerical-features.html#power-features","title":"\ud83d\udd25 Power Features","text":"\ud83d\udcca Distribution-Aware Processing <p>Let KDP automatically detect and handle distributions:</p> <pre><code># \u2728 Enable distribution-aware processing for all numerical features\npreprocessor = PreprocessingModel(\n    features_specs=features,\n    use_distribution_aware=True      # \ud83d\udd0d Enable distribution detection\n)\n</code></pre> \ud83e\udde0 Advanced Numerical Embeddings <p>Using advanced numerical embeddings:</p> <pre><code># Configure numerical embeddings\npreprocessor = PreprocessingModel(\n    features_specs={\n        \"income\": NumericalFeature(\n            name=\"income\",\n            feature_type=FeatureType.FLOAT_RESCALED,\n            use_embedding=True,\n            embedding_dim=32,\n            preferred_distribution=\"log_normal\"\n        )\n    }\n)\n</code></pre>"},{"location":"features/numerical-features.html#real-world-examples","title":"\ud83d\udcbc Real-World Examples","text":"\ud83d\udcb0 Financial Analysis <pre><code># \ud83d\udcc8 Financial metrics with appropriate processing\npreprocessor = PreprocessingModel(\n    features_specs={\n        \"income\": NumericalFeature(\n            name=\"income\",\n            feature_type=FeatureType.FLOAT_RESCALED,\n            preferred_distribution=\"log_normal\"   # \ud83d\udcc9 Log-normal distribution\n        ),\n        \"credit_score\": NumericalFeature(\n            name=\"credit_score\",\n            feature_type=FeatureType.FLOAT_NORMALIZED,\n            use_embedding=True,\n            embedding_dim=16\n        ),\n        \"debt_ratio\": NumericalFeature(\n            name=\"debt_ratio\",\n            feature_type=FeatureType.FLOAT_NORMALIZED,\n            preferred_distribution=\"bounded\"      # \ud83d\udcca Bounded between 0 and 1\n        )\n    },\n    use_distribution_aware=True                   # \ud83e\udde0 Smart distribution handling\n)\n</code></pre> \ud83d\udd0c Sensor Data <pre><code># \ud83d\udce1 Processing sensor readings\npreprocessor = PreprocessingModel(\n    features_specs={\n        \"temperature\": NumericalFeature(\n            name=\"temperature\",\n            feature_type=FeatureType.FLOAT_RESCALED,\n            use_embedding=True,\n            embedding_dim=16\n        ),\n        \"humidity\": NumericalFeature(\n            name=\"humidity\",\n            feature_type=FeatureType.FLOAT_NORMALIZED,\n            preferred_distribution=\"bounded\"      # \ud83d\udca7 Bounded between 0 and 100\n        ),\n        \"pressure\": NumericalFeature(\n            name=\"pressure\",\n            feature_type=FeatureType.FLOAT_RESCALED,\n            use_embedding=True,\n            embedding_dim=16\n        )\n    }\n)\n</code></pre>"},{"location":"features/numerical-features.html#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"\ud83d\udcca Understand Your Data Distribution <ul> <li>Use <code>FLOAT_NORMALIZED</code> when your data has clear bounds (e.g., 0-100%)</li> <li>Use <code>FLOAT_RESCALED</code> when your data has outliers (e.g., income, prices)</li> <li>Use <code>FLOAT_DISCRETIZED</code> when your values naturally form groups (e.g., age groups)</li> </ul> \ud83e\udde0 Consider Neural Embeddings for Complex Relationships <ul> <li>Enable when a simple scaling doesn't capture the pattern</li> <li>Increase embedding dimensions for more complex patterns (16\u219232\u219264)</li> </ul> \ud83d\udd0d Let KDP Handle Distribution Detection <ul> <li>Enable <code>use_distribution_aware=True</code> and let KDP automatically choose</li> <li>This is especially important for skewed or multi-modal distributions</li> </ul> \ud83d\udccf Custom Bin Boundaries <ul> <li>Use <code>num_bins</code> parameter to control discretization granularity</li> <li>More bins = finer granularity but more parameters to learn</li> </ul>"},{"location":"features/numerical-features.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"\ud83d\udcca Distribution-Aware Encoding <p>Smart numerical handling</p> \ud83e\udde0 Advanced Numerical Embeddings <p>Neural representations</p> \ud83c\udfaf Feature Selection <p>Finding important features</p>"},{"location":"features/numerical-features.html#types-of-numerical-features","title":"\ud83e\uddee Types of Numerical Features","text":"<p>KDP supports different types of numerical features, each with specialized processing:</p> \ud83d\udd04 FLOAT <p>Basic floating-point features with default normalization</p> \ud83d\udccf FLOAT_NORMALIZED <p>Values normalized to the [0,1] range using min-max scaling</p> \u2696\ufe0f FLOAT_RESCALED <p>Values rescaled using standardization (mean=0, std=1)</p> \ud83d\udcca FLOAT_DISCRETIZED <p>Continuous values binned into discrete buckets</p>"},{"location":"features/numerical-features.html#architecture-diagrams","title":"\ud83d\udcca Architecture Diagrams","text":"\ud83d\udccf Normalized Numerical Feature <p>Below is a visualization of a model with a normalized numerical feature:</p> \u2696\ufe0f Rescaled Numerical Feature <p>Below is a visualization of a model with a rescaled numerical feature:</p> \ud83d\udcca Discretized Numerical Feature <p>Below is a visualization of a model with a discretized numerical feature:</p> \ud83e\udde0 Advanced Numerical Embeddings <p>When using advanced numerical embeddings, the model architecture looks like this:</p> \u2190 Feature Overview Categorical Features \u2192"},{"location":"features/overview.html","title":"\ud83d\udee0\ufe0f Feature Types Overview","text":"Making Data ML-Ready <p>KDP makes feature processing intuitive and powerful by transforming your raw data into the optimal format for machine learning.</p>"},{"location":"features/overview.html#feature-types-at-a-glance","title":"\ud83d\udcaa Feature Types at a Glance","text":"Feature Type What It's For Processing Magic \ud83d\udd22 Numerical Continuous values like age, income, scores Normalization, scaling, embeddings, distribution analysis \ud83c\udff7\ufe0f Categorical Discrete values like occupation, product type Embeddings, one-hot encoding, vocabulary management \ud83d\udcdd Text Free-form text like reviews, descriptions Tokenization, embeddings, sequence handling \ud83d\udcc5 Date Temporal data like signup dates, transactions Component extraction, cyclical encoding, seasonality \u2795 Cross Features Feature interactions Combined embeddings, interaction modeling \ud83d\udd0d Passthrough Pre-processed data, custom vectors No modification, type casting only"},{"location":"features/overview.html#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>The simplest way to define features is with the <code>FeatureType</code> enum:</p> <pre><code>from kdp import PreprocessingModel, FeatureType\n\n# \u2728 Quick and easy feature definition\nfeatures = {\n    # \ud83d\udd22 Numerical features - different processing strategies\n    \"age\": FeatureType.FLOAT_NORMALIZED,        # \ud83d\udcca [0,1] range normalization\n    \"income\": FeatureType.FLOAT_RESCALED,       # \ud83d\udcc8 Standard scaling\n    \"transaction_count\": FeatureType.FLOAT,     # \ud83e\uddee Default normalization (same as FLOAT_NORMALIZED)\n\n    # \ud83c\udff7\ufe0f Categorical features - automatic encoding\n    \"occupation\": FeatureType.STRING_CATEGORICAL,      # \ud83d\udc54 Job titles, roles\n    \"education_level\": FeatureType.INTEGER_CATEGORICAL, # \ud83c\udf93 Education codes\n\n    # \ud83d\udcdd Text and dates - specialized processing\n    \"product_review\": FeatureType.TEXT,         # \ud83d\udcac Customer feedback\n    \"signup_date\": FeatureType.DATE,            # \ud83d\udcc6 User registration date\n\n    # \ud83d\udd0d Passthrough feature - use without any processing\n    \"embedding_vector\": FeatureType.PASSTHROUGH # \ud83d\udd04 Pre-processed data passes directly to output\n}\n\n# \ud83c\udfd7\ufe0f Create your preprocessor\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",\n    features_specs=features\n)\n</code></pre>"},{"location":"features/overview.html#why-strong-feature-types-matter","title":"\u2b50 Why Strong Feature Types Matter","text":"\ud83c\udfaf Optimized Processing <p>Each feature type gets specialized handling for better ML performance</p> \ud83d\udc1b Reduced Errors <p>Catch type mismatches early in development, not during training</p> \ud83d\udcdd Clearer Code <p>Self-documenting feature definitions make your code more maintainable</p> \u26a1 Enhanced Performance <p>Type-specific optimizations improve preprocessing speed</p>"},{"location":"features/overview.html#feature-type-documentation","title":"\ud83d\udcda Feature Type Documentation","text":"\ud83d\udd22 Numerical Features <p>Handle continuous values with advanced normalization and distribution-aware processing</p> \ud83c\udff7\ufe0f Categorical Features <p>Process discrete categories with smart embedding techniques and vocabulary management</p> \ud83d\udcdd Text Features <p>Work with free-form text using tokenization, embeddings, and sequence handling</p> \ud83d\udcc5 Date Features <p>Extract temporal patterns from dates with component extraction and cyclical encoding</p> \u2795 Cross Features <p>Model feature interactions with combined embeddings and interaction modeling</p> \ud83d\udd0d Passthrough Features <p>Include unmodified data or pre-computed features directly in your model</p>"},{"location":"features/overview.html#advanced-feature-configuration","title":"\ud83d\udc68\u200d\ud83d\udcbb Advanced Feature Configuration","text":"<p>For more control, use specialized feature classes:</p> <pre><code>from kdp.features import NumericalFeature, CategoricalFeature, TextFeature, DateFeature, PassthroughFeature\nimport tensorflow as tf\n\n# \ud83d\udd27 Advanced feature configuration\nfeatures = {\n    # \ud83d\udcb0 Numerical with advanced embedding\n    \"income\": NumericalFeature(\n        name=\"income\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        use_embedding=True,\n        embedding_dim=32\n    ),\n\n    # \ud83c\udfea Categorical with hashing\n    \"product_id\": CategoricalFeature(\n        name=\"product_id\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        max_tokens=10000,\n        category_encoding=\"hashing\"\n    ),\n\n    # \ud83d\udccb Text with custom tokenization\n    \"description\": TextFeature(\n        name=\"description\",\n        max_tokens=5000,\n        embedding_dim=64,\n        sequence_length=128,\n        ngrams=2\n    ),\n\n    # \ud83d\uddd3\ufe0f Date with cyclical encoding\n    \"purchase_date\": DateFeature(\n        name=\"purchase_date\",\n        add_day_of_week=True,\n        add_month=True,\n        cyclical_encoding=True\n    ),\n\n    # \ud83e\udde0 Passthrough feature\n    \"embedding\": PassthroughFeature(\n        name=\"embedding\",\n        dtype=tf.float32\n    )\n}\n</code></pre>"},{"location":"features/overview.html#pro-tips-for-feature-definition","title":"\ud83d\udca1 Pro Tips for Feature Definition","text":"1 Start Simple <p>Begin with basic <code>FeatureType</code> definitions</p> 2 Add Complexity Gradually <p>Refactor to specialized feature classes when needed</p> 3 Combine Approaches <p>Mix distribution-aware, attention, embeddings for best results</p> 4 Check Distributions <p>Review your data distribution before choosing feature types</p> 5 Experiment with Types <p>Sometimes a different encoding provides better results</p> 6 Consider Passthrough <p>Use passthrough features for pre-processed data or custom vectors</p>"},{"location":"features/overview.html#model-architecture-diagrams","title":"\ud83d\udcca Model Architecture Diagrams","text":"<p>KDP creates optimized preprocessing architectures based on your feature definitions. Here are examples of different model configurations:</p> \ud83d\udd04 Basic Feature Combinations <p>When combining numerical and categorical features:</p> \ud83c\udf1f All Feature Types Combined <p>KDP can handle all feature types in a single model:</p> \ud83d\udd0b Advanced Configurations \u2728 Tabular Attention <p>Enhance feature interactions with tabular attention:</p> \ud83d\udd04 Transformer Blocks <p>Process categorical features with transformer blocks:</p> \ud83e\udde0 Feature MoE (Mixture of Experts) <p>Specialized feature processing with Mixture of Experts:</p> \ud83d\udce4 Output Modes <p>KDP supports different output modes for your preprocessed features:</p> \ud83d\udd17 Concatenated Output \ud83d\udce6 Dictionary Output \u2190 Architecture Numerical Features \u2192"},{"location":"features/passthrough-features.html","title":"\ud83d\udd0d Passthrough Features","text":"Passthrough Features in KDP <p>Include pre-processed data and custom vectors in your model without additional transformations.</p>"},{"location":"features/passthrough-features.html#overview","title":"\ud83d\udccb Overview","text":"<p>Passthrough features allow you to include data in your model without any preprocessing modifications. They're perfect for pre-processed data, custom vectors, and scenarios where you need to preserve exact values.</p> \u26a1 Direct Integration <p>Include pre-processed data without modifications</p> \ud83d\udd22 Custom Vectors <p>Use pre-computed embeddings and vectors</p> \ud83d\udcca Raw Values <p>Preserve exact original values in your model</p> \ud83d\udd04 Flexible Integration <p>Seamlessly combine with other feature types</p>"},{"location":"features/passthrough-features.html#when-to-use-passthrough-features","title":"\ud83d\ude80 When to Use Passthrough Features","text":"\ud83d\udd04 Pre-processed Data <p>You have already processed the data externally</p> \ud83d\udd22 Custom Vectors <p>You want to include pre-computed embeddings or vectors</p> \ud83d\udcca Raw Values <p>You need the exact original values in your model</p> \ud83d\udd0d Feature Testing <p>You want to compare raw vs processed feature performance</p> \ud83d\udd04 Gradual Migration <p>You're moving from a legacy system and need compatibility</p>"},{"location":"features/passthrough-features.html#defining-passthrough-features","title":"\ud83d\udca1 Defining Passthrough Features","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\nfrom kdp.features import PassthroughFeature\nimport tensorflow as tf\n\n# Simple approach using enum\nfeatures = {\n    \"embedding_vector\": FeatureType.PASSTHROUGH,\n    \"age\": FeatureType.FLOAT_NORMALIZED,\n    \"category\": FeatureType.STRING_CATEGORICAL\n}\n\n# Advanced configuration with PassthroughFeature class\nfeatures = {\n    \"embedding_vector\": PassthroughFeature(\n        name=\"embedding_vector\",\n        dtype=tf.float32  # Specify the data type\n    ),\n    \"raw_text_embedding\": PassthroughFeature(\n        name=\"raw_text_embedding\",\n        dtype=tf.float32\n    ),\n    \"age\": FeatureType.FLOAT_NORMALIZED,\n    \"category\": FeatureType.STRING_CATEGORICAL\n}\n\n# Create your preprocessor\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",\n    features_specs=features\n)\n</code></pre>"},{"location":"features/passthrough-features.html#how-passthrough-features-work","title":"\ud83d\udcca How Passthrough Features Work","text":"<p>Passthrough features are included in model inputs without any transformations, maintaining their original values throughout the pipeline.</p> \u2795 Added to Inputs <p>Included in model inputs like other features</p> \ud83d\udd04 Type Casting <p>Cast to specified dtype for compatibility</p> \u26a1 No Transformation <p>Pass through without normalization or encoding</p> \ud83c\udfaf Feature Selection <p>Optional feature selection if enabled</p>"},{"location":"features/passthrough-features.html#configuration-options","title":"\ud83d\udd27 Configuration Options","text":"Parameter Type Description <code>name</code> str The name of the feature <code>feature_type</code> FeatureType Set to <code>FeatureType.PASSTHROUGH</code> by default <code>dtype</code> tf.DType The data type of the feature (default: tf.float32)"},{"location":"features/passthrough-features.html#example-using-pre-computed-embeddings","title":"\ud83c\udfaf Example: Using Pre-computed Embeddings","text":"<pre><code>import pandas as pd\nfrom kdp import PreprocessingModel, FeatureType\nfrom kdp.features import PassthroughFeature, NumericalFeature\nimport tensorflow as tf\n\n# Define features\nfeatures = {\n    # Regular features\n    \"age\": NumericalFeature(\n        name=\"age\",\n        feature_type=FeatureType.FLOAT_NORMALIZED\n    ),\n    \"category\": FeatureType.STRING_CATEGORICAL,\n\n    # Passthrough features for pre-computed embeddings\n    \"product_embedding\": PassthroughFeature(\n        name=\"product_embedding\",\n        dtype=tf.float32\n    )\n}\n\n# Create your preprocessor\npreprocessor = PreprocessingModel(\n    path_data=\"data.csv\",\n    features_specs=features\n)\n\n# Build the model\nmodel = preprocessor.build_preprocessor()\n</code></pre>"},{"location":"features/passthrough-features.html#things-to-consider","title":"\u26a0\ufe0f Things to Consider","text":"\ud83d\udd0d Data Type Compatibility <p>Ensure the data type of your passthrough feature is compatible with the overall model</p> \ud83d\udcca Dimensionality <p>Make sure the feature dimensions fit your model architecture</p> \ud83e\uddf9 Data Quality <p>Since no preprocessing is applied, ensure your data is clean and ready for use</p> \u26a1 Performance Impact <p>Using raw data may affect model performance; test both approaches</p>"},{"location":"features/passthrough-features.html#best-practices","title":"\ud83d\ude80 Best Practices","text":"\ud83d\udcdd Document Your Decision <p>Make it clear why certain features are passed through</p> \ud83d\udd0d Test Both Approaches <p>Compare passthrough vs preprocessed features for performance</p> \ud83c\udfaf Feature Importance <p>Use feature selection to see if passthrough features contribute meaningfully</p> \ud83d\udcc8 Monitor Gradients <p>Watch for gradient issues since passthrough features may have different scales</p> \u2190 Cross Features Optimization \u2192"},{"location":"features/text-features.html","title":"\ud83d\udcdd Text Features","text":"Text Features in KDP <p>Transform textual data into meaningful features with advanced text processing techniques.</p>"},{"location":"features/text-features.html#overview","title":"\ud83d\udccb Overview","text":"<p>Text features represent natural language data like product descriptions, user reviews, comments, and other forms of unstructured text. KDP provides powerful tools to convert raw text into compact, meaningful representations that capture semantic meaning and context.</p>"},{"location":"features/text-features.html#text-processing-approaches","title":"\ud83d\ude80 Text Processing Approaches","text":"\ud83d\udd24 Tokenization <p>Breaking text into words, subwords, or characters</p> \ud83e\uddee Vectorization <p>Converting tokens into numerical representations</p> \ud83d\udd0d Embeddings <p>Mapping tokens to dense vector spaces that capture semantics</p> \ud83d\udccf Sequence Handling <p>Managing variable-length text with padding, truncation</p>"},{"location":"features/text-features.html#basic-usage","title":"\ud83d\udcdd Basic Usage","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Define text features with simple configuration\nfeatures = {\n    \"product_description\": FeatureType.TEXT,\n    \"user_review\": FeatureType.TEXT,\n    \"comment\": FeatureType.TEXT\n}\n\n# Create preprocessor\npreprocessor = PreprocessingModel(\n    path_data=\"text_data.csv\",\n    features_specs=features\n)\n</code></pre>"},{"location":"features/text-features.html#advanced-configuration","title":"\ud83e\udde0 Advanced Configuration","text":"<p>For more control over text processing, use the <code>TextFeature</code> class:</p> <pre><code>from kdp import PreprocessingModel, FeatureType, TextFeature\n\n# Detailed text feature configuration\nfeatures = {\n    # Basic text feature\n    \"short_comment\": FeatureType.TEXT,\n\n    # Full configuration with TextFeature\n    \"product_description\": TextFeature(\n        name=\"product_description\",\n        max_tokens=10000,               # Vocabulary size\n        embedding_dim=64,               # Embedding dimensionality\n        sequence_length=128,            # Max sequence length\n        tokenizer=\"word\",               # Tokenization strategy\n        ngrams=2,                       # Include bigrams\n        output_mode=\"embedding\"         # Return embeddings\n    ),\n\n    # Text feature with pre-trained embeddings\n    \"user_query\": TextFeature(\n        name=\"user_query\",\n        use_pretrained=True,            # Use pre-trained embeddings\n        pretrained_name=\"glove.6B.100d\",# GloVe embeddings\n        trainable=False                 # Freeze embeddings during training\n    ),\n\n    # Multilingual text processing\n    \"multilingual_text\": TextFeature(\n        name=\"multilingual_text\",\n        use_pretrained=True,\n        pretrained_name=\"multilingual\", # Multilingual embeddings\n        max_sequence_length=256\n    )\n}\n\npreprocessor = PreprocessingModel(\n    path_data=\"text_data.csv\",\n    features_specs=features\n)\n</code></pre>"},{"location":"features/text-features.html#key-configuration-parameters","title":"\u2699\ufe0f Key Configuration Parameters","text":"Parameter Description Default Options <code>max_tokens</code> Maximum vocabulary size 10000 Typically 5K-50K for most applications <code>sequence_length</code> Maximum sequence length 64 Shorter for queries (32-64), longer for documents (128-512) <code>embedding_dim</code> Size of embedding vectors 32 16-300 depending on complexity of text <code>tokenizer</code> Tokenization strategy \"word\" \"word\", \"char\", \"subword\" <code>output_mode</code> Text representation format \"embedding\" \"embedding\", \"int\", \"binary\", \"tfidf\" <code>ngrams</code> Include n-grams in tokenization 1 1 (unigrams only), 2 (uni+bigrams), 3 (uni+bi+trigrams)"},{"location":"features/text-features.html#powerful-features","title":"\ud83d\udca1 Powerful Features","text":"\ud83c\udf10 Pre-trained Embeddings <p>KDP supports several pre-trained embeddings to jump-start your text processing:</p> <pre><code># Using GloVe embeddings\ntext_feature = TextFeature(\n    name=\"article_text\",\n    use_pretrained=True,\n    pretrained_name=\"glove.6B.100d\",\n    trainable=False  # Freeze embeddings\n)\n\n# Using Word2Vec embeddings\ntext_feature = TextFeature(\n    name=\"article_text\",\n    use_pretrained=True,\n    pretrained_name=\"word2vec.google.300d\",\n    trainable=True  # Fine-tune embeddings\n)\n\n# Using BERT embeddings for contextual representations\ntext_feature = TextFeature(\n    name=\"article_text\",\n    use_pretrained=True,\n    pretrained_name=\"bert-base-uncased\",\n    use_attention=True  # Enable attention mechanism\n)\n</code></pre> \ud83d\udd04 Attention Mechanisms <p>Enable attention to better capture the context and important parts of text:</p> <pre><code># Text feature with self-attention\ntext_feature = TextFeature(\n    name=\"long_document\",\n    sequence_length=512,\n    use_attention=True,            # Enable attention\n    attention_heads=8,             # Multi-head attention\n    attention_dropout=0.1          # Regularization\n)\n\n# Create a preprocessor with text attention\npreprocessor = PreprocessingModel(\n    path_data=\"documents.csv\",\n    features_specs={\"document\": text_feature},\n    text_attention_mode=\"self\"     # Self-attention mode\n)\n</code></pre>"},{"location":"features/text-features.html#real-world-examples","title":"\ud83d\udd27 Real-World Examples","text":"Sentiment Analysis from Product Reviews <pre><code># Text preprocessing for sentiment analysis\npreprocessor = PreprocessingModel(\n    path_data=\"reviews.csv\",\n    features_specs={\n        # Review text with attention for key sentiment phrases\n        \"review_text\": TextFeature(\n            name=\"review_text\",\n            max_tokens=15000,\n            embedding_dim=64,\n            use_attention=True,\n            attention_heads=4\n        ),\n\n        # Additional metadata features\n        \"product_category\": FeatureType.STRING_CATEGORICAL,\n        \"star_rating\": FeatureType.FLOAT_NORMALIZED,\n        \"verified_purchase\": FeatureType.BOOLEAN\n    },\n    tabular_attention=True  # Enable attention across all features\n)\n</code></pre> Document Classification System <pre><code># Document classification preprocessor\npreprocessor = PreprocessingModel(\n    path_data=\"documents.csv\",\n    features_specs={\n        # Main document text\n        \"document_text\": TextFeature(\n            name=\"document_text\",\n            max_tokens=20000,\n            sequence_length=256,\n            embedding_dim=128,\n            tokenizer=\"subword\",  # Better for rare words\n            ngrams=3              # Include n-grams\n        ),\n\n        # Document metadata\n        \"document_title\": TextFeature(\n            name=\"document_title\",\n            max_tokens=5000,\n            sequence_length=32\n        ),\n        \"author\": FeatureType.STRING_CATEGORICAL,\n        \"publication_date\": FeatureType.DATE\n    }\n)\n</code></pre>"},{"location":"features/text-features.html#pro-tips","title":"\ud83d\udc8e Pro Tips","text":"\ud83e\uddf9 Text Cleaning <p>Clean your text data before feeding it to KDP for better results:</p> <pre><code>import re\nimport pandas as pd\n\n# Clean text data before preprocessing\ndef clean_text(text):\n    text = text.lower()  # Lowercase\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n    return text.strip()\n\n# Apply cleaning to your data\ndata = pd.read_csv(\"raw_reviews.csv\")\ndata[\"cleaned_review\"] = data[\"review\"].apply(clean_text)\n\n# Use cleaned text in KDP\npreprocessor = PreprocessingModel(\n    path_data=data,\n    features_specs={\"cleaned_review\": FeatureType.TEXT}\n)\n</code></pre> \ud83d\udccf Choose the Right Sequence Length <p>Set sequence length based on your text distribution to avoid truncating important information:</p> <pre><code>import pandas as pd\nimport numpy as np\n\n# Analyze text length distribution\ndata = pd.read_csv(\"reviews.csv\")\nlengths = data[\"review\"].apply(lambda x: len(x.split()))\n\n# Get statistics\nprint(f\"Mean length: {np.mean(lengths)}\")\nprint(f\"Median length: {np.median(lengths)}\")\nprint(f\"95th percentile: {np.percentile(lengths, 95)}\")\n\n# Choose sequence length based on distribution\n# A common approach is to use the 95th percentile\nsequence_length = int(np.percentile(lengths, 95))\n\n# Configure with appropriate length\ntext_feature = TextFeature(\n    name=\"review\",\n    sequence_length=sequence_length\n)\n</code></pre> \ud83d\udd0d Combine Multiple Representations <p>Use different text representations for the same field to capture different aspects:</p> <pre><code># Use multiple representations of the same text\npreprocessor = PreprocessingModel(\n    path_data=\"reviews.csv\",\n    features_specs={\n        # Semantic embedding representation\n        \"review_embedding\": TextFeature(\n            name=\"review\",\n            output_mode=\"embedding\",\n            embedding_dim=64\n        ),\n\n        # Bag-of-words representation (good for keywords)\n        \"review_bow\": TextFeature(\n            name=\"review\",\n            output_mode=\"binary\",  # Binary bag-of-words\n            max_tokens=5000\n        )\n    }\n)\n</code></pre> \ud83d\udcca Visualize Embeddings <p>Visualize your text embeddings to understand the semantic space:</p> <pre><code>from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Get embeddings from preprocessor\npreprocessor.fit()\nresult = preprocessor.build_preprocessor()\n\n# Extract embeddings for visualization\nembeddings = preprocessor.get_text_embeddings(\"review_text\")\nwords = preprocessor.get_text_vocabulary(\"review_text\")\n\n# Visualize with t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nembeddings_2d = tsne.fit_transform(embeddings)\n\n# Plot most common words\nplt.figure(figsize=(12, 10))\nplt.scatter(embeddings_2d[:100, 0], embeddings_2d[:100, 1])\n\nfor i, word in enumerate(words[:100]):\n    plt.annotate(word, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]))\n\nplt.title(\"Text Embedding Visualization\")\nplt.show()\n</code></pre>"},{"location":"features/text-features.html#understanding-text-processing","title":"\ud83d\udcca Understanding Text Processing","text":"graph TD       A[Raw Text Data] --&gt;|Tokenization| B[Tokens]       B --&gt;|Vocabulary Lookup| C[Token Indices]       C --&gt;|Embedding Layer| D[Token Embeddings]       D --&gt;|Pooling/Attention| E[Text Representation]        style A fill:#f9f9f9,stroke:#ccc,stroke-width:2px       style B fill:#e1f5fe,stroke:#4fc3f7,stroke-width:2px       style C fill:#e8f5e9,stroke:#66bb6a,stroke-width:2px       style D fill:#fff8e1,stroke:#ffd54f,stroke-width:2px       style E fill:#f3e5f5,stroke:#ce93d8,stroke-width:2px    <p>KDP converts raw text into meaningful vector representations through a series of transformations, from tokenization to final pooling or attention mechanisms.</p>"},{"location":"features/text-features.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"\ud83d\udd22 Numerical Features \ud83c\udff7\ufe0f Categorical Features \ud83e\udde0 Advanced Embedding Techniques \ud83d\udcda Text Processing Examples \u2190 Categorical Features Date Features \u2192"},{"location":"generated/api_index.html","title":"API Reference","text":"<p>This page contains the auto-generated API reference for the Keras Data Processor library.</p>"},{"location":"generated/api_index.html#classes","title":"Classes","text":"<p>Auto-generated documentation for KDP classes will appear here. This documentation is created automatically from code docstrings.</p>"},{"location":"generated/api_index.html#functions","title":"Functions","text":"<p>Auto-generated documentation for KDP functions will appear here. This documentation is created automatically from code docstrings.</p>"},{"location":"generated/api_index.html#modules","title":"Modules","text":"<p>Auto-generated documentation for KDP modules will appear here. This documentation is created automatically from code docstrings.</p>"},{"location":"getting-started/architecture.html","title":"\ud83c\udfd7\ufe0f KDP Architecture: How the Magic Works","text":"What happens behind the scenes? <p>Ever wondered what happens when KDP transforms your raw data into ML-ready features? This guide takes you under the hood.</p>"},{"location":"getting-started/architecture.html#quick-overview","title":"\ud83d\udccb Quick Overview","text":"<p>KDP's architecture consists of interconnected components that work together to make preprocessing faster, smarter, and more efficient. This guide will walk you through each component and show you how they transform raw data into powerful ML features.</p>"},{"location":"getting-started/architecture.html#kdps-building-blocks","title":"\ud83e\udde9 KDP's Building Blocks","text":"<p>KDP operates like a high-performance factory with specialized stations:</p> 1 Feature Definition Layer <p>Where you describe your data</p> 2 Smart Processors <p>Specialized handlers for each data type</p> 3 Advanced Processing Modules <p>Deep learning enhancements</p> 4 Combination Engine <p>Brings everything together</p> 5 Deployment Bridge <p>Connects to your ML pipeline</p>"},{"location":"getting-started/architecture.html#the-magic-in-action","title":"\ud83d\ude80 The Magic in Action","text":"Let's follow the journey of your data through KDP: Raw Data \u2192 Feature Processing \u2192 Advanced Transformations \u2192 Feature Combination \u2192 ML-Ready Features"},{"location":"getting-started/architecture.html#1-feature-definition-tell-kdp-about-your-data","title":"1\ufe0f\u20e3 Feature Definition: Tell KDP About Your Data","text":"<pre><code># This is your blueprint - tell KDP what you're working with\nfeatures = {\n    \"age\": FeatureType.FLOAT_NORMALIZED,          # Simple definition\n    \"income\": NumericalFeature(                   # Detailed configuration\n        name=\"income\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        use_embedding=True\n    ),\n    \"occupation\": FeatureType.STRING_CATEGORICAL,\n    \"purchase_date\": FeatureType.DATE\n}\n\n# Create your data transformer\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",\n    features_specs=features\n)\n</code></pre>"},{"location":"getting-started/architecture.html#2-smart-processors-type-specific-transformation","title":"2\ufe0f\u20e3 Smart Processors: Type-Specific Transformation","text":"<p>Each feature gets processed by a specialized component:</p> Feature Type Handled By What It Does \ud83d\udd22 Numerical <code>NumericalProcessor</code> Normalization, scaling, distribution-aware transformations \ud83c\udff7\ufe0f Categorical <code>CategoricalProcessor</code> Vocabulary creation, embedding generation, encoding \ud83d\udcdd Text <code>TextProcessor</code> Tokenization, n-gram analysis, semantic embedding \ud83d\udcc5 Date <code>DateProcessor</code> Component extraction, cyclical encoding, temporal pattern detection <pre><code># Behind the scenes: KDP creates a processor chain\nnumerical_processor = NumericalProcessor(feature_config)\ncategory_processor = CategoricalProcessor(feature_config)\ntext_processor = TextProcessor(feature_config)\ndate_processor = DateProcessor(feature_config)\n</code></pre>"},{"location":"getting-started/architecture.html#3-advanced-modules-deep-learning-power","title":"3\ufe0f\u20e3 Advanced Modules: Deep Learning PowerDistribution-Aware EncoderTabular AttentionFeature SelectionFeature MoE","text":"<p>KDP enhances basic processing with deep learning:</p> \ud83d\udcca <p>Automatically detects and handles data distributions</p> \ud83d\udc41\ufe0f <p>Learns relationships between features</p> \ud83c\udfaf <p>Identifies which features matter most</p> \ud83d\udd00 <p>Applies different processing strategies per feature</p> <pre><code># Enable advanced processing in one line each\npreprocessor = PreprocessingModel(\n    features_specs=features,\n    use_distribution_aware=True,       # Smart distribution handling\n    tabular_attention=True,            # Feature relationships\n    feature_selection_placement=\"all\"  # Automatic feature importance\n)\n</code></pre>"},{"location":"getting-started/architecture.html#4-combination-engine-bringing-features-together","title":"4\ufe0f\u20e3 Combination Engine: Bringing Features TogetherConcatenationWeighted CombinationMulti-head AttentionTransformer Blocks","text":"<p>KDP combines all processed features based on your configuration:</p> \ud83d\udd17 <p>Simple joining of features</p> \u2696\ufe0f <p>Features weighted by importance</p> \ud83e\udde0 <p>Complex interaction modeling</p> \ud83d\udd04 <p>Advanced feature mixing</p>"},{"location":"getting-started/architecture.html#5-deployment-bridge-production-ready","title":"5\ufe0f\u20e3 Deployment Bridge: Production-Ready","text":"<p>The final component connects your preprocessing to training and inference:</p> <pre><code># Build the processing pipeline\nresult = preprocessor.build_preprocessor()\nmodel = result[\"model\"]  # Standard Keras model\n\n# Save for production\npreprocessor.save_model(\"customer_preprocess_model\")\n\n# Load anywhere\nfrom kdp import PreprocessingModel\nloaded = PreprocessingModel.load_model(\"customer_preprocess_model\")\n</code></pre>"},{"location":"getting-started/architecture.html#smart-decision-making","title":"\ud83e\udde0 Smart Decision Making","text":"KDP makes intelligent decisions at multiple points to optimize your preprocessing pipeline:"},{"location":"getting-started/architecture.html#feature-type-detection","title":"\ud83d\udd0d Feature Type Detection","text":"<p>KDP can automatically analyze your data to determine the most appropriate feature types:</p> <pre><code># KDP detects the best type when you don't specify\nauto_detected_features = {\n    \"mystery_column\": None  # KDP will analyze and decide\n}\n\n# Behind the scenes, KDP:\n# 1. Examines sample distribution and uniqueness\n# 2. Detects data patterns (numbers, text, dates)\n# 3. Recommends optimal encoding strategy\n</code></pre>"},{"location":"getting-started/architecture.html#distribution-detection-handling","title":"\ud83d\udcca Distribution Detection &amp; Handling","text":"<p>KDP examines the statistical properties of each numerical feature to apply appropriate transformations:</p> <pre><code># Enable distribution-aware processing\npreprocessor = PreprocessingModel(\n    features_specs=features,\n    use_distribution_aware=True,\n    distribution_aware_bins=1000  # Higher resolution for complex distributions\n)\n\n# KDP automatically detects and handles:\n# - Normal distributions \u2192 Standard scaling\n# - Skewed distributions \u2192 Log transformations\n# - Multimodal distributions \u2192 Specialized encoding\n# - Outliers \u2192 Robust scaling techniques\n# - Missing values \u2192 Imputation strategies\n</code></pre>"},{"location":"getting-started/architecture.html#optimization-strategies","title":"\u2699\ufe0f Optimization Strategies","text":"<p>KDP dynamically optimizes preprocessing for both efficiency and effectiveness:</p> <pre><code># KDP automatically:\n# - Caches intermediate results for faster processing\n# - Uses batch processing for memory efficiency\n# - Parallelizes operations when possible\n# - Reduces dimensionality when beneficial\n</code></pre> <p>Processing strategies are determined based on:</p> <ul> <li>Data size and complexity</li> <li>Available computational resources</li> <li>Feature interdependencies</li> <li>Statistical significance of features</li> </ul>"},{"location":"getting-started/architecture.html#edge-case-handling","title":"\ud83d\uded1 Edge Case Handling","text":"<p>KDP implements sophisticated handling for challenging data situations:</p> <pre><code># KDP handles these edge cases automatically:\npreprocessor = PreprocessingModel(\n    features_specs=features,\n    # No additional configuration needed!\n)\n</code></pre> <p>Edge cases managed by KDP include:</p> <ul> <li>Out-of-vocabulary values in categorical features</li> <li>Previously unseen patterns in text data</li> <li>Date values outside training range</li> <li>Missing values or unexpected nulls</li> <li>Extreme outliers in numerical columns</li> </ul>"},{"location":"getting-started/architecture.html#adaptive-learning","title":"\ud83d\udd04 Adaptive Learning","text":"<p>KDP continually refines its understanding of your data:</p> <pre><code># Analyze additional data after initial build\npreprocessor.update_statistics(new_data)\n\n# Preprocessor automatically adapts to:\n# - Shifting distributions\n# - New categorical values\n# - Changing relationships between features\n</code></pre> <p>This adaptive approach ensures your preprocessing remains optimal even as data evolves over time.</p> \u2190 Motivation Quick Start Guide \u2192"},{"location":"getting-started/installation.html","title":"\ud83d\udce6 Installation Guide","text":"Installation Guide <p>Get KDP up and running in your environment quickly and easily</p>"},{"location":"getting-started/installation.html#overview","title":"\ud83d\udccb Overview","text":"<p>KDP can be installed through various methods, from simple pip installation to building from source. Choose the method that best fits your needs and environment.</p> \ud83d\ude80 Quick Installation <p>Simple pip install for most users</p> \ud83d\udee0\ufe0f Multiple Methods <p>pip, Poetry, or source installation</p> \ud83e\udde9 Optional Dependencies <p>Install only what you need</p> \u26a1 GPU Support <p>Leverage GPU acceleration</p>"},{"location":"getting-started/installation.html#quick-installation","title":"\ud83d\ude80 Quick Installation","text":"<pre><code>pip install kdp\n</code></pre>"},{"location":"getting-started/installation.html#installation-methods","title":"\ud83d\udee0\ufe0f Installation Methods","text":"Using pip (Recommended) <pre><code># Basic installation\npip install kdp\n</code></pre> Using Poetry <pre><code># Add to your project\npoetry add kdp\n</code></pre> From Source <pre><code># Clone the repository\ngit clone https://github.com/piotrlaczkowski/keras-data-processor.git\ncd keras-data-processor\n\n# Install using pip\npip install -e .\n\n# Or using poetry\npoetry install\n</code></pre>"},{"location":"getting-started/installation.html#dependencies","title":"\ud83e\udde9 Dependencies","text":"Core Dependencies <ul> <li>\ud83d\udc0d Python 3.7+</li> <li>\ud83d\udd04 TensorFlow 2.5+</li> <li>\ud83d\udd22 NumPy 1.19+</li> <li>\ud83d\udcca Pandas 1.2+</li> </ul> Optional Dependencies Package Purpose Install Command scipy \ud83e\uddea Scientific computing and statistical functions <code>pip install \"kdp[dev]\"</code> ipython \ud83d\udd0d Interactive Python shell and notebook support <code>pip install \"kdp[dev]\"</code> pytest \u2705 Testing framework and utilities <code>pip install \"kdp[dev]\"</code> pydot \ud83d\udcca Graph visualization for model architecture <code>pip install \"kdp[dev]\"</code> Development Tools \ud83d\udee0\ufe0f All development dependencies <code>pip install \"kdp[dev]\"</code> Documentation Tools \ud83d\udcda Documentation generation tools <code>pip install \"kdp[doc]\"</code>"},{"location":"getting-started/installation.html#gpu-support","title":"\ud83d\udda5\ufe0f GPU Support","text":"Enable GPU Acceleration <pre><code># Install TensorFlow with GPU support\npip install tensorflow-gpu\n</code></pre> <p>Ensure you have the appropriate CUDA and cuDNN versions installed.</p>"},{"location":"getting-started/installation.html#verifying-your-installation","title":"\u2705 Verifying Your Installation","text":"<pre><code>import kdp\n\n# Check version\nprint(f\"KDP version: {kdp.__version__}\")\n\n# Basic functionality test\nfrom kdp import PreprocessingModel, FeatureType\nfeatures = {\"test\": FeatureType.FLOAT}\nmodel = PreprocessingModel(features_specs=features)\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"getting-started/installation.html#next-steps","title":"\ud83d\udc63 Next Steps","text":"\ud83c\udfc1 Quick Start Guide <p>Learn the basics of KDP</p> Get Started \u2192 \ud83c\udfd7\ufe0f Architecture Overview <p>Understand KDP's components</p> Learn More \u2192 \ud83d\udd0d Feature Processing <p>Explore KDP's capabilities</p> Explore \u2192"},{"location":"getting-started/motivation.html","title":"\ud83d\ude80 Why KDP Exists: The Origin Story","text":"Born from frustration with existing preprocessing tools <p>KDP was created when traditional preprocessing tools collapsed under the weight of real-world data.</p>"},{"location":"getting-started/motivation.html#the-breaking-point-with-existing-tools","title":"\u2753 The Breaking Point with Existing Tools","text":"\ud83d\udc0c Preprocessing Took Forever <p>Each feature required a separate data pass, turning minutes into hours</p> \ud83d\udca5 Memory Explosions <p>OOM errors became the norm rather than the exception</p> \ud83e\udde9 Customization Nightmares <p>Implementing specialized preprocessing meant fighting the framework</p> \ud83d\udd0d Feature-Specific Needs <p>Different data types needed different handling, not one-size-fits-all approaches</p>"},{"location":"getting-started/motivation.html#how-kdp-changes-everything","title":"\ud83d\udee0\ufe0f How KDP Changes Everything","text":"KDP fundamentally reimagines tabular data preprocessing: \u26a1 10-50x Faster Processing <p>Single-pass architecture transforms preprocessing from hours to minutes</p> \ud83e\udde0 Smart Memory Management <p>Process GB-scale datasets on standard laptops without OOM errors</p> \ud83d\udd27 Built for Customization <p>Plug in your own processing components or use our advanced features</p> \ud83e\udd16 Distribution-Aware Processing <p>Automatically detects and handles complex data distributions</p>"},{"location":"getting-started/motivation.html#see-the-difference","title":"\ud83d\udcca See the Difference","text":"Our benchmarks show the dramatic impact on real-world workloads: Performance Benchmarks <p>KDP outperforms alternative preprocessing approaches, especially as data size increases:</p> Scaling with Features <p>KDP's scaling is nearly linear with feature count:</p> <p>As your data grows: Traditional tools scale linearly or worse, while KDP stays efficient.</p>"},{"location":"getting-started/motivation.html#from-real-world-pain-to-real-world-solution","title":"\ud83d\udc68\u200d\ud83d\udcbb From Real-World Pain to Real-World Solution","text":"\u275d <p>We were spending 70% of our ML development time just waiting for preprocessing to finish. With KDP, that dropped to under 10%.</p> \u275d <p>Our preprocessing pipeline kept crashing on 50GB datasets. KDP processed it without breaking a sweat on the same hardware.</p>"},{"location":"getting-started/motivation.html#benefits-youll-feel-immediately","title":"\ud83d\udc8e Benefits You'll Feel Immediately","text":"\ud83d\ude80 From Idea to Model Faster <p>When preprocessing takes minutes instead of hours, you can iterate rapidly</p> \ud83d\udcbb Works on Your Existing Hardware <p>No need for specialized machines just for preprocessing</p> \ud83e\uddea More Experiments, Better Models <p>Run 10x more experiments in the same time</p> \ud83d\udd04 Smoother Production Transitions <p>The same code works for both small-scale development and production-scale deployment</p>"},{"location":"getting-started/motivation.html#kdps-unique-approaches","title":"\u2728 KDP's Unique Approaches","text":"1 Smart Feature Detection <p>Automatic identification of feature types and optimal processing</p> 2 Efficient Caching System <p>Intelligently caches intermediate results to avoid redundant computation</p> 3 Vectorized Operations <p>Utilizes TensorFlow's optimized ops for maximum throughput</p> 4 Batch Processing Architecture <p>Processes data in optimized chunks to balance memory and speed</p>"},{"location":"getting-started/motivation.html#the-future-were-building","title":"\ud83d\udd2e The Future We're Building","text":"1 Expanded Hardware Support <p>Optimizations for specialized processors (TPUs, etc.)</p> 2 Even Smarter Defaults <p>Auto-configuration based on your specific dataset characteristics</p> 3 More Integration Options <p>Seamless workflows with popular ML frameworks</p> 4 Community Contributions <p>Your ideas becoming features that help everyone</p>"},{"location":"getting-started/motivation.html#join-the-kdp-movement","title":"\ud83e\udd1d Join the KDP Movement","text":"Found this useful? Help us make KDP even better: \ud83c\udf1f <p>Star our repository and spread the word</p> \ud83d\udc1b <p>Report issues when you find them</p> \ud83d\udd27 <p>Contribute improvements and extensions</p> \ud83d\udca1 <p>Share your success stories</p> <p>Check out our Contributing Guide to get started.</p>"},{"location":"getting-started/motivation.html#ready-to-begin","title":"\ud83d\udea6 Ready to Begin?","text":"\u23f1\ufe0f 5-Minute Quick Start <p>See KDP in action with minimal code</p> \ud83d\udd0d Performance Deep Dive <p>Understand the optimizations in detail</p> \ud83e\udde9 Complex Real-World Examples <p>See how KDP handles sophisticated scenarios</p> \u2190 Installation Architecture Overview \u2192"},{"location":"getting-started/quick-start.html","title":"\ud83d\ude80 Quick Start Guide: KDP in 5 Minutes","text":"Get your tabular data ML-ready in record time! <p>This guide will have you transforming raw data into powerful features before your coffee gets cold.</p>"},{"location":"getting-started/quick-start.html#the-kdp-experience-in-3-steps","title":"\ud83c\udfc1 The KDP Experience in 3 Steps","text":"1 Define Your Features <pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Quick feature definition - KDP handles the complexity\nfeatures = {\n    # Numerical features with smart preprocessing\n    \"age\": FeatureType.FLOAT_NORMALIZED,          # Age gets 0-1 normalization\n    \"income\": FeatureType.FLOAT_RESCALED,         # Income gets robust scaling\n\n    # Categorical features with automatic encoding\n    \"occupation\": FeatureType.STRING_CATEGORICAL, # Text categories to embeddings\n    \"education\": FeatureType.INTEGER_CATEGORICAL, # Numeric categories\n\n    # Special types get special treatment\n    \"feedback\": FeatureType.TEXT,                 # Text gets tokenization &amp; embedding\n    \"signup_date\": FeatureType.DATE               # Dates become useful components\n}\n</code></pre> 2 Build Your Processor <pre><code># Create with smart defaults - one line setup\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",     # Point to your data\n    features_specs=features,           # Your feature definitions\n    use_distribution_aware=True        # Automatic distribution handling\n)\n\n# Build analyzes your data and creates the preprocessing pipeline\nresult = preprocessor.build_preprocessor()\nmodel = result[\"model\"]                # This is your transformer!\n</code></pre> 3 Process Your Data <pre><code># Your data can be a dict, DataFrame, or tensors\nnew_customer_data = {\n    \"age\": [24, 67, 31],\n    \"income\": [48000, 125000, 52000],\n    \"occupation\": [\"developer\", \"manager\", \"designer\"],\n    \"education\": [4, 5, 3],\n    \"feedback\": [\"Great product!\", \"Could be better\", \"Love it\"],\n    \"signup_date\": [\"2023-06-15\", \"2022-03-22\", \"2023-10-01\"]\n}\n\n# Transform into ML-ready features with a single call\nprocessed_features = model(new_customer_data)\n\n# That's it! Your data is now ready for modeling\n</code></pre>"},{"location":"getting-started/quick-start.html#power-features","title":"\ud83d\udd25 Power Features","text":"Take your preprocessing to the next level with these one-liners: <pre><code># Create a more advanced preprocessor\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",\n    features_specs=features,\n\n    # Power features - each adds capability\n    use_distribution_aware=True,        # Smart distribution handling\n    use_numerical_embedding=True,       # Neural embeddings for numbers\n    tabular_attention=True,             # Learn feature relationships\n    feature_selection_placement=\"all\",  # Automatic feature importance\n\n    # Add transformers for state-of-the-art performance\n    transfo_nr_blocks=2,                # Two transformer blocks\n    transfo_nr_heads=4                  # With four attention heads\n)\n</code></pre>"},{"location":"getting-started/quick-start.html#real-world-examples","title":"\ud83d\udcbc Real-World Examples","text":"\ud83d\udc65 Customer Churn Prediction <pre><code># Perfect setup for churn prediction\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",\n    features_specs={\n        \"days_active\": FeatureType.FLOAT_NORMALIZED,\n        \"monthly_spend\": FeatureType.FLOAT_RESCALED,\n        \"total_purchases\": FeatureType.FLOAT_RESCALED,\n        \"product_category\": FeatureType.STRING_CATEGORICAL,\n        \"last_support_ticket\": FeatureType.DATE,\n        \"support_messages\": FeatureType.TEXT\n    },\n    use_distribution_aware=True,\n    feature_selection_placement=\"all\",    # Identify churn drivers\n    tabular_attention=True                # Model feature interactions\n)\n</code></pre> \ud83d\udcc8 Financial Time Series <pre><code># Setup for financial forecasting\npreprocessor = PreprocessingModel(\n    path_data=\"stock_data.csv\",\n    features_specs={\n        \"open\": FeatureType.FLOAT_RESCALED,\n        \"high\": FeatureType.FLOAT_RESCALED,\n        \"low\": FeatureType.FLOAT_RESCALED,\n        \"volume\": FeatureType.FLOAT_RESCALED,\n        \"sector\": FeatureType.STRING_CATEGORICAL,\n        \"date\": FeatureType.DATE\n    },\n    use_numerical_embedding=True,        # Neural embeddings for price data\n    numerical_embedding_dim=32,          # Larger embeddings for complex patterns\n    tabular_attention_heads=4            # Multiple attention heads\n)\n</code></pre>"},{"location":"getting-started/quick-start.html#production-integration","title":"\ud83d\udcf1 Production Integration","text":"<pre><code># Save your preprocessor after building\npreprocessor.save_model(\"customer_churn_preprocessor\")\n\n# --- Later in production ---\n\n# Load your preprocessor\nfrom kdp import PreprocessingModel\npreprocessor = PreprocessingModel.load_model(\"customer_churn_preprocessor\")\n\n# Process new data\nnew_customer = {\"age\": 35, \"income\": 75000, ...}\nfeatures = preprocessor(new_customer)\n\n# Use with your prediction model\nprediction = my_model(features)\n</code></pre>"},{"location":"getting-started/quick-start.html#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"1 Start Simple First <pre><code># Begin with basic configuration\nbasic = PreprocessingModel(features_specs=features)\n\n# Then add advanced features as needed\nadvanced = PreprocessingModel(\n    features_specs=features,\n    use_distribution_aware=True,\n    tabular_attention=True\n)\n</code></pre> 2 Handle Big Data Efficiently <pre><code># For large datasets\npreprocessor = PreprocessingModel(\n    features_specs=features,\n    enable_caching=True,        # Speed up repeated processing\n    batch_size=10000            # Process in manageable chunks\n)\n</code></pre> 3 Get Feature Importance <pre><code># First enable feature selection when creating the model\npreprocessor = PreprocessingModel(\n    features_specs=features,\n    feature_selection_placement=\"all_features\",  # Required for feature importance\n    feature_selection_units=32\n)\n\n# Build the preprocessor\npreprocessor.build_preprocessor()\n\n# After building, you can get feature importances\nimportances = preprocessor.get_feature_importances()\nprint(\"Most important features:\", sorted(\n    importances.items(), key=lambda x: x[1], reverse=True\n)[:3])\n</code></pre>"},{"location":"getting-started/quick-start.html#where-to-next","title":"\ud83d\udd17 Where to Next?","text":"\ud83d\udd0d Feature Processing Guide <p>Deep dive into feature types</p> \ud83d\udcca Distribution-Aware Encoding <p>Smart numerical handling</p> \ud83e\udde0 Advanced Numerical Embeddings <p>Neural representations</p> \ud83d\udc41\ufe0f Tabular Attention <p>Model feature relationships</p> \ud83d\udee0\ufe0f Complex Examples <p>Complete real-world scenarios</p> \u2190 Installation Architecture Overview \u2192"},{"location":"integrations/overview.html","title":"\ud83d\udd17 Integrations: Connect KDP to Your World","text":""},{"location":"integrations/overview.html#quick-overview","title":"\ud83d\udccb Quick Overview","text":"<p>KDP is designed to play nicely with your existing ML ecosystem. This guide shows you how to seamlessly connect KDP's powerful preprocessing with your models, pipelines, and production systems.</p>"},{"location":"integrations/overview.html#one-minute-integration","title":"\ud83d\ude80 One-Minute Integration","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\nimport tensorflow as tf\n\n# Define features\nfeatures = {\n    \"age\": FeatureType.FLOAT_NORMALIZED,\n    \"income\": FeatureType.FLOAT_RESCALED,\n    \"occupation\": FeatureType.STRING_CATEGORICAL\n}\n\n# Create &amp; build preprocessor\npreprocessor = PreprocessingModel(\n    path_data=\"customers.csv\",\n    features_specs=features\n).build_preprocessor()[\"model\"]  # This is a Keras model!\n\n# Connect to your model - just like any Keras layer\ninputs = preprocessor.input\nx = preprocessor.output\nx = tf.keras.layers.Dense(64, activation=\"relu\")(x)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n\n# Your full model with preprocessing built-in!\nfull_model = tf.keras.Model(inputs=inputs, outputs=outputs)\nfull_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n</code></pre>"},{"location":"integrations/overview.html#integration-patterns","title":"\ud83c\udfd7\ufe0f Integration Patterns","text":"<p>Choose the pattern that fits your workflow:</p>"},{"location":"integrations/overview.html#pattern-1-functional-api-most-flexible","title":"Pattern 1: Functional API (Most Flexible)","text":"<pre><code># This approach gives you complete flexibility\ndef create_full_model(preprocessor, num_classes=1):\n    # Get preprocessor inputs and outputs\n    inputs = preprocessor.input\n    x = preprocessor.output\n\n    # Build your model architecture\n    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n\n    # Classification or regression output\n    if num_classes == 1:\n        outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    else:\n        outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n\n    # Create the combined model\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Create your complete pipeline\nmodel = create_full_model(preprocessor, num_classes=3)\n</code></pre>"},{"location":"integrations/overview.html#pattern-2-model-subclassing-for-complex-models","title":"Pattern 2: Model Subclassing (For Complex Models)","text":"<pre><code>class CustomerChurnModel(tf.keras.Model):\n    def __init__(self, preprocessor):\n        super().__init__()\n        # Store the preprocessor\n        self.preprocessor = preprocessor\n\n        # Define your layers\n        self.dense1 = tf.keras.layers.Dense(128, activation=\"relu\")\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.dropout = tf.keras.layers.Dropout(0.3)\n        self.dense2 = tf.keras.layers.Dense(64, activation=\"relu\")\n        self.output_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n\n    def call(self, inputs, training=False):\n        # First run through preprocessor\n        x = self.preprocessor(inputs)\n\n        # Then through your model\n        x = self.dense1(x)\n        x = self.bn1(x)\n        if training:\n            x = self.dropout(x)\n        x = self.dense2(x)\n        return self.output_layer(x)\n\n# Create your model\nchurn_model = CustomerChurnModel(preprocessor)\n</code></pre>"},{"location":"integrations/overview.html#tfdata-integration","title":"\ud83d\udcca tf.data Integration","text":"<p>KDP works beautifully with tf.data for efficient data loading:</p> <pre><code># Create a tf.data pipeline with KDP\ndef create_dataset(file_path, batch_size=32):\n    # Create dataset from CSV\n    dataset = tf.data.experimental.make_csv_dataset(\n        file_path,\n        batch_size=batch_size,\n        select_columns=list(features.keys()) + [\"target\"],\n        num_epochs=1,\n        shuffle=True\n    )\n\n    # Split features and labels\n    def split_features_label(data):\n        label = data.pop(\"target\")\n        return data, label\n\n    dataset = dataset.map(split_features_label)\n\n    # Performance optimizations\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n\n    return dataset\n\n# Create your datasets\ntrain_ds = create_dataset(\"train.csv\", batch_size=64)\nval_ds = create_dataset(\"val.csv\")\n\n# Train with the data pipeline\nmodel.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=10\n)\n</code></pre>"},{"location":"integrations/overview.html#real-world-examples","title":"\ud83c\udf10 Real-World Examples","text":""},{"location":"integrations/overview.html#e-commerce-recommendation-system","title":"E-Commerce Recommendation System","text":"<pre><code># Create a recommendation system with KDP preprocessing\ndef build_recommendation_system(user_features, item_features):\n    # User branch\n    user_inputs = {k: tf.keras.Input(shape=(1,), name=k) for k in user_features}\n    user_preprocessor = PreprocessingModel(\n        features_specs=user_features\n    ).build_preprocessor()[\"model\"]\n    user_vector = user_preprocessor(user_inputs)\n\n    # Item branch\n    item_inputs = {k: tf.keras.Input(shape=(1,), name=k) for k in item_features}\n    item_preprocessor = PreprocessingModel(\n        features_specs=item_features\n    ).build_preprocessor()[\"model\"]\n    item_vector = item_preprocessor(item_inputs)\n\n    # Combine branches\n    dot_product = tf.keras.layers.Dot(axes=1)([user_vector, item_vector])\n    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dot_product)\n\n    # Create final model\n    model = tf.keras.Model(\n        inputs={**user_inputs, **item_inputs},\n        outputs=output\n    )\n    return model\n\n# Define features\nuser_features = {\n    \"user_id\": FeatureType.STRING_CATEGORICAL,\n    \"age_group\": FeatureType.STRING_CATEGORICAL,\n    \"purchase_history\": FeatureType.TEXT\n}\n\nitem_features = {\n    \"item_id\": FeatureType.STRING_CATEGORICAL,\n    \"category\": FeatureType.STRING_CATEGORICAL,\n    \"description\": FeatureType.TEXT\n}\n\n# Build the recommendation system\nrec_model = build_recommendation_system(user_features, item_features)\n</code></pre>"},{"location":"integrations/overview.html#fraud-detection-system","title":"Fraud Detection System","text":"<pre><code># Build a fraud detection pipeline with KDP\npreprocessor = PreprocessingModel(\n    path_data=\"transactions.csv\",\n    features_specs={\n        \"amount\": NumericalFeature(\n            name=\"amount\",\n            feature_type=FeatureType.FLOAT_RESCALED,\n            use_embedding=True\n        ),\n        \"timestamp\": FeatureType.DATE,\n        \"merchant_category\": FeatureType.STRING_CATEGORICAL,\n        \"location\": FeatureType.STRING_CATEGORICAL,\n        \"user_history\": FeatureType.TEXT\n    },\n    # Enable advanced preprocessing\n    use_distribution_aware=True,\n    tabular_attention=True\n).build_preprocessor()[\"model\"]\n\n# Add a time-series layer for sequential patterns\ninputs = preprocessor.input\nfeatures = preprocessor.output\n\n# Add LSTM for sequence modeling\nx = tf.keras.layers.Reshape((-1, features.shape[-1]))(features)\nx = tf.keras.layers.LSTM(64, return_sequences=False)(x)\nx = tf.keras.layers.Dense(32, activation=\"relu\")(x)\noutputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n\n# Create the fraud detection model\nfraud_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n</code></pre>"},{"location":"integrations/overview.html#production-deployment","title":"\ud83d\ude80 Production Deployment","text":""},{"location":"integrations/overview.html#method-1-all-in-one-deployment","title":"Method 1: All-in-One Deployment","text":"<pre><code># Save the complete model (preprocessing + prediction)\nfull_model.save(\"production_model.keras\")\n\n# Later in production:\nfrom tensorflow import keras\nmodel = keras.models.load_model(\"production_model.keras\")\n\n# Single-step inference\nprediction = model({\n    \"age\": [35],\n    \"income\": [75000],\n    \"occupation\": [\"engineer\"]\n})\n</code></pre>"},{"location":"integrations/overview.html#method-2-modular-deployment","title":"Method 2: Modular Deployment","text":"<pre><code># Save components separately\npreprocessor.save_model(\"preprocessor.keras\")\nprediction_model.save(\"predictor.keras\")\n\n# In production:\npreprocessor = keras.models.load_model(\"preprocessor.keras\")\nprediction_model = keras.models.load_model(\"predictor.keras\")\n\n# Two-step inference\nfeatures = preprocessor(input_data)\nprediction = prediction_model(features)\n</code></pre>"},{"location":"integrations/overview.html#method-3-tensorflow-serving","title":"Method 3: TensorFlow Serving","text":"<pre><code># Export for TensorFlow Serving\nimport tensorflow as tf\n\n# Define serving signature\n@tf.function(input_signature=[{\n    \"age\": tf.TensorSpec(shape=[None], dtype=tf.float32),\n    \"income\": tf.TensorSpec(shape=[None], dtype=tf.float32),\n    \"occupation\": tf.TensorSpec(shape=[None], dtype=tf.string)\n}])\ndef serving_fn(inputs):\n    return {\"prediction\": full_model(inputs)}\n\n# Save with signature\ntf.saved_model.save(\n    full_model,\n    \"serving_model/1\",\n    signatures={\"serving_default\": serving_fn}\n)\n\n# Now deploy with TensorFlow Serving\n# docker run -p 8501:8501 --mount type=bind,source=/path/to/serving_model,target=/models/my_model -e MODEL_NAME=my_model tensorflow/serving\n</code></pre>"},{"location":"integrations/overview.html#pro-tips-for-production","title":"\ud83d\udca1 Pro Tips for Production","text":"<ol> <li> <p>Optimize for Speed <pre><code># Make preprocessing faster in production\npreprocessor = PreprocessingModel(\n    features_specs=features,\n    enable_caching=True,        # Cache intermediate results\n    batch_size=100,             # Process in batches\n    output_dtypes=\"float32\"     # Use smaller precision if possible\n)\n</code></pre></p> </li> <li> <p>Staged Training Strategy <pre><code># Train in stages for better results\n\n# Stage 1: Freeze preprocessing, train model\npreprocessor.trainable = False\nmodel.fit(train_data, epochs=5)\n\n# Stage 2: Fine-tune everything with lower rate\npreprocessor.trainable = True\nmodel.compile(optimizer=tf.keras.optimizers.Adam(1e-5))\nmodel.fit(train_data, epochs=3)\n</code></pre></p> </li> <li> <p>A/B Testing Different Preprocessors <pre><code># Create different preprocessors\nbasic_preprocessor = PreprocessingModel(features_specs=features)\nadvanced_preprocessor = PreprocessingModel(\n    features_specs=features,\n    use_distribution_aware=True\n)\n\n# Create models with identical architecture\nmodel_A = create_model(basic_preprocessor)\nmodel_B = create_model(advanced_preprocessor)\n\n# Train and compare\nhistory_A = model_A.fit(train_data, validation_data=val_data)\nhistory_B = model_B.fit(train_data, validation_data=val_data)\n\n# Compare validation metrics\nimport matplotlib.pyplot as plt\nplt.plot(history_A.history['val_accuracy'], label='Basic')\nplt.plot(history_B.history['val_accuracy'], label='Advanced')\nplt.legend()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"integrations/overview.html#integrating-with-other-frameworks","title":"\ud83d\udd04 Integrating with Other Frameworks","text":""},{"location":"integrations/overview.html#pytorch-integration","title":"PyTorch Integration","text":"<pre><code># Use KDP preprocessing with PyTorch models\nimport tensorflow as tf\nimport torch\nimport numpy as np\n\n# Build your preprocessor\npreprocessor = PreprocessingModel(\n    features_specs=features\n).build_preprocessor()[\"model\"]\n\n# TensorFlow preprocessing + PyTorch model\nclass HybridModel:\n    def __init__(self, tf_preprocessor, torch_model):\n        self.preprocessor = tf_preprocessor\n        self.torch_model = torch_model\n\n    def predict(self, inputs):\n        # TensorFlow preprocessing\n        features = self.preprocessor(inputs).numpy()\n\n        # Convert to PyTorch tensor\n        features_torch = torch.from_numpy(features).float()\n\n        # PyTorch inference\n        with torch.no_grad():\n            output = self.torch_model(features_torch)\n\n        return output.numpy()\n</code></pre>"},{"location":"integrations/overview.html#scikit-learn-integration","title":"Scikit-learn Integration","text":"<pre><code># Create a scikit-learn compatible wrapper\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass KDPTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_specs=None):\n        self.feature_specs = feature_specs\n        self.preprocessor = None\n\n    def fit(self, X, y=None):\n        # Create and build preprocessor\n        self.preprocessor = PreprocessingModel(\n            features_specs=self.feature_specs\n        ).build_preprocessor()[\"model\"]\n        return self\n\n    def transform(self, X):\n        # Apply preprocessing\n        return self.preprocessor(X).numpy()\n\n# Use in scikit-learn pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline([\n    ('kdp_preprocessor', KDPTransformer(feature_specs=features)),\n    ('classifier', RandomForestClassifier())\n])\n\n# Train and use the pipeline\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)\n</code></pre>"},{"location":"integrations/overview.html#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ul> <li>TensorFlow Best Practices - Optimize your model performance</li> <li>Quick Start Guide - Review KDP basics</li> <li>Tabular Optimization - Further optimize your pipeline</li> <li>Feature Selection - Enhance model efficiency</li> </ul>"},{"location":"integrations/overview.html#memory-and-performance-optimization","title":"Memory and Performance Optimization","text":"<p>KDP provides several optimization techniques to improve model performance:</p> <ul> <li>Memory Optimization: Techniques for reducing memory usage. See Tabular Optimization.</li> <li>Feature Selection: Automatically identify the most important features. See Feature Selection.</li> </ul>"},{"location":"optimization/auto-configuration.html","title":"\ud83e\uddd9\u200d\u2642\ufe0f Auto-Configuration: Analytics and Recommendations","text":"Auto-Configuration: Analytics and Recommendations <p>Let KDP analyze your data and suggest the optimal preprocessing</p> Intelligent Data Analysis <p>Auto-Configuration examines your dataset and provides intelligent recommendations for feature processing, helping you build better models faster.</p>"},{"location":"optimization/auto-configuration.html#getting-started","title":"\ud83d\ude80 Getting Started","text":"Basic Usage <pre><code>from kdp import auto_configure, PreprocessingModel\n\n# Analyze data and get recommendations\nconfig = auto_configure(\"customer_data.csv\")\n\n# Review the recommendations\nrecommendations = config[\"recommendations\"]\ncode_snippet = config[\"code_snippet\"]\n\n# Create your preprocessor using the code snippet as a guide\n# Note: You'll need to manually implement the suggestions\n</code></pre>"},{"location":"optimization/auto-configuration.html#what-auto-configuration-provides","title":"\u2728 What Auto-Configuration Provides","text":"\ud83d\udd0d Distribution Analysis <p>Identifies patterns in your numeric data to suggest optimal transformations</p> \ud83d\udcca Feature Statistics <p>Calculates important statistics about your features to guide preprocessing</p> \ud83d\udca1 Preprocessing Recommendations <p>Suggests appropriate feature types and transformations based on data analysis</p> \ud83d\udcdd Example Code <p>Generates ready-to-use code snippets based on the analysis</p>"},{"location":"optimization/auto-configuration.html#what-it-analyzes","title":"\ud83d\udd0d What It Analyzes","text":"Data Characteristic Example What It Detects Distribution Types Log-normal income, bimodal age Statistical distribution patterns Feature Statistics Mean, variance, skewness Basic statistical properties Data Ranges Min/max values, outliers Value boundaries and extremes Value Patterns Discrete vs continuous How values are distributed"},{"location":"optimization/auto-configuration.html#examples","title":"\ud83d\udcbc Examples","text":"\ud83d\udd0e Basic Analysis <pre><code># Basic auto-configuration analysis\nconfig = auto_configure(\n    \"customer_data.csv\",  # Your dataset\n    batch_size=50000,     # Process in batches of this size\n    save_stats=True       # Save computed statistics\n)\n\n# Review the recommendations\nfor feature_name, recommendation in config[\"recommendations\"].items():\n    print(f\"Feature: {feature_name}\")\n    print(f\"  Type: {recommendation['feature_type']}\")\n    print(f\"  Preprocessing: {recommendation['preprocessing']}\")\n\n# Get the suggested code snippet\nprint(config[\"code_snippet\"])\n</code></pre>"},{"location":"optimization/auto-configuration.html#understanding-the-results","title":"\ud83d\udcca Understanding the Results","text":"\ud83d\udcca Results Structure <pre><code># Example results structure\nconfig = {\n    \"recommendations\": {\n        \"income\": {\n            \"feature_type\": \"NumericalFeature\",\n            \"preprocessing\": [\"NORMALIZATION\"],\n            \"detected_distribution\": \"log_normal\",\n            \"config\": {\n                # Specific configuration recommendations\n            }\n        },\n        # More features...\n    },\n    \"code_snippet\": \"# Python code with recommended configuration\",\n    \"statistics\": {\n        # If save_stats=True, contains computed statistics\n    }\n}\n</code></pre>"},{"location":"optimization/auto-configuration.html#available-options","title":"\ud83d\udee0\ufe0f Available Options","text":"\u2699\ufe0f Configuration Options <pre><code># Auto-configuration with options\nconfig = auto_configure(\n    data_path=\"customer_data.csv\",      # Path to your dataset\n    features_specs=None,                # Optional: provide existing features specs\n    batch_size=50000,                   # Batch size for processing\n    save_stats=True,                    # Whether to include statistics in results\n    stats_path=\"features_stats.json\",   # Where to save/load statistics\n    overwrite_stats=False               # Whether to recalculate existing stats\n)\n</code></pre>"},{"location":"optimization/auto-configuration.html#pro-tips","title":"\ud83d\udca1 Pro Tips","text":"\ud83d\udc40 Review Before Implementing <p>Always review the recommendations before blindly applying them</p> <pre><code># Inspect the recommendations first\nconfig = auto_configure(\"data.csv\")\n\n# Review before implementing\nfor feature, recommendation in config[\"recommendations\"].items():\n    print(f\"{feature}: {recommendation['detected_distribution']}\")\n</code></pre> \ud83e\udde0 Combine with Domain Knowledge <p>Use the recommendations alongside your domain expertise</p> <pre><code># Get recommendations\nconfig = auto_configure(\"data.csv\")\n\n# Create your features dictionary, informed by recommendations\nfeatures = {\n    \"income\": FeatureType.FLOAT_RESCALED,  # Based on recommendation\n    \"age\": FeatureType.FLOAT_NORMALIZED,   # Based on domain knowledge\n}\n</code></pre> \ud83d\udd04 Update When Data Changes <p>Rerun when your data distribution changes</p> <pre><code># Update statistics with new data\nnew_config = auto_configure(\n    \"updated_data.csv\",\n    overwrite_stats=True  # Force recalculation with new data\n)\n</code></pre>"},{"location":"optimization/auto-configuration.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"\ud83d\udcca Distribution-Aware Encoding <p>Apply recommendations for numerical features</p> Learn more \u2192 \ud83c\udfaf Feature Selection <p>Improve model performance</p> Learn more \u2192 \ud83d\udcda Feature Types Overview <p>Learn about all available feature types</p> Learn more \u2192"},{"location":"optimization/feature-selection.html","title":"\ud83c\udfaf Feature Selection: Focus on What Matters","text":""},{"location":"optimization/feature-selection.html#quick-overview","title":"\ud83d\udccb Quick Overview","text":"<p>Feature Selection in KDP automatically identifies and prioritizes your most important features, cutting through the noise to focus on what really drives your predictions. Built on the advanced Gated Residual Variable Selection Network (GRVSN) architecture, it's like having a data scientist automatically analyze your feature importance.</p>"},{"location":"optimization/feature-selection.html#key-benefits","title":"\u2728 Key Benefits","text":"<ul> <li>\ud83e\udde0 Smarter Models: Direct computational power to features that actually matter</li> <li>\ud83d\udcc8 Better Performance: Often boosts accuracy by 5-15% by reducing noise</li> <li>\ud83d\udd0d Instant Insights: See which features drive predictions without manual analysis</li> <li>\u26a1 Training Speedup: Typically 30-50% faster training with optimized feature sets</li> <li>\ud83d\udee1\ufe0f Better Generalization: Models that focus on signal, not noise</li> </ul>"},{"location":"optimization/feature-selection.html#quick-start-example","title":"\ud83d\ude80 Quick Start Example","text":"<pre><code>from kdp import PreprocessingModel, FeatureType\n\n# Define your features\nfeatures = {\n    \"age\": FeatureType.FLOAT_NORMALIZED,\n    \"income\": FeatureType.FLOAT_RESCALED,\n    \"education\": FeatureType.STRING_CATEGORICAL,\n    \"occupation\": FeatureType.STRING_CATEGORICAL,\n    \"marital_status\": FeatureType.STRING_CATEGORICAL,\n    \"last_purchase\": FeatureType.DATE\n}\n\n# Enable feature selection with just a few lines\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",\n    features_specs=features,\n\n    # Enable feature selection for all features\n    feature_selection_placement=\"all_features\",\n    feature_selection_units=64,        # Neural network size\n    feature_selection_dropout=0.2      # Regularization strength\n)\n\n# Build and use as normal\nresult = preprocessor.build_preprocessor()\nmodel = result[\"model\"]\n\n# Now you can see which features matter most!\nimportances = preprocessor.get_feature_importances()\nprint(\"Top features:\", sorted(\n    importances.items(),\n    key=lambda x: x[1],\n    reverse=True\n)[:3])  # Shows your 3 most important features\n</code></pre>"},{"location":"optimization/feature-selection.html#architecture","title":"\ud83e\udde9 Architecture","text":"<p>Feature Selection can be applied at different points in your KDP pipeline:</p> <pre><code># Apply feature selection to all features\npreprocessor = PreprocessingModel(\n    features_specs=features,\n    feature_selection_placement=\"all_features\",\n    feature_selection_method=\"correlation\",\n    feature_selection_threshold=0.01\n)\n</code></pre> <p>Note: Feature selection integrates directly into your model architecture. The importance scores are calculated during training and can be visualized using the provided utility methods.</p>"},{"location":"optimization/feature-selection.html#configuration-options","title":"\ud83c\udf9b\ufe0f Configuration Options","text":""},{"location":"optimization/feature-selection.html#placement-options","title":"Placement Options","text":"<p>Choose where to apply feature selection with the <code>feature_selection_placement</code> parameter:</p> Option Description Best For <code>\"none\"</code> Disable feature selection When you know all features matter <code>\"numeric\"</code> Only select among numerical features Financial or scientific data <code>\"categorical\"</code> Only select among categorical features Marketing or demographic data <code>\"all_features\"</code> Apply selection to all feature types Most use cases - let KDP decide"},{"location":"optimization/feature-selection.html#key-parameters","title":"Key Parameters","text":"Parameter Purpose Default Recommended Range <code>feature_selection_units</code> Size of neural network 64 32-128 (larger = more capacity) <code>feature_selection_dropout</code> Prevents overfitting 0.2 0.1-0.3 (higher for smaller datasets) <code>feature_selection_use_bias</code> Adds bias term to gates True Usually keep as True"},{"location":"optimization/feature-selection.html#real-world-examples","title":"\ud83d\udcca Real-World Examples","text":""},{"location":"optimization/feature-selection.html#customer-churn-prediction","title":"Customer Churn Prediction","text":"<pre><code># Perfect for churn prediction with many potential factors\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",\n    features_specs={\n        \"customer_age\": FeatureType.FLOAT_NORMALIZED,\n        \"subscription_length\": FeatureType.FLOAT_RESCALED,\n        \"monthly_spend\": FeatureType.FLOAT_RESCALED,\n        \"support_tickets\": FeatureType.FLOAT_RESCALED,\n        \"product_tier\": FeatureType.STRING_CATEGORICAL,\n        \"last_upgrade\": FeatureType.DATE,\n        \"industry\": FeatureType.STRING_CATEGORICAL,\n        \"region\": FeatureType.STRING_CATEGORICAL,\n        \"company_size\": FeatureType.INTEGER_CATEGORICAL\n    },\n    # Powerful feature selection configuration\n    feature_selection_placement=\"all_features\",\n    feature_selection_units=96,       # Larger for complex patterns\n    feature_selection_dropout=0.15,   # Moderate regularization\n\n    # Combine with distribution-aware for best results\n    use_distribution_aware=True\n)\n\n# After building, analyze what drives churn\nimportances = preprocessor.get_feature_importances()\n</code></pre>"},{"location":"optimization/feature-selection.html#medical-diagnosis-support","title":"Medical Diagnosis Support","text":"<pre><code># For medical applications where feature interpretation is critical\npreprocessor = PreprocessingModel(\n    path_data=\"patient_data.csv\",\n    features_specs={\n        \"age\": FeatureType.FLOAT_NORMALIZED,\n        \"heart_rate\": FeatureType.FLOAT_NORMALIZED,\n        \"blood_pressure\": FeatureType.FLOAT_NORMALIZED,\n        \"glucose_level\": FeatureType.FLOAT_NORMALIZED,\n        \"cholesterol\": FeatureType.FLOAT_NORMALIZED,\n        \"bmi\": FeatureType.FLOAT_NORMALIZED,\n        \"smoking_status\": FeatureType.STRING_CATEGORICAL,\n        \"family_history\": FeatureType.STRING_CATEGORICAL\n    },\n    # Focus on numerical biomarkers\n    feature_selection_placement=\"numeric\",\n    feature_selection_units=64,\n    feature_selection_dropout=0.2,\n\n    # Medical applications benefit from careful regularization\n    use_numerical_embedding=True,\n    numerical_embedding_dim=32\n)\n</code></pre>"},{"location":"optimization/feature-selection.html#visualizing-feature-importance","title":"\ud83d\udcca Visualizing Feature Importance","text":"<p>KDP provides utilities to visualize which features are most important:</p> <pre><code># After building and training your preprocessor\nfeature_importance = preprocessor.get_feature_importance()\n\n# Visualize the importance scores\npreprocessor.plot_feature_importance()\n\n# Get the top N most important features\ntop_features = preprocessor.get_top_features(n=10)\n</code></pre> <p>Note: The feature importance visualization shows a bar chart with features sorted by their importance scores, helping you identify which features contribute most to your model's performance.</p>"},{"location":"optimization/feature-selection.html#pro-tips-for-feature-selection","title":"\ud83d\udca1 Pro Tips for Feature Selection","text":"<ol> <li> <p>Use With Distribution-Aware Encoding <pre><code># This combination often works exceptionally well\npreprocessor = PreprocessingModel(\n    features_specs=features,\n    feature_selection_placement=\"all_features\",\n    use_distribution_aware=True  # Add this line\n)\n</code></pre></p> </li> <li> <p>Focus Selection for Speed <pre><code># For large datasets, focus on specific feature types first\npreprocessor = PreprocessingModel(\n    features_specs=many_features,\n    feature_selection_placement=\"numeric\",  # Start with just numerical\n    enable_caching=True  # Speed up repeated processing\n)\n</code></pre></p> </li> <li> <p>Progressive Feature Refinement <pre><code># First run to identify important features\nimportances = first_preprocessor.get_feature_importances()\n\n# Keep only features with importance &gt; 0.05\nimportant_features = {k: v for k, v in features.items()\n                     if importances.get(k, 0) &gt; 0.05}\n\n# Create refined model with just important features\nrefined_preprocessor = PreprocessingModel(\n    features_specs=important_features,\n    # More advanced processing now with fewer features\n    transfo_nr_blocks=2,\n    tabular_attention=True\n)\n</code></pre></p> </li> <li> <p>Tracking Importance Over Time <pre><code># For production systems, monitor if important features change\nimport json\nfrom datetime import datetime\n\n# Save importance scores with timestamp\ndef log_importances(preprocessor, name):\n    importances = preprocessor.get_feature_importances()\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    with open(f\"importance_{name}_{timestamp}.json\", \"w\") as f:\n        json.dump(importances, f, indent=2)\n\n# Call periodically in production\nlog_importances(my_preprocessor, \"customer_model\")\n</code></pre></p> </li> </ol>"},{"location":"optimization/feature-selection.html#related-topics","title":"\ud83d\udd17 Related Topics","text":"<ul> <li>Distribution-Aware Encoding</li> <li>Tabular Attention</li> <li>Feature MoE</li> <li>Complex Examples</li> </ul>"},{"location":"optimization/tabular-optimization.html","title":"\ud83d\ude80 Tabular Optimization: Beating Traditional Models","text":""},{"location":"optimization/tabular-optimization.html#quick-overview","title":"\ud83d\udccb Quick Overview","text":"<p>Want to outperform XGBoost and other traditional tabular models? KDP's advanced optimization techniques help you achieve state-of-the-art results by addressing the core limitations of tree-based approaches. This guide shows you how to unlock neural superpowers for tabular data.</p>"},{"location":"optimization/tabular-optimization.html#why-kdp-beats-traditional-models","title":"\u2728 Why KDP Beats Traditional Models","text":"Challenge Traditional Approach KDP's Solution Complex Distributions Fixed binning strategies \ud83d\udcca Distribution-Aware Encoding that adapts to your specific data Interaction Discovery Manual feature crosses or tree splits \ud83d\udc41\ufe0f Tabular Attention that automatically finds important relationships Feature Importance Post-hoc analysis \ud83c\udfaf Built-in Feature Selection during training Deep Representations Limited embedding capabilities \ud83e\udde0 Advanced Neural Embeddings for all feature types Performance at Scale Memory issues with large datasets \u26a1 Optimized Processing Pipeline with batching and caching"},{"location":"optimization/tabular-optimization.html#performance-comparison","title":"\ud83c\udfc6 Performance Comparison","text":"<p>In our benchmarks against top tabular models:</p> <ul> <li>Accuracy: +3-7% improvement over XGBoost on complex datasets</li> <li>AUC: +5% average improvement on financial and user behavior data</li> <li>Training Time: 2-5x faster than comparable deep learning approaches</li> <li>Memory Usage: 50-70% reduction compared to one-hot encoding pipelines</li> </ul>"},{"location":"optimization/tabular-optimization.html#one-minute-optimization","title":"\ud83d\ude80 One-Minute Optimization","text":"<pre><code>from kdp import PreprocessingModel\n\n# Create an optimized preprocessor in one step\npreprocessor = PreprocessingModel(\n    path_data=\"customer_data.csv\",\n    features_specs=features,\n\n    # Enable performance-enhancing features\n    use_distribution_aware=True,       # Smart distribution handling\n    tabular_attention=True,            # Feature interaction learning\n    feature_selection_placement=\"all\",  # Remove noise automatically\n\n    # Performance optimizations\n    enable_caching=True,               # Speed up repeated processing\n    batch_size=10000                   # Process in manageable chunks\n)\n\n# Build and get metrics\nresult = preprocessor.build_preprocessor()\nmodel = result[\"model\"]\n\n# Check optimization results\nprint(f\"Memory usage: {preprocessor.get_memory_usage()['peak_mb']} MB\")\nprint(f\"Processing time: {preprocessor.get_timing_metrics()['total_seconds']:.2f}s\")\n</code></pre>"},{"location":"optimization/tabular-optimization.html#advanced-optimization-techniques","title":"\ud83d\udd27 Advanced Optimization Techniques","text":""},{"location":"optimization/tabular-optimization.html#1-distribution-aware-optimization","title":"1. Distribution-Aware Optimization","text":"<pre><code># Fine-tune distribution handling for better performance\npreprocessor = PreprocessingModel(\n    features_specs=features,\n\n    # Enable and customize distribution-aware encoding\n    use_distribution_aware=True,\n    distribution_detection_confidence=0.85,   # Higher = more precise detection\n    adaptive_binning=True,                    # Learn optimal bin boundaries\n    distribution_aware_bins=1000,             # More bins = finer-grained encoding\n    handle_outliers=\"clip\"                    # Options: \"clip\", \"remove\", \"special_token\"\n)\n</code></pre>"},{"location":"optimization/tabular-optimization.html#2-feature-interaction-optimization","title":"2. Feature Interaction Optimization","text":"<pre><code># Optimize how features interact with each other\npreprocessor = PreprocessingModel(\n    features_specs=features,\n\n    # Enable and customize tabular attention\n    tabular_attention=True,\n    tabular_attention_heads=8,                # More heads = more interaction patterns\n    tabular_attention_dim=128,                # Larger = richer representations\n    tabular_attention_placement=\"multi_resolution\",  # Process at multiple scales\n\n    # Advanced interaction learning\n    transfo_nr_blocks=2,                      # Add transformer blocks\n    transfo_dropout_rate=0.1                  # Regularization for better generalization\n)\n</code></pre>"},{"location":"optimization/tabular-optimization.html#3-memory-performance-optimization","title":"3. Memory &amp; Performance Optimization","text":"<pre><code># Optimize for large datasets and faster processing\npreprocessor = PreprocessingModel(\n    features_specs=features,\n\n    # Memory optimization\n    batch_size=50000,                         # Adjust based on available RAM\n    enable_caching=True,                      # Cache intermediate results\n    cache_location=\"memory\",                  # Options: \"memory\", \"disk\"\n\n    # Computational efficiency\n    use_mixed_precision=True,                 # Faster computation with fp16\n    parallel_feature_processing=True,         # Process features in parallel\n    distribution_encoding_threads=4           # Parallel distribution encoding\n)\n</code></pre>"},{"location":"optimization/tabular-optimization.html#real-world-optimization-examples","title":"\ud83d\udcc8 Real-World Optimization Examples","text":""},{"location":"optimization/tabular-optimization.html#financial-fraud-detection","title":"Financial Fraud Detection","text":"<pre><code># Optimize for fraud detection (imbalanced, complex distributions)\npreprocessor = PreprocessingModel(\n    path_data=\"transactions.csv\",\n    features_specs={\n        \"amount\": FeatureType.FLOAT_RESCALED,\n        \"transaction_time\": FeatureType.DATE,\n        \"merchant_id\": FeatureType.STRING_CATEGORICAL,\n        \"device_id\": FeatureType.STRING_CATEGORICAL,\n        \"location\": FeatureType.STRING_CATEGORICAL,\n        \"history_summary\": FeatureType.TEXT\n    },\n    # Distribution optimization for financial data\n    use_distribution_aware=True,\n    distribution_aware_bins=2000,            # More precise for financial values\n\n    # Interaction learning for fraud patterns\n    tabular_attention=True,\n    tabular_attention_heads=12,              # More heads for complex interactions\n\n    # Performance optimizations\n    feature_selection_placement=\"all\",       # Focus on relevant signals\n    enable_caching=True,\n    batch_size=5000                          # Smaller batches for complex processing\n)\n</code></pre>"},{"location":"optimization/tabular-optimization.html#e-commerce-recommendations","title":"E-Commerce Recommendations","text":"<pre><code># Optimize for recommendation systems (high-dimensional, sparse)\npreprocessor = PreprocessingModel(\n    path_data=\"user_product_interactions.csv\",\n    features_specs={\n        \"user_id\": FeatureType.STRING_CATEGORICAL,\n        \"product_id\": FeatureType.STRING_CATEGORICAL,\n        \"category\": FeatureType.STRING_CATEGORICAL,\n        \"price\": FeatureType.FLOAT_RESCALED,\n        \"past_purchases\": FeatureType.TEXT,\n        \"last_visit\": FeatureType.DATE\n    },\n    # Memory optimization for high cardinality\n    categorical_embedding_dim=32,            # Smaller embeddings for many categories\n    max_vocabulary_size=100000,              # Limit vocabulary size\n\n    # Specialized recommendation processing\n    feature_crosses=[(\"user_id\", \"category\")],  # Important interaction\n    use_feature_moe=True,                    # Mixture of Experts for different features\n\n    # Performance optimizations\n    enable_caching=True,\n    use_mixed_precision=True                 # Faster computation with mixed precision\n)\n</code></pre>"},{"location":"optimization/tabular-optimization.html#measuring-optimization-impact","title":"\ud83e\uddea Measuring Optimization Impact","text":"<p>Check if your optimizations are working:</p> <pre><code># Create baseline and optimized models\nbaseline = PreprocessingModel(features_specs=features).build_preprocessor()[\"model\"]\noptimized = PreprocessingModel(\n    features_specs=features,\n    use_distribution_aware=True,\n    tabular_attention=True\n).build_preprocessor()[\"model\"]\n\n# Create identical downstream models\ndef create_model(preprocessor):\n    inputs = preprocessor.input\n    x = preprocessor.output\n    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"auc\"])\n    return model\n\n# Build and evaluate both models\nbaseline_model = create_model(baseline)\noptimized_model = create_model(optimized)\n\n# Train and compare\nbaseline_history = baseline_model.fit(train_data, validation_data=val_data)\noptimized_history = optimized_model.fit(train_data, validation_data=val_data)\n\n# Visualize the difference\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 6))\nplt.plot(baseline_history.history['val_auc'], label='Baseline')\nplt.plot(optimized_history.history['val_auc'], label='Optimized')\nplt.title('Optimization Impact on Validation AUC')\nplt.ylabel('AUC')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig('optimization_impact.png', dpi=300)\nplt.show()\n</code></pre>"},{"location":"optimization/tabular-optimization.html#optimization-pro-tips","title":"\ud83d\udca1 Optimization Pro Tips","text":"<ol> <li> <p>Start with Distribution-Aware Encoding <pre><code># Always enable this first - it's the biggest win\npreprocessor = PreprocessingModel(\n    features_specs=features,\n    use_distribution_aware=True  # Just this one change helps significantly\n)\n</code></pre></p> </li> <li> <p>Profile Before Optimizing <pre><code># See where the bottlenecks are\npreprocessor = PreprocessingModel(features_specs=features)\nresult = preprocessor.build_preprocessor()\n\n# Check timing metrics\ntiming = preprocessor.get_timing_metrics()\nprint(\"Timing breakdown:\")\nfor step, time in timing['steps'].items():\n    print(f\"- {step}: {time:.2f}s ({time/timing['total_seconds']*100:.1f}%)\")\n\n# Check memory metrics\nmemory = preprocessor.get_memory_usage()\nprint(f\"Peak memory: {memory['peak_mb']} MB\")\nfor feature, mem in memory['per_feature'].items():\n    print(f\"- {feature}: {mem:.1f}MB\")\n</code></pre></p> </li> <li> <p>Progressive Optimization Strategy <pre><code># Step 1: Basic optimization\nbasic = PreprocessingModel(\n    features_specs=features,\n    use_distribution_aware=True,\n    enable_caching=True\n)\n\n# Step 2: Add interaction learning\nintermediate = PreprocessingModel(\n    features_specs=features,\n    use_distribution_aware=True,\n    tabular_attention=True,\n    enable_caching=True\n)\n\n# Step 3: Full optimization\nadvanced = PreprocessingModel(\n    features_specs=features,\n    use_distribution_aware=True,\n    tabular_attention=True,\n    transfo_nr_blocks=2,\n    feature_selection_placement=\"all\",\n    use_mixed_precision=True,\n    enable_caching=True\n)\n\n# Compare metrics at each stage\n# This helps you find the optimal cost/benefit point\n</code></pre></p> </li> <li> <p>Feature-Specific Optimization <pre><code># Focus optimization on problematic features\nfrom kdp.features import NumericalFeature, CategoricalFeature\n\noptimized_features = {\n    # Standard feature\n    \"age\": FeatureType.FLOAT_NORMALIZED,\n\n    # Optimized high-cardinality feature\n    \"product_id\": CategoricalFeature(\n        name=\"product_id\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_dim=16,             # Smaller embedding\n        max_vocabulary_size=10000,    # Limit vocabulary\n        handle_unknown=\"use_oov\"      # Handle unseen values\n    ),\n\n    # Optimized skewed numerical feature\n    \"transaction_amount\": NumericalFeature(\n        name=\"transaction_amount\",\n        feature_type=FeatureType.FLOAT_RESCALED,\n        use_embedding=True,\n        preferred_distribution=\"log_normal\"  # Distribution hint\n    )\n}\n</code></pre></p> </li> </ol>"},{"location":"optimization/tabular-optimization.html#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ul> <li>Distribution-Aware Encoding - Deep dive into distribution optimization</li> <li>Tabular Attention - Advanced feature interaction learning</li> <li>Memory Optimization - Handle large-scale datasets efficiently</li> <li>Benchmarking - Compare KDP against other approaches</li> </ul>"},{"location":"optimization/tabular-optimization.html#related-features","title":"Related Features","text":"<ul> <li>Distribution-Aware Encoding</li> <li>Tabular Attention</li> <li>Memory Optimization</li> <li>Benchmarking</li> </ul>"},{"location":"optimization/tabular-optimization.html#advanced-techniques","title":"\ud83d\udcca Advanced Techniques","text":""},{"location":"optimization/tabular-optimization.html#memory-optimization","title":"Memory Optimization","text":""},{"location":"optimization/tabular-optimization.html#strategies-for-large-scale-datasets","title":"Strategies for Large-Scale Datasets","text":"<p>KDP provides several strategies for optimizing memory usage:</p> <ol> <li>Lazy Loading - Process data in batches instead of loading everything at once</li> <li>Feature Compression - Use dimensionality reduction techniques for high-cardinality features</li> <li>Quantization - Use numerical precision optimizations when applicable</li> <li>Sparse Representations - Leverage sparse tensors for categorical features</li> </ol> <pre><code># Memory-optimized preprocessing\nmodel = PreprocessingModel(\n    path_data=\"large_dataset.csv\",\n    features_specs=features,\n    batch_size=1024,  # Process in smaller batches\n    use_memory_optimization=True\n)\n</code></pre>"},{"location":"optimization/tabular-optimization.html#benchmarking","title":"Benchmarking","text":""},{"location":"optimization/tabular-optimization.html#performance-comparison_1","title":"Performance Comparison","text":"<p>KDP is designed to be efficient and performant. Here's how it compares to other preprocessing approaches:</p> Metric KDP Pandas TF.Transform PyTorch Memory Usage \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Processing Speed \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 Feature Coverage \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Integration Ease \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50"},{"location":"optimization/tabular-optimization.html#benchmark-code-example","title":"Benchmark Code Example","text":"<pre><code>import time\nfrom kdp import PreprocessingModel\nimport pandas as pd\n\n# Sample dataset\ndf = pd.read_csv(\"benchmark_dataset.csv\")\n\n# KDP approach\nstart_time = time.time()\nmodel = PreprocessingModel(features_specs=features)\nmodel.fit(df)\npreprocessor = model.build_preprocessor()\nkdp_time = time.time() - start_time\n\n# Pandas approach\nstart_time = time.time()\n# Traditional pandas preprocessing code...\npandas_time = time.time() - start_time\n\nprint(f\"KDP processing time: {kdp_time:.2f}s\")\nprint(f\"Pandas processing time: {pandas_time:.2f}s\")\nprint(f\"Speedup: {pandas_time/kdp_time:.2f}x\")\n</code></pre>"}]}