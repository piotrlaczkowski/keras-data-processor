"""
Model Advisor module that analyzes dataset statistics and provides
recommendations for optimal preprocessing configurations.
"""

from typing import Dict, Any
import numpy as np
from loguru import logger


class ModelAdvisor:
    """
    Analyzes dataset statistics and recommends optimal preprocessing configurations
    based on feature characteristics and distributions.
    """

    def __init__(self, features_stats: Dict[str, Any]):
        """
        Initialize the ModelAdvisor with dataset statistics.

        Args:
            features_stats: Dictionary containing feature statistics generated by DatasetStatistics
        """
        self.features_stats = features_stats
        self.recommendations = {}
        self.global_config = {}

    def analyze_feature_stats(self) -> Dict[str, Any]:
        """
        Analyze feature statistics and generate recommendations.

        Returns:
            Dictionary containing recommendations for all features and global configuration
        """
        if not self.features_stats:
            logger.warning(
                "No feature statistics provided. Cannot generate recommendations."
            )
            return {}

        # Process numeric features
        self._analyze_numeric_features()

        # Process categorical features
        self._analyze_categorical_features()

        # Process text features
        self._analyze_text_features()

        # Process date features
        self._analyze_date_features()

        # Generate global recommendations
        self._generate_global_recommendations()

        return {"features": self.recommendations, "global_config": self.global_config}

    def _analyze_numeric_features(self):
        """Analyze numeric features and generate specific recommendations."""
        numeric_stats = self.features_stats.get("numeric_stats", {})

        for feature, stats in numeric_stats.items():
            recommendation = {
                "feature_type": "NumericalFeature",
                "preprocessing": [],
                "config": {},
            }

            # Extract statistics
            mean = stats.get("mean", 0)
            variance = stats.get("var", 1)
            std_dev = np.sqrt(variance) if variance is not None else 1

            # Check for zero variance
            if variance is not None and variance < 1e-10:
                recommendation["preprocessing"].append("CONSTANT")
                recommendation["config"]["constant_value"] = mean
                recommendation["notes"] = [
                    "Feature has near-zero variance, consider removing"
                ]
                self.recommendations[feature] = recommendation
                continue

            # Determine distribution properties
            skewness = self._calculate_skewness(mean, std_dev)
            kurtosis = self._calculate_kurtosis(mean, std_dev)

            # Detect distribution type
            dist_type, dist_confidence = self._detect_distribution_type(
                stats, skewness, kurtosis
            )

            recommendation["detected_distribution"] = dist_type
            recommendation["distribution_confidence"] = dist_confidence

            # Recommend feature transformations
            if dist_type == "normal":
                recommendation["preprocessing"].append("FLOAT_NORMALIZED")
                recommendation["config"]["normalization"] = "z_score"
                recommendation["notes"] = [
                    "Normal distribution detected, standard normalization recommended"
                ]

            elif dist_type == "uniform":
                recommendation["preprocessing"].append("FLOAT_RESCALED")
                recommendation["config"]["min"] = stats.get("min", 0)
                recommendation["config"]["max"] = stats.get("max", 1)
                recommendation["notes"] = [
                    "Uniform distribution detected, rescaling recommended"
                ]

            elif dist_type == "heavy_tailed":
                recommendation["preprocessing"].append("DISTRIBUTION_AWARE")
                recommendation["config"]["prefered_distribution"] = "heavy_tailed"
                recommendation["notes"] = [
                    "Heavy-tailed distribution detected, specialized transformation recommended"
                ]

            elif dist_type == "log_normal":
                recommendation["preprocessing"].append("DISTRIBUTION_AWARE")
                recommendation["config"]["prefered_distribution"] = "log_normal"
                recommendation["notes"] = [
                    "Log-normal distribution detected, logarithmic transformation recommended"
                ]

            elif dist_type == "periodic":
                recommendation["preprocessing"].append("DISTRIBUTION_AWARE")
                recommendation["config"]["prefered_distribution"] = "periodic"
                recommendation["notes"] = [
                    "Periodic distribution detected, trigonometric features recommended"
                ]

            elif dist_type == "multimodal":
                recommendation["preprocessing"].append("DISTRIBUTION_AWARE")
                recommendation["config"]["prefered_distribution"] = "multimodal"
                recommendation["notes"] = [
                    "Multimodal distribution detected, specialized encoding recommended"
                ]

            elif dist_type == "sparse":
                recommendation["preprocessing"].append("DISTRIBUTION_AWARE")
                recommendation["config"]["prefered_distribution"] = "sparse"
                recommendation["notes"] = [
                    "Sparse distribution detected, specialized handling recommended"
                ]

            else:
                # Default normalization for unknown distributions
                recommendation["preprocessing"].append("FLOAT_NORMALIZED")
                recommendation["config"]["normalization"] = "z_score"
                recommendation["notes"] = [
                    f"Unknown distribution type: {dist_type}, using standard normalization"
                ]

            # Determine if advanced numerical embedding would be beneficial
            if abs(skewness) > 1.5 or abs(kurtosis - 3) > 2:
                recommendation["advanced_options"] = {
                    "use_advanced_numerical_embedding": True,
                    "embedding_dim": 8,
                    "num_bins": min(
                        100, max(10, int(np.sqrt(stats.get("count", 1000))))
                    ),
                }
                recommendation["notes"].append(
                    "Complex distribution detected, advanced numerical embedding recommended"
                )

            self.recommendations[feature] = recommendation

    def _analyze_categorical_features(self):
        """Analyze categorical features and generate specific recommendations."""
        categorical_stats = self.features_stats.get("categorical_stats", {})

        for feature, stats in categorical_stats.items():
            recommendation = {
                "feature_type": "CategoricalFeature",
                "preprocessing": [],
                "config": {},
            }

            # Extract statistics
            vocab_size = stats.get("size", 0)

            # Determine encoding strategy based on vocabulary size
            if vocab_size < 5:
                recommendation["preprocessing"].append("ONE_HOT")
                recommendation["notes"] = [
                    f"Small vocabulary ({vocab_size} categories), one-hot encoding recommended"
                ]

            elif vocab_size < 50:
                recommendation["preprocessing"].append("EMBEDDING")
                recommendation["config"]["embedding_dim"] = min(
                    8, max(2, vocab_size // 4)
                )
                recommendation["notes"] = [
                    f"Medium vocabulary ({vocab_size} categories), embedding recommended"
                ]

            else:
                recommendation["preprocessing"].append("HASHING")
                recommendation["config"]["hash_bins"] = min(
                    1024, max(100, vocab_size * 2)
                )
                recommendation["notes"] = [
                    f"Large vocabulary ({vocab_size} categories), hashing recommended"
                ]

            self.recommendations[feature] = recommendation

    def _analyze_text_features(self):
        """Analyze text features and generate specific recommendations."""
        text_stats = self.features_stats.get("text", {})

        for feature, stats in text_stats.items():
            recommendation = {
                "feature_type": "TextFeature",
                "preprocessing": [],
                "config": {},
            }

            # Extract statistics
            vocab_size = stats.get("vocab_size", 10000)
            sequence_length = stats.get("sequence_length", 100)

            # Recommend tokenization and embedding strategy
            recommendation["preprocessing"].append("TEXT_VECTORIZATION")
            recommendation["config"]["max_tokens"] = min(20000, vocab_size)
            recommendation["config"]["output_sequence_length"] = min(
                200, sequence_length
            )
            recommendation["config"]["standardize"] = "lower_and_strip_punctuation"

            # Recommend embedding dimension based on vocabulary size
            embedding_dim = min(300, max(16, vocab_size // 100))
            recommendation["config"]["embedding_dim"] = embedding_dim

            recommendation["notes"] = [
                f"Text feature with vocabulary size {vocab_size}",
                f"Using sequence length {sequence_length} and embedding dimension {embedding_dim}",
            ]

            self.recommendations[feature] = recommendation

    def _analyze_date_features(self):
        """Analyze date features and generate specific recommendations."""
        date_stats = self.features_stats.get("date_stats", {})

        for feature, stats in date_stats.items():
            recommendation = {
                "feature_type": "DateFeature",
                "preprocessing": [],
                "config": {},
            }

            # Always recommend cyclical encoding for date components
            recommendation["preprocessing"].append("DATE_CYCLICAL")
            recommendation["config"]["add_season"] = True
            recommendation["config"]["date_format"] = "%Y-%m-%d"  # Default format

            # Check for periodicity in year
            year_variance = stats.get("var_year", 0)
            if year_variance > 0.1:
                recommendation["config"]["add_year"] = True
                recommendation["notes"] = [
                    "Year component varies significantly, including as feature"
                ]
            else:
                recommendation["config"]["add_year"] = False
                recommendation["notes"] = [
                    "Year component has low variance, excluding as feature"
                ]

            self.recommendations[feature] = recommendation

    def _generate_global_recommendations(self):
        """Generate global configuration recommendations based on all features."""
        # Count features by type
        num_numeric = len(self.features_stats.get("numeric_stats", {}))
        num_categorical = len(self.features_stats.get("categorical_stats", {}))
        num_text = len(self.features_stats.get("text", {}))
        num_date = len(self.features_stats.get("date_stats", {}))
        total_features = num_numeric + num_categorical + num_text + num_date

        # Basic configuration
        self.global_config = {
            "output_mode": "CONCAT",
            "use_distribution_aware": num_numeric > 0,
            "notes": [],
        }

        # Recommend advanced features based on data composition

        # Distribution-aware encoding
        if num_numeric > 0:
            complex_distributions = self._count_complex_distributions()
            if complex_distributions > 0:
                self.global_config["use_distribution_aware"] = True
                self.global_config["distribution_aware_bins"] = 1000
                self.global_config["notes"].append(
                    f"Found {complex_distributions} features with complex distributions, enabling distribution-aware encoding"
                )

        # Feature crosses
        if num_numeric >= 2 or (num_numeric >= 1 and num_categorical >= 1):
            self.global_config["feature_crosses"] = self._recommend_feature_crosses()
            if self.global_config["feature_crosses"]:
                self.global_config["notes"].append(
                    f"Recommended {len(self.global_config['feature_crosses'])} feature crosses based on data correlation"
                )

        # Attention mechanism
        if total_features > 3:
            self.global_config["tabular_attention"] = True
            self.global_config["tabular_attention_heads"] = min(8, total_features // 2)

            # Multi-resolution attention for mixed feature types
            if (num_numeric > 0 and num_categorical > 0) or num_text > 0:
                self.global_config["tabular_attention_placement"] = "multi_resolution"
                self.global_config["notes"].append(
                    "Mixed feature types detected, recommending multi-resolution attention"
                )
            else:
                self.global_config["tabular_attention_placement"] = "all_features"
                self.global_config["notes"].append(
                    "Homogeneous feature types, recommending standard tabular attention"
                )

            self.global_config["tabular_attention_dim"] = 64

        # Transformer blocks
        if total_features > 5 or num_text > 0:
            self.global_config["transfo_nr_blocks"] = 2
            self.global_config["transfo_nr_heads"] = 4
            self.global_config["transfo_ff_units"] = 64
            self.global_config["notes"].append(
                "Complex feature set detected, recommending transformer blocks for better feature interaction"
            )

    def _calculate_skewness(self, mean, std_dev):
        """Estimate skewness based on available statistics."""
        # This is a simplistic estimate since we don't have full data
        # In reality, we would need third moment, but we're using heuristics
        if std_dev == 0:
            return 0

        # Estimate from the features we have
        # We're using a placeholder value for demonstration
        return 0.5  # Default moderate skewness

    def _calculate_kurtosis(self, mean, std_dev):
        """Estimate kurtosis based on available statistics."""
        # This is a simplistic estimate since we don't have full data
        # In reality, we would need fourth moment, but we're using heuristics

        # Default to normal distribution kurtosis
        return 3.0

    def _detect_distribution_type(self, stats, skewness, kurtosis):
        """
        Detect distribution type based on statistics.

        Returns:
            tuple: (distribution_type, confidence_score)
        """
        # This is a simplified version of what the DistributionAwareEncoder does
        # In practice, you would implement more sophisticated detection

        # Placeholder for demonstration
        return "normal", 0.8

    def _count_complex_distributions(self):
        """Count the number of features with complex distributions."""
        # Placeholder implementation
        return 2

    def _recommend_feature_crosses(self):
        """Recommend feature crosses based on data correlation."""
        # Placeholder implementation
        # In practice, would analyze correlation between features
        return [("feature1", "feature2", 10)]

    def generate_code_snippet(self) -> str:
        """
        Generate a Python code snippet to implement the recommendations.

        Returns:
            str: Python code for implementing the recommended configuration
        """
        code = [
            "from kdp.processor import PreprocessingModel, OutputModeOptions",
            "from kdp.features import NumericalFeature, CategoricalFeature, TextFeature, DateFeature, FeatureType",
            "",
            "# Define features based on recommendations",
            "features_specs = {",
        ]

        # Add feature definitions
        for feature_name, recommendation in self.recommendations.items():
            feature_type = recommendation["feature_type"]

            if feature_type == "NumericalFeature":
                preprocessing = (
                    recommendation["preprocessing"][0]
                    if recommendation["preprocessing"]
                    else "FLOAT"
                )
                config_params = []

                if "prefered_distribution" in recommendation["config"]:
                    config_params.append(
                        f'prefered_distribution="{recommendation["config"]["prefered_distribution"]}"'
                    )

                config_str = ", ".join(config_params)
                if config_str:
                    config_str = ", " + config_str

                code.append(
                    f'    "{feature_name}": NumericalFeature(name="{feature_name}", feature_type=FeatureType.{preprocessing}{config_str}),'
                )

            elif feature_type == "CategoricalFeature":
                preprocessing = "STRING_CATEGORICAL"  # Default
                if "INTEGER" in recommendation["preprocessing"][0]:
                    preprocessing = "INTEGER_CATEGORICAL"

                code.append(
                    f'    "{feature_name}": CategoricalFeature(name="{feature_name}", feature_type=FeatureType.{preprocessing}),'
                )

            elif feature_type == "TextFeature":
                code.append(
                    f'    "{feature_name}": TextFeature(name="{feature_name}"),'
                )

            elif feature_type == "DateFeature":
                add_season = recommendation["config"].get("add_season", False)
                date_format = recommendation["config"].get("date_format", "%Y-%m-%d")

                code.append(
                    f'    "{feature_name}": DateFeature(name="{feature_name}", add_season={add_season}, date_format="{date_format}"),'
                )

        code.append("}")
        code.append("")

        # Add model configuration
        code.append("# Initialize preprocessing model with recommended configuration")
        code.append("ppr = PreprocessingModel(")
        code.append('    path_data="data.csv",')
        code.append("    features_specs=features_specs,")

        # Add global configuration parameters
        for key, value in self.global_config.items():
            if key == "notes":
                continue

            if isinstance(value, str):
                code.append(f'    {key}="{value}",')
            else:
                code.append(f"    {key}={value},")

        code.append(")")
        code.append("")
        code.append("# Build the preprocessor")
        code.append("ppr.build_preprocessor()")

        return "\n".join(code)


def recommend_model_configuration(features_stats: Dict[str, Any]) -> Dict[str, Any]:
    """
    Analyze dataset statistics and recommend optimal preprocessing configuration.

    Args:
        features_stats: Dictionary containing feature statistics generated by DatasetStatistics

    Returns:
        Dictionary containing recommendations for features and global configuration
    """
    advisor = ModelAdvisor(features_stats)
    recommendations = advisor.analyze_feature_stats()

    # Add code snippet to recommendations
    recommendations["code_snippet"] = advisor.generate_code_snippet()

    return recommendations
