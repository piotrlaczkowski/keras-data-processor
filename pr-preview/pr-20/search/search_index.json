{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf1f Welcome to Keras Data Processor - Preprocessing Power with TensorFlow Keras \ud83c\udf1f","text":"<p>Welcome to the Future of Data Preprocessing!</p> <p>Diving into the world of machine learning and data science, we often find ourselves tangled in the preprocessing jungle. Worry no more! Introducing a state-of-the-art data preprocessing model based on TensorFlow Keras and the innovative use of Keras preprocessing layers.</p> <p>Say goodbye to tedious data preparation tasks and hello to streamlined, efficient, and scalable data pipelines. Whether you're a seasoned data scientist or just starting out, this tool is designed to supercharge your ML workflows, making them more robust and faster than ever!</p>"},{"location":"#key-features","title":"\ud83d\udd11 Key Features:","text":"<ul> <li> <p>\ud83d\udee0 Flexible Feature Engineering: Applies predefined preprocessing steps based on user-specified feature types, allowing for efficient and customizable data preparation with minimal manual coding.</p> </li> <li> <p>\ud83c\udfa8 Customizable Preprocessing Pipelines: Tailor your preprocessing steps with ease. Choose from a comprehensive range of options for numeric, categorical, text data, and even complex feature crosses, allowing for precise and effective data handling.</p> </li> <li> <p>\ud83d\udcca Scalability and Efficiency: Engineered for high performance, this tool handles large datasets effortlessly, leveraging TensorFlow's robust computational capabilities.</p> </li> <li> <p>\ud83e\udde0 Enhanced with Transformer Blocks: Incorporate transformer blocks into your preprocessing model to boost feature interaction analysis and uncover complex patterns, enhancing predictive model accuracy.</p> </li> <li> <p>\u2699\ufe0f Easy Integration: Designed to seamlessly integrate as the first layers in your TensorFlow Keras models, facilitating a smooth transition from raw data to trained model, accelerating your workflow significantly.</p> </li> </ul>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting started:","text":"<p>We use poetry for handling dependencies so you will need to install it first. Then you can install the dependencies by running:</p> <p>To install dependencies:</p> <pre><code>poetry install\n</code></pre> <p>or to enter a dedicated env directly:</p> <pre><code>poetry shell\n</code></pre> <p>Then you can simply configure your preprocessor:</p>"},{"location":"#building-preprocessor","title":"\ud83d\udee0\ufe0f Building Preprocessor:","text":"<p>The simplest application of the preprocessing model is as follows:</p> <pre><code>from kdp import PreprocessingModel\nfrom kdp import FeatureType\n\n# DEFINING FEATURES PROCESSORS\nfeatures_specs = {\n    # ======= NUMERICAL Features =========================\n    \"feat1\": FeatureType.FLOAT_NORMALIZED,\n    \"feat2\": FeatureType.FLOAT_RESCALED,\n    # ======= CATEGORICAL Features ========================\n    \"feat3\": FeatureType.STRING_CATEGORICAL,\n    \"feat4\": FeatureType.INTEGER_CATEGORICAL,\n    # ======= TEXT Features ========================\n    \"feat5\": FeatureType.TEXT,\n}\n\n# INSTANTIATE THE PREPROCESSING MODEL with your data\nppr = PreprocessingModel(\n    path_data=\"data/my_data.csv\",\n    features_specs=features_spec,\n)\n# construct the preprocessing pipelines\nppr.build_preprocessor()\n</code></pre> <p>This will output:</p> <pre><code>{\n'model': &lt;Functional name=preprocessor, built=True&gt;,\n'inputs': {\n    'feat1': &lt;KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=feat1&gt;,\n    'feat2': &lt;KerasTensor shape=(None, 1), dtype=float32, sparse=None, name=feat2&gt;,\n    'feat3': &lt;KerasTensor shape=(None, 1), dtype=string, sparse=None, name=feat3&gt;,\n    'feat4': &lt;KerasTensor shape=(None, 1), dtype=int32, sparse=None, name=feat4&gt;,\n    'feat5': &lt;KerasTensor shape=(None, 1), dtype=string, sparse=None, name=feat5&gt;\n    },\n'signature': {\n    'feat1': TensorSpec(shape=(None, 1), dtype=tf.float32, name='feat1'),\n    'feat2': TensorSpec(shape=(None, 1), dtype=tf.float32, name='feat2'),\n    'feat3': TensorSpec(shape=(None, 1), dtype=tf.string, name='feat3'),\n    'feat4': TensorSpec(shape=(None, 1), dtype=tf.int32, name='feat4'),\n    'feat5': TensorSpec(shape=(None, 1), dtype=tf.string, name='feat5')\n    },\n'output_dims': 45\n}\n</code></pre> <p>This will result in the following preprocessing steps:</p> <p> </p> <p>Success</p> <p>You can define the preprocessing model with the <code>features_specs</code> dictionary, where the keys are the feature names and the values are the feature types. The model will automatically apply the appropriate preprocessing steps based on the feature type.</p> <p>You have access to several layers of customization per feature type, such as normalization, rescaling, or even definition of custom preprocessing steps.</p> <p>See \ud83d\udc40 Defining Features for more details.</p> <p>Info</p> <p>You can use the preprocessing model independently to preprocess your data or integrate it into your Keras model as the first layer, see \ud83d\udc40 Integrations</p>"},{"location":"#advanced-configuration-options","title":"\ud83d\udcaa\ud83c\udffb Advanced Configuration Options","text":""},{"location":"#transformer-blocks-configuration","title":"\ud83e\udd16 Transformer Blocks Configuration","text":"<p>Enhance your preprocessing model with transformer blocks to capture complex patterns and interactions between features, see \ud83d\udc40 Transformer Blocks. You can configure the transformer blocks as follows:</p> <ul> <li>Number of Blocks: Define how many transformer blocks to include in the preprocessing pipeline.</li> <li>Number of Heads: Set the number of attention heads in each transformer block.</li> <li>Feed Forward Units: Specify the number of units in the feed-forward network of each block.</li> <li>Dropout Rate: Adjust the dropout rate to prevent overfitting during training.</li> <li>Placement: Choose whether to apply transformer blocks to only categorical features (<code>CATEGORICAL</code>) or to all features (<code>ALL_FEATURES</code>).</li> </ul> <p>Example configuration:</p> <pre><code>transfo_config = {\n    'transfo_nr_blocks': 3,\n    'transfo_nr_heads': 4,\n    'transfo_ff_units': 64,\n    'transfo_dropout_rate': 0.1,\n    'transfo_placement': 'ALL_FEATURES'\n}\n\nppr = PreprocessingModel(\n    path_data=\"data/my_data.csv\",\n    features_specs=features_spec,\n    **transfo_config\n)\n</code></pre>"},{"location":"#custom-preprocessors","title":"\ud83c\udfd7\ufe0f Custom Preprocessors","text":"<p>Tailor your preprocessing steps with custom preprocessors for each feature type. Define specific preprocessing logic that fits your data characteristics or domain-specific requirements, see \ud83d\udc40 Custom Preprocessors.</p> <ul> <li>Custom Steps: Add custom preprocessing layers or functions to the predefined feature types.</li> <li>Flexibility: Mix and match standard and custom preprocessing steps to achieve optimal data transformation.</li> </ul> <p>Example of adding a custom preprocessor:</p> <pre><code>from kdp.custom_preprocessors import MyCustomScaler\n\nfeatures_specs = {\n    \"feat1\": {\n        'feature_type': FeatureType.FLOAT_NORMALIZED,\n        'preprocessors': [MyCustomScaler()]\n    }\n}\n\nppr = PreprocessingModel(\n    path_data=\"data/my_data.csv\",\n    features_specs=features_spec\n)\n</code></pre>"},{"location":"#feature-crosses","title":"\u2671 Feature Crosses","text":"<p>Create complex feature interactions by crossing features. This method combines features into a single feature, which can be particularly useful for models that benefit from understanding interactions between specific features, see \ud83d\udc40 Feature Crosses.</p> <ul> <li>Crossing Features: Specify pairs of features to be crossed and the number of bins for hashing the crossed feature.</li> </ul> <p>Example of defining feature crosses:</p> <pre><code>feature_crosses = [\n    (\"feat1\", \"feat2\", 10),\n    (\"feat3\", \"feat4\", 5)\n]\n\nppr = PreprocessingModel(\n    path_data=\"data/my_data.csv\",\n    features_specs=features_spec,\n    feature_crosses=feature_crosses\n)\n</code></pre> <p>These advanced configurations allow for greater flexibility and power in your preprocessing pipelines, enabling more sophisticated data transformations and feature engineering.</p>"},{"location":"complex_example/","title":"\ud83d\udcda Complex Example \ud83c\udf1f","text":"<p>This example shows how to create a compound model with both transformer blocks and attention mechanisms.</p> <pre><code>import pandas as pd\nimport tensorflow as tf\nfrom kdp.features import (\n    NumericalFeature,\n    CategoricalFeature,\n    TextFeature,\n    DateFeature,\n    FeatureType\n)\nfrom kdp.processor import PreprocessingModel, OutputModeOptions\n\n# Define features\nfeatures = {\n    # Numerical features\n    \"price\": NumericalFeature(\n        name=\"price\",\n        feature_type=FeatureType.FLOAT_NORMALIZED\n    ),\n    \"quantity\": NumericalFeature(\n        name=\"quantity\",\n        feature_type=FeatureType.FLOAT_RESCALED\n    ),\n\n    # Categorical features\n    \"category\": CategoricalFeature(\n        name=\"category\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_size=32\n    ),\n    \"brand\": CategoricalFeature(\n        name=\"brand\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n        embedding_size=16\n    ),\n\n    # Text features\n    \"description\": TextFeature(\n        name=\"description\",\n        feature_type=FeatureType.TEXT,\n        max_tokens=100\n    ),\n    \"title\": TextFeature(\n        name=\"title\",\n        feature_type=FeatureType.TEXT,\n        max_tokens=50, # max number of tokens to keep\n    ),\n\n    # Date features\n    \"sale_date\": DateFeature(\n        name=\"sale_date\",\n        feature_type=FeatureType.DATE,\n        add_season=True, # adds one-hot season indicator (summer, winter, etc) defaults to False\n    )\n}\n\n# Create sample data\ndf = pd.DataFrame({\n    \"price\": [10.5, 20.0, 15.75, 30.25, 25.50] * 20,\n    \"quantity\": [5, 10, 3, 8, 12] * 20,\n    \"category\": [\"electronics\", \"books\", \"clothing\", \"food\", \"toys\"] * 20,\n    \"brand\": [\"brandA\", \"brandB\", \"brandC\", \"brandD\", \"brandE\"] * 20,\n    \"description\": [\n        \"High quality product with great features\",\n        \"Must-read book for enthusiasts\",\n        \"Comfortable and stylish clothing\",\n        \"Fresh and organic produce\",\n        \"Educational toy for children\"\n    ] * 20,\n    \"title\": [\n        \"Premium Device\",\n        \"Best Seller Book\",\n        \"Fashion Item\",\n        \"Organic Food\",\n        \"Kids Toy\"\n    ] * 20,\n    \"sale_date\": [\n        \"2023-01-15\",\n        \"2023-02-20\",\n        \"2023-03-25\",\n        \"2023-04-30\",\n        \"2023-05-05\"\n    ] * 20\n})\n\n# Format data\ndf.to_csv(\"sample_data.csv\", index=False)\ntest_batch = tf.data.Dataset.from_tensor_slices(dict(df.head(3))).batch(3)\n\n# Create preprocessor with both transformer blocks and attention\nppr = PreprocessingModel(\n    path_data=\"sample_data.csv\",\n    features_stats_path=\"features_stats.json\",\n    overwrite_stats=True,             # Force stats generation, recommended to be set to True\n    features_specs=features,\n    output_mode=OutputModeOptions.CONCAT,\n\n    # Transformer block configuration\n    transfo_placement=\"all_features\",  # Choose between (categorical|all_features)\n    transfo_nr_blocks=2,              # Number of transformer blocks\n    transfo_nr_heads=4,               # Number of attention heads in transformer\n    transfo_ff_units=64,              # Feed-forward units in transformer\n    transfo_dropout_rate=0.1,         # Dropout rate for transformer\n\n    # Tabular attention configuration\n    tabular_attention=True,\n    tabular_attention_placement=\"all_features\",  # Choose between (none|numeric|categorical|all_features| multi_resolution)\n    tabular_attention_heads=3,                   # Number of attention heads\n    tabular_attention_dim=32,                    # Attention dimension\n    tabular_attention_dropout=0.1,               # Attention dropout rate\n    tabular_attention_embedding_dim=16,          # Embedding dimension\n\n    # Feature selection configuration\n    feature_selection_placement=\"all_features\", # Choose between (all_features|numeric|categorical)\n    feature_selection_units=32,\n    feature_selection_dropout=0.15,\n)\n\n# Build the preprocessor\nresult = ppr.build_preprocessor()\n</code></pre> <p>Now if one wants to plot the a block diagram of the model or get the outout of the NN or get the importance weights of the features, use the following:</p> <pre><code># Plot the model architecture\nppr.plot_model(\"complex_model.png\")\n\n# Transform data using direct model prediction\ntransformed_data = ppr.model.predict(test_batch)\n\n# Transform data using batch_predict\ntransformed_data = ppr.batch_predict(test_batch)\ntransformed_batches = list(transformed_data)  # For better visualization\n\n# Get feature importances\nfeature_importances = ppr.get_feature_importances()\nprint(\"Feature importances:\", feature_importances)\n</code></pre> <p>Here is the plot of the model: </p>"},{"location":"contributing/","title":"\ud83d\udcbb Contributing: Join the Preprocessing Revolution! \ud83d\udee0\ufe0f","text":"<p>Eager to contribute? Great! We're excited to welcome new contributors to our project. Here's how you can get involved:</p>"},{"location":"contributing/#new-ideas-features-requests","title":"\ud83d\udca1 New Ideas / Features Requests","text":"<p>If you wan't to request a new feature or you have detected an issue, please use the following link: ISSUES</p>"},{"location":"contributing/#getting-started","title":"\ud83d\ude80 Getting Started:","text":"<ul> <li> <p> Fork the Repository: Visit our GitHub page, fork the repository, and clone it to your local machine.</p> </li> <li> <p> Set Up Your Environment: Make sure you have TensorFlow, Loguru, and all necessary dependencies installed.</p> </li> <li> <p> Make sure you have installed the pre-commit hook locally</p> </li> </ul> <p>??? installation-guide   Before using pre-commit hook you need to install it in your python environment.</p> <pre><code>    ```bash\n    conda install -c conda-forge pre-commit\n    ```\n\n    go to the root folder of this repository, activate your venv and use the following command:\n\n    ```bash\n    pre-commit install\n    ```\n</code></pre> <ul> <li> <p> Create a new branch to package your code</p> </li> <li> <p> Use standarized commit message:</p> </li> </ul> <p><code>{LABEL}(KDP): {message}</code></p> <p>This is very important for the automatic releases (semantic release) and to have clean history on the master branch.</p> <p>??? Labels-types</p> <pre><code>    | Label    | Usage                                                                                                                                                                                                                                             |\n    | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n    | break    | `break` is used to identify changes related to old compatibility or functionality that breaks the current usage (major)                                                                                                                           |\n    | feat     | `feat` is used to identify changes related to new backward-compatible abilities or functionality (minor)                                                                                                                                          |\n    | init     | `init` is used to indentify the starting related to the project (minor)                                                                                                                                                                           |\n    | enh      | `enh` is used to indentify changes related to amelioration of abilities or functionality (patch)                                                                                                                                                  |\n    | build    | `build` (also known as `chore`) is used to identify **development** changes related to the build system (involving scripts, configurations, or tools) and package dependencies (patch)                                                            |\n    | ci       | `ci` is used to identify **development** changes related to the continuous integration and deployment system - involving scripts, configurations, or tools (minor)                                                                                |\n    | docs     | `docs`  is used to identify documentation changes related to the project; whether intended externally for the end-users or internally for the developers (patch)                                                                                  |\n    | perf     | `perf`  is used to identify changes related to backward-compatible **performance improvements** (patch)                                                                                                                                           |\n    | refactor | `refactor` is used to identify changes related to modifying the codebase, which neither adds a feature nor fixes a bug - such as removing redundant code, simplifying the code, renaming variables, etc.&lt;br /&gt;i.e. handy for your wip ; ) (patch) |\n    | style    | `style`  is used to identify **development** changes related to styling the codebase, regardless of the meaning - such as indentations, semi-colons, quotes, trailing commas, and so on (patch)                                                   |\n    | test     | `test` is used to identify **development** changes related to tests - such as refactoring existing tests or adding new tests. (minor)                                                                                                             |\n    | fix      | `fix`  is used to identify changes related to backward-compatible bug fixes. (patch)                                                                                                                                                              |\n    | ops      | `ops` is used to identify changes related to deployment files like `values.yml`, `gateway.yml,` or `Jenkinsfile` in the **ops** directory. (minor)                                                                                                |\n    | hotfix   | `hotfix` is used to identify **production** changes related to backward-compatible bug fixes (patch)                                                                                                                                              |\n    | revert   | `revert` is used to identify backward changes (patch)                                                                                                                                                                                             |\n    | maint    | `maint` is used to identify **maintenance** changes related to project (patch)                                                                                                                                                                    |\n</code></pre> <ul> <li> Create your first Merge Request (MR) as soon as possible.</li> </ul> <p>Merge requests will be responsible for semantic-release storytelling and so use them wisely! The changelog report generated automatically will be based on your commits merged into main branch and should cover all the things you did for the project, as an example:</p> <ul> <li> Separate your merge requests based on LABEL or functionality if you are working on <code>feat</code> label</li> </ul> <p>This about what part of feature you are working on, (messages) i.e.:</p> <pre><code>    - `initializaing base pre-processing code`\n    - `init repo structure`\n    - `adding pre-processing unit-tests`\n</code></pre> <ul> <li> Once the code is ready create a Merge Request (MR) into the MAIN branch with a proper naming convention</li> </ul> <p>The name of your MR should follow the same exact convention as your commits (we have a dedicated check for this in the CI):</p> <pre><code>    `{LABEL}(KDP): {message}`\n</code></pre> <ul> <li> <p> Use small Merge Requests but do them more ofthen &lt; 400 lines for quicker and simple review and not the whole project !</p> </li> <li> <p> Ask for a Code Review !</p> </li> <li> <p> Once your MR is approved, solve all your unresolved conversation and pass all the CI check before you can merge it.</p> </li> <li> <p> All the Tests for your code should pass -&gt; REMEMBER NO TESTS = NO MERGE \ud83d\udea8</p> </li> </ul>"},{"location":"distribution_aware_encoder/","title":"Distribution-Aware Encoder","text":""},{"location":"distribution_aware_encoder/#overview","title":"Overview","text":"<p>The Distribution-Aware Encoder is an advanced preprocessing layer that automatically detects and handles various types of data distributions. It uses TensorFlow Probability (tfp) for accurate modeling and applies specialized transformations while preserving the statistical properties of the data.</p>"},{"location":"distribution_aware_encoder/#features","title":"Features","text":""},{"location":"distribution_aware_encoder/#distribution-types-supported","title":"Distribution Types Supported","text":"<ol> <li>Normal Distribution</li> <li>For standard normally distributed data</li> <li>Handled via z-score normalization</li> <li> <p>Detection: Kurtosis \u2248 3.0, Skewness \u2248 0</p> </li> <li> <p>Heavy-Tailed Distribution</p> </li> <li>For data with heavier tails than normal</li> <li>Handled via Student's t-distribution</li> <li> <p>Detection: Kurtosis &gt; 3.5</p> </li> <li> <p>Multimodal Distribution</p> </li> <li>For data with multiple peaks</li> <li>Handled via Gaussian Mixture Models</li> <li> <p>Detection: KDE-based peak detection</p> </li> <li> <p>Uniform Distribution</p> </li> <li>For evenly distributed data</li> <li>Handled via min-max scaling</li> <li> <p>Detection: Kurtosis \u2248 1.8</p> </li> <li> <p>Exponential Distribution</p> </li> <li>For data with exponential decay</li> <li>Handled via rate-based transformation</li> <li> <p>Detection: Skewness \u2248 2.0</p> </li> <li> <p>Log-Normal Distribution</p> </li> <li>For data that is normal after log transform</li> <li>Handled via logarithmic transformation</li> <li> <p>Detection: Log-transformed kurtosis \u2248 3.0</p> </li> <li> <p>Discrete Distribution</p> </li> <li>For data with finite distinct values</li> <li>Handled via empirical CDF-based encoding</li> <li> <p>Detection: Unique values analysis</p> </li> <li> <p>Periodic Distribution</p> </li> <li>For data with cyclic patterns</li> <li>Handled via Fourier features (sin/cos)</li> <li> <p>Detection: Autocorrelation analysis</p> </li> <li> <p>Sparse Distribution</p> </li> <li>For data with many zeros</li> <li>Handled via separate zero/non-zero transformations</li> <li> <p>Detection: Zero ratio analysis</p> </li> <li> <p>Beta Distribution</p> <ul> <li>For bounded data between 0 and 1</li> <li>Handled via beta CDF transformation</li> <li>Detection: Value range and shape analysis</li> </ul> </li> <li> <p>Gamma Distribution</p> <ul> <li>For positive, right-skewed data</li> <li>Handled via gamma CDF transformation</li> <li>Detection: Positive support and skewness</li> </ul> </li> <li> <p>Poisson Distribution</p> <ul> <li>For count data</li> <li>Handled via rate parameter estimation</li> <li>Detection: Integer values and variance\u2248mean</li> </ul> </li> <li> <p>Weibull Distribution</p> <ul> <li>For lifetime/failure data</li> <li>Handled via Weibull CDF</li> <li>Detection: Shape and scale analysis</li> </ul> </li> <li> <p>Cauchy Distribution</p> <ul> <li>For extremely heavy-tailed data</li> <li>Handled via robust location-scale estimation</li> <li>Detection: Undefined moments</li> </ul> </li> <li> <p>Zero-Inflated Distribution</p> <ul> <li>For data with excess zeros</li> <li>Handled via mixture model approach</li> <li>Detection: Zero proportion analysis</li> </ul> </li> <li> <p>Bounded Distribution</p> <ul> <li>For data with known bounds</li> <li>Handled via scaled beta transformation</li> <li>Detection: Value range analysis</li> </ul> </li> <li> <p>Ordinal Distribution</p> <ul> <li>For ordered categorical data</li> <li>Handled via learned mapping</li> <li>Detection: Discrete ordered values</li> </ul> </li> </ol>"},{"location":"distribution_aware_encoder/#usage","title":"Usage","text":""},{"location":"distribution_aware_encoder/#basic-usage","title":"Basic Usage","text":"<pre><code>from kdp.processor import PreprocessingModel\n\npreprocessor = PreprocessingModel(\n    features_stats=stats,\n    features_specs=specs,\n    use_distribution_aware=True\n)\n</code></pre>"},{"location":"distribution_aware_encoder/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>encoder = DistributionAwareEncoder(\n    num_bins=1000,\n    epsilon=1e-6,\n    detect_periodicity=True,\n    handle_sparsity=True,\n    adaptive_binning=True,\n    mixture_components=3,\n    trainable=True\n)\n</code></pre>"},{"location":"distribution_aware_encoder/#parameters","title":"Parameters","text":"Parameter Type Default Description num_bins int 1000 Number of bins for quantile encoding epsilon float 1e-6 Small value for numerical stability detect_periodicity bool True Enable periodic pattern detection handle_sparsity bool True Enable special handling for sparse data adaptive_binning bool True Enable adaptive bin boundaries mixture_components int 3 Number of components for mixture models trainable bool True Whether parameters are trainable"},{"location":"distribution_aware_encoder/#key-features","title":"Key Features","text":""},{"location":"distribution_aware_encoder/#1-automatic-distribution-detection","title":"1. Automatic Distribution Detection","text":"<ul> <li>Uses statistical moments and tests</li> <li>Employs KDE for multimodality detection</li> <li>Handles mixed distributions via ensemble approach</li> </ul>"},{"location":"distribution_aware_encoder/#2-adaptive-transformations","title":"2. Adaptive Transformations","text":"<ul> <li>Learns optimal parameters during training</li> <li>Adjusts to data distribution changes</li> <li>Handles complex periodic patterns</li> </ul>"},{"location":"distribution_aware_encoder/#3-fourier-feature-generation","title":"3. Fourier Feature Generation","text":"<ul> <li>Automatic frequency detection</li> <li>Multiple harmonic components</li> <li>Phase-aware transformations</li> </ul>"},{"location":"distribution_aware_encoder/#4-robust-handling","title":"4. Robust Handling","text":"<ul> <li>Special treatment for zeros</li> <li>Outlier-resistant transformations</li> <li>Numerical stability safeguards</li> </ul>"},{"location":"distribution_aware_encoder/#implementation-details","title":"Implementation Details","text":""},{"location":"distribution_aware_encoder/#1-periodic-data-handling","title":"1. Periodic Data Handling","text":"<pre><code># Normalize to [-\u03c0, \u03c0] range\nnormalized = inputs * \u03c0 / scale\n# Generate Fourier features\nfeatures = [\n    sin(freq * normalized + phase),\n    cos(freq * normalized + phase)\n]\n# Add harmonics if multimodal\nif is_multimodal:\n    for h in [2, 3, 4]:\n        features.extend([\n            sin(h * freq * normalized + phase),\n            cos(h * freq * normalized + phase)\n        ])\n</code></pre>"},{"location":"distribution_aware_encoder/#2-distribution-detection","title":"2. Distribution Detection","text":"<pre><code># Statistical moments\nmean = tf.reduce_mean(inputs)\nvariance = tf.math.reduce_variance(inputs)\nskewness = compute_skewness(inputs)\nkurtosis = compute_kurtosis(inputs)\n\n# Distribution tests\nis_normal = test_normality(inputs)\nis_multimodal = detect_multimodality(inputs)\nis_periodic = check_periodicity(inputs)\n</code></pre>"},{"location":"distribution_aware_encoder/#3-adaptive-parameters","title":"3. Adaptive Parameters","text":"<pre><code>self.boundaries = self.add_weight(\n    name=\"boundaries\",\n    shape=(num_bins - 1,),\n    initializer=\"zeros\",\n    trainable=adaptive_binning\n)\n\nself.mixture_weights = self.add_weight(\n    name=\"mixture_weights\",\n    shape=(mixture_components,),\n    initializer=\"ones\",\n    trainable=True\n)\n</code></pre>"},{"location":"distribution_aware_encoder/#best-practices","title":"Best Practices","text":"<ol> <li>Data Preparation</li> <li>Clean obvious outliers</li> <li>Handle missing values</li> <li> <p>Ensure numeric data types</p> </li> <li> <p>Configuration</p> </li> <li>Enable periodicity detection for time-related features</li> <li>Use adaptive binning for changing distributions</li> <li> <p>Adjust mixture components based on complexity</p> </li> <li> <p>Performance</p> </li> <li>Use appropriate batch sizes</li> <li>Enable caching when possible</li> <li> <p>Monitor transformation times</p> </li> <li> <p>Monitoring</p> </li> <li>Check distribution detection accuracy</li> <li>Validate transformation quality</li> <li>Watch for numerical instabilities</li> </ol>"},{"location":"distribution_aware_encoder/#integration-with-preprocessing-pipeline","title":"Integration with Preprocessing Pipeline","text":"<p>The DistributionAwareEncoder is integrated into the numeric feature processing pipeline:</p> <ol> <li>Feature Statistics Collection</li> <li>Basic statistics (mean, variance)</li> <li>Distribution characteristics</li> <li> <p>Sparsity patterns</p> </li> <li> <p>Automatic Distribution Detection</p> </li> <li>Statistical tests</li> <li>Pattern recognition</li> <li> <p>Threshold-based decisions</p> </li> <li> <p>Dynamic Transformation</p> </li> <li>Distribution-specific handling</li> <li>Adaptive parameter adjustment</li> <li>Quality monitoring</li> </ol>"},{"location":"distribution_aware_encoder/#performance-considerations","title":"Performance Considerations","text":""},{"location":"distribution_aware_encoder/#memory-usage","title":"Memory Usage","text":"<ul> <li>Adaptive binning weights: O(num_bins)</li> <li>GMM parameters: O(mixture_components)</li> <li>Periodic components: O(1)</li> </ul>"},{"location":"distribution_aware_encoder/#computational-complexity","title":"Computational Complexity","text":"<ul> <li>Distribution detection: O(n)</li> <li>Transformation: O(n)</li> <li>GMM fitting: O(n * mixture_components)</li> </ul>"},{"location":"distribution_aware_encoder/#best-practices_1","title":"Best Practices","text":"<ol> <li>Data Preparation</li> <li>Clean outliers if not meaningful</li> <li>Handle missing values before encoding</li> <li> <p>Ensure numeric data type</p> </li> <li> <p>Configuration</p> </li> <li>Start with default parameters</li> <li>Adjust based on data characteristics</li> <li> <p>Monitor distribution detection results</p> </li> <li> <p>Performance Optimization</p> </li> <li>Use appropriate batch sizes</li> <li>Enable caching for repeated processing</li> <li>Adjust mixture components based on data</li> </ol>"},{"location":"distribution_aware_encoder/#example-use-cases","title":"Example Use Cases","text":""},{"location":"distribution_aware_encoder/#1-financial-data","title":"1. Financial Data","text":"<pre><code># Handle heavy-tailed return distributions\npreprocessor = PreprocessingModel(\n    use_distribution_aware=True,\n    handle_sparsity=False,\n    mixture_components=2\n)\n</code></pre>"},{"location":"distribution_aware_encoder/#2-temporal-data","title":"2. Temporal Data","text":"<pre><code># Handle periodic patterns\npreprocessor = PreprocessingModel(\n    use_distribution_aware=True,\n    detect_periodicity=True,\n    adaptive_binning=True\n)\n</code></pre>"},{"location":"distribution_aware_encoder/#3-sparse-features","title":"3. Sparse Features","text":"<pre><code># Handle sparse categorical data\npreprocessor = PreprocessingModel(\n    use_distribution_aware=True,\n    handle_sparsity=True,\n    mixture_components=1\n)\n</code></pre>"},{"location":"distribution_aware_encoder/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"distribution_aware_encoder/#distribution-detection","title":"Distribution Detection","text":"<pre><code># Access distribution information\ndist_info = encoder._estimate_distribution(inputs)\nprint(f\"Detected distribution: {dist_info['type']}\")\nprint(f\"Statistics: {dist_info['stats']}\")\n</code></pre>"},{"location":"distribution_aware_encoder/#transformation-quality","title":"Transformation Quality","text":"<pre><code># Monitor transformed output statistics\ntransformed = encoder(inputs)\nprint(f\"Output mean: {tf.reduce_mean(transformed)}\")\nprint(f\"Output variance: {tf.math.reduce_variance(transformed)}\")\n</code></pre>"},{"location":"feature_selection/","title":"\ud83c\udfaf Feature Selection in KDP","text":""},{"location":"feature_selection/#overview","title":"\ud83d\udcda Overview","text":"<p>KDP includes a sophisticated feature selection mechanism based on the Gated Residual Variable Selection Network (GRVSN) architecture. This powerful system automatically learns and selects the most important features in your data.</p>"},{"location":"feature_selection/#core-components","title":"\ud83e\udde9 Core Components","text":""},{"location":"feature_selection/#1-gatedlinearunit","title":"1. \ud83d\udd00 GatedLinearUnit","text":"<p>The foundation of our feature selection system:</p> <pre><code>gl = GatedLinearUnit(units=64)\nx = tf.random.normal((32, 100))\ny = gl(x)\n</code></pre> <p>Key Features: * \ud83d\udd04 Applies linear transformation with sigmoid gate * \ud83c\udf9b\ufe0f Selectively filters input data * \ud83d\udd0d Controls information flow through the network</p>"},{"location":"feature_selection/#2-gatedresidualnetwork","title":"2. \ud83c\udfd7\ufe0f GatedResidualNetwork","text":"<p>Combines gated units with residual connections:</p> <pre><code>grn = GatedResidualNetwork(units=64, dropout_rate=0.2)\nx = tf.random.normal((32, 100))\ny = grn(x)\n</code></pre> <p>Key Features: * \u26a1 Uses ELU activation for non-linearity * \ud83c\udfb2 Includes dropout for regularization * \ud83d\udd04 Adds residual connections for better gradient flow * \ud83d\udcca Applies layer normalization for stability</p>"},{"location":"feature_selection/#3-variableselection","title":"3. \ud83c\udfaf VariableSelection","text":"<p>The main feature selection component:</p> <pre><code>vs = VariableSelection(nr_features=3, units=64, dropout_rate=0.2)\nx1 = tf.random.normal((32, 100))\nx2 = tf.random.normal((32, 200))\nx3 = tf.random.normal((32, 300))\nselected_features, weights = vs([x1, x2, x3])\n</code></pre> <p>Key Features: * \ud83d\udd04 Independent GRN processing for each feature * \u2696\ufe0f Calculates feature importance weights via softmax * \ud83d\udcca Returns both selected features and their weights * \ud83d\udd27 Supports varying input dimensions per feature</p>"},{"location":"feature_selection/#usage-guide","title":"\ud83d\udcbb Usage Guide","text":""},{"location":"feature_selection/#configuration","title":"Configuration","text":"<p>Set up feature selection in your preprocessing model:</p> <pre><code>model = PreprocessingModel(\n    # ... other parameters ...\n    feature_selection_placement=\"all_features\",  # or \"numeric\" or \"categorical\"\n    feature_selection_units=64,\n    feature_selection_dropout=0.2\n)\n</code></pre>"},{"location":"feature_selection/#placement-options","title":"\ud83c\udfaf Placement Options","text":"<p>Choose where to apply feature selection using <code>FeatureSelectionPlacementOptions</code>:</p> Option Description <code>NONE</code> Disable feature selection <code>NUMERIC</code> Apply to numeric features only <code>CATEGORICAL</code> Apply to categorical features only <code>ALL_FEATURES</code> Apply to all features"},{"location":"feature_selection/#accessing-feature-weights","title":"\ud83d\udcca Accessing Feature Weights","text":"<p>Monitor feature importance after processing:</p> <pre><code># Process your data\nprocessed = model.transform(data)\n\n# Access feature weights\nnumeric_weights = processed[\"numeric_feature_weights\"]\ncategorical_weights = processed[\"categorical_feature_weights\"]\n\n# Print feature importance\nfor feature_name in features:\n    weights = processed_data[f\"{feature_name}_weights\"]\n    print(f\"Feature {feature_name} importance: {weights.mean()}\")\n</code></pre>"},{"location":"feature_selection/#benefits","title":"\ud83c\udf1f Benefits","text":"<ol> <li>\ud83e\udd16 Automatic Feature Selection</li> <li>Learns feature importance automatically</li> <li>Adapts to your specific dataset</li> <li> <p>Reduces manual feature engineering</p> </li> <li> <p>\ud83d\udcca Interpretability</p> </li> <li>Clear feature importance weights</li> <li>Insights into model decisions</li> <li> <p>Easy to explain to stakeholders</p> </li> <li> <p>\u26a1 Improved Performance</p> </li> <li>Focuses on relevant features</li> <li>Reduces noise in the data</li> <li>Better model convergence</li> </ol>"},{"location":"feature_selection/#best-practices","title":"\ud83d\udd27 Best Practices","text":""},{"location":"feature_selection/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<ul> <li>\ud83c\udfaf Start with default values</li> <li>\ud83d\udcc8 Adjust based on validation performance</li> <li>\ud83d\udd04 Monitor feature importance stability</li> </ul>"},{"location":"feature_selection/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>\u26a1 Use appropriate batch sizes</li> <li>\ud83c\udfb2 Adjust dropout rates as needed</li> <li>\ud83d\udcca Monitor memory usage</li> </ul>"},{"location":"feature_selection/#references","title":"\ud83d\udcda References","text":"<ul> <li>GRVSN Paper</li> <li>Feature Selection in Deep Learning</li> <li>KDP Documentation</li> </ul>"},{"location":"feature_selection/#example","title":"\ud83d\udcda Example","text":"<p>Here's a complete example of using feature selection:</p> <pre><code>from kdp.processor import PreprocessingModel\nfrom kdp.features import NumericalFeature, CategoricalFeature\n\n# Define features\nfeatures = {\n    \"numeric_1\": NumericalFeature(\n        name=\"numeric_1\",\n        feature_type=FeatureType.FLOAT_NORMALIZED\n    ),\n    \"numeric_2\": NumericalFeature(\n        name=\"numeric_2\",\n        feature_type=FeatureType.FLOAT_NORMALIZED\n    ),\n    \"category_1\": CategoricalFeature(\n        name=\"category_1\",\n        feature_type=FeatureType.STRING_CATEGORICAL\n    )\n}\n\n# Create model with feature selection\nmodel = PreprocessingModel(\n    # ... other parameters ...\n    features_specs=features,\n    feature_selection_placement=\"all_features\", # or \"numeric\" or \"categorical\"\n    feature_selection_units=64,\n    feature_selection_dropout=0.2\n)\n\n# Build and use the model\npreprocessor = model.build_preprocessor()\nprocessed_data = model.transform(data) # data can be pd.DataFrame, python Dict, or tf.data.Dataset\n\n# Analyze feature importance\nfor feature_name in features:\n    weights = processed_data[f\"{feature_name}_weights\"]\n    print(f\"Feature {feature_name} importance: {weights.mean()}\")\n</code></pre>"},{"location":"feature_selection/#testing","title":"\ud83d\udcca Testing","text":"<p>The feature selection components include comprehensive unit tests that verify:</p> <ol> <li>Output shapes and types</li> <li>Gating mechanism behavior</li> <li>Residual connections</li> <li>Dropout behavior</li> <li>Feature weight properties</li> <li>Serialization/deserialization</li> </ol> <p>Run the tests using: ```bash python -m pytest test/test_feature_selection.py -v</p>"},{"location":"features/","title":"Defining Features for Preprocessing \ud83c\udf1f","text":"<p>Customize the preprocessing pipeline by setting up a dictionary that maps feature names to their respective types, tailored to your specific requirements.</p>"},{"location":"features/#numeric-features","title":"\ud83d\udcaf Numeric Features","text":"<p>Explore various methods to define numerical features tailored to your needs:</p> \u2139\ufe0f Simple Declaration\ud83d\udd27 Using FeatureType\ud83d\udcaa Custom NumericalFeature <pre><code>features_specs = {\n    \"feat1\": \"float\",\n    \"feat2\": \"FLOAT\",\n    \"feat3\": \"FLOAT_NORMALIZED\",\n    \"feat3\": \"FLOAT_RESCALED\",\n    ...\n}\n</code></pre> <p>Utilize predefined preprocessing configurations with <code>FeatureType</code>.</p> <pre><code>from kdp.features import FeatureType\n\nfeatures_specs = {\n    \"feat1\": FeatureType.FLOAT_NORMALIZED,\n    \"feat2\": FeatureType.FLOAT_RESCALED,\n    ...\n}\n</code></pre> <p>Available <code>FeatureType</code> options:</p> <ul> <li>FLOAT</li> <li>FLOAT_NORMALIZED</li> <li>FLOAT_RESCALED</li> <li>FLOAT_DISCRETIZED</li> </ul> <p>Customize preprocessing by passing specific parameters to <code>NumericalFeature</code>.</p> <pre><code>from kdp.features import NumericalFeature\n\nfeatures_specs = {\n    \"feat3\": NumericalFeature(\n        name=\"feat3\",\n        feature_type=FeatureType.FLOAT_DISCRETIZED,\n        bin_boundaries=[(1, 10)],\n    ),\n    \"feat4\": NumericalFeature(\n        name=\"feat4\",\n        feature_type=FeatureType.FLOAT,\n    ),\n    ...\n}\n</code></pre> <p>Here's how the numeric preprocessing pipeline looks:</p> <p></p>"},{"location":"features/#categorical-features","title":"\ud83d\udc08\u200d\u2b1b Categorical Features","text":"<p>Define categorical features flexibly:</p> \u2139\ufe0f Simple Declaration\ud83d\udd27 Using FeatureType\ud83d\udcaa Custom CategoricalFeature <pre><code>features_specs = {\n    \"feat1\": \"INTEGER_CATEGORICAL\",\n    \"feat2\": \"STRING_CATEGORICAL\",\n    \"feat3\": \"string_categorical\",\n    ...\n}\n</code></pre> <p>Leverage default configurations with <code>FeatureType</code>.</p> <pre><code>from kdp.features import FeatureType\n\nfeatures_specs = {\n    \"feat1\": FeatureType.INTEGER_CATEGORICAL,\n    \"feat2\": FeatureType.STRING_CATEGORICAL,\n    ...\n}\n</code></pre> <p>Available <code>FeatureType</code> options:</p> <ul> <li>STRING_CATEGORICAL</li> <li>INTEGER_CATEGORICAL</li> </ul> <p>Tailor feature processing by specifying properties in <code>CategoricalFeature</code>.</p> <pre><code>from kdp.features\nfrom kdp.features import CategoricalFeature\n\nfeatures_specs = {\n    \"feat1\": CategoricalFeature(\n        name=\"feat7\",\n        feature_type=FeatureType.INTEGER_CATEGORICAL,\n        embedding_size=100,\n    ),\n    \"feat2\": CategoricalFeature(\n        name=\"feat2\",\n        feature_type=FeatureType.STRING_CATEGORICAL,\n    ),\n    ...\n}\n</code></pre> <p>See how the categorical preprocessing pipeline appears:</p> <p></p>"},{"location":"features/#text-features","title":"\ud83d\udcdd Text Features","text":"<p>Customize text features in multiple ways to fit your project's demands:</p> \u2139\ufe0f Simple Declaration\ud83d\udd27 Using FeatureType\ud83d\udcaa Custom TextFeature <pre><code>features_specs = {\n    \"feat1\": \"text\",\n    \"feat2\": \"TEXT\",\n    ...\n}\n</code></pre> <p>Use <code>FeatureType</code> for automatic default preprocessing setups.</p> <pre><code>from kdp.features import FeatureType\n\nfeatures_specs = {\n    \"feat1\": FeatureType.TEXT,\n    \"feat2\": FeatureType.TEXT,\n    ...\n}\n</code></pre> <p>Available <code>FeatureType</code> options:</p> <ul> <li>TEXT</li> </ul> <p>Customize text preprocessing by passing specific arguments to <code>TextFeature</code>.</p> <pre><code>from kdp.features import TextFeature\n\nfeatures_specs = {\n    \"feat1\": TextFeature(\n        name=\"feat2\",\n        feature_type=FeatureType.TEXT,\n        max_tokens=100,\n        stop_words=[\"stop\", \"next\"],\n    ),\n    \"feat2\": TextFeature(\n        name=\"feat2\",\n        feature_type=FeatureType.TEXT,\n    ),\n    ...\n}\n</code></pre> <p>Here's how the text feature preprocessing pipeline looks:</p> <p></p>"},{"location":"features/#cross-features","title":"\u274c Cross Features","text":"<p>To implement cross features, specify a list of feature tuples in the <code>PreprocessingModel</code> like so:</p> <pre><code>from kdp.processor import PreprocessingModel\n\nppr = PreprocessingModel(\n    path_data=\"data/data.csv\",\n    features_specs={\n        \"feat6\": FeatureType.STRING_CATEGORICAL,\n        \"feat7\": FeatureType.INTEGER_CATEGORICAL,\n    },\n    feature_crosses=[(\"feat6\", \"feat7\", 5)],\n)\n</code></pre> <p>Example cross feature between INTEGER_CATEGORICAL and STRING_CATEGORICAL:</p> <p></p>"},{"location":"features/#date-features","title":"\ud83d\udcc6 Date Features","text":"<p>You can even process string encoded date features (format: 'YYYY-MM-DD' or 'YYYY/MM/DD'):</p> \ud83d\udd27 Using FeatureType\ud83d\udcaa Custom DateFeature <p>Use <code>FeatureType</code> for automatic default preprocessing setups.</p> <pre><code>from kdp.processor import PreprocessingModel\n\nppr = PreprocessingModel(\n    path_data=\"data/data.csv\",\n    features_specs={\n        \"feat1\": FeatureType.FLOAT,\n        \"feat2\": FeatureType.DATE,\n    },\n)\n</code></pre> <p>Customize text preprocessing by passing specific arguments to <code>TextFeature</code>.</p> <pre><code>from kdp.features import DateFeature\n\nfeatures_specs = {\n    \"feat1\": DateFeature(\n        name=\"feat1\",\n        feature_type=FeatureType.DATE,\n    ),\n    \"feat2\": DateFeature(\n        name=\"feat2\",\n        feature_type=FeatureType.DATE,\n        date_format=\"%Y-%m-%d\", # date format of the input data\n        output_format=\"year\", # output format of the feature\n        # additional option to add season layer:\n        add_season=True,  # adds one-hot season indicator (summer, winter, autumn or spring) defaults to False\n    ),\n    ...\n}\n</code></pre> <p>Example date and numeric processing pipeline:</p> <p></p>"},{"location":"features/#custom-preprocessing-steps","title":"\ud83d\ude80 Custom Preprocessing Steps","text":"<p>If you require even more customization, you can define custom preprocessing steps using the <code>Feature</code> class, using <code>preprocessors</code> attribute.</p> <p>The <code>preprocessors</code> attribute accepts a list of methods defined in <code>PreprocessorLayerFactory</code>.</p> <pre><code>from kdp.features import Feature\n\nfeatures_specs = {\n    \"feat1\": FeatureType.FLOAT_NORMALIZED,\n    \"feat2\": Feature(\n        name=\"custom_feature_pipeline\",\n        feature_type=FeatureType.FLOAT_NORMALIZED,\n        preprocessors=[\n            tf.keras.layers.Rescaling,\n            tf.keras.layers.Normalization,\n\n        ],\n        # leyers required kwargs\n        scale=1,\n    )\n}\n</code></pre> <p>Here's how the text feature preprocessing pipeline looks:</p> <p></p> <p>The full list of available layers can be found: Preprocessing Layers Factory</p>"},{"location":"integrations/","title":"\ud83d\udd17 Integrating Preprocessing Model with other Keras Model:","text":"<p>You can then easily ingetrate this model into your keras model as the first layer:</p>"},{"location":"integrations/#example-1-using-the-preprocessing-model-as-the-first-layer-of-a-sequential-model","title":"Example 1: Using the Preprocessing Model as the first layer of a Sequential Model","text":"<pre><code>class FunctionalModelWithPreprocessing(tf.keras.Model):\n    def __init__(self, preprocessing_model: tf.keras.Model) -&gt; None:\n        \"\"\"Initialize the user model.\n\n        Args:\n            preprocessing_model (tf.keras.Model): The preprocessing model.\n        \"\"\"\n        super().__init__()\n        self.preprocessing_model = preprocessing_model\n\n        # Dynamically create inputs based on the preprocessing model's input shape\n        inputs = {\n            name: tf.keras.Input(shape=shape[1:], name=name)\n            for name, shape in self.preprocessing_model.input_shape.items()\n        }\n\n        # You can use the preprocessing model directly in the functional API.\n        x = self.preprocessing_model(inputs)\n\n        # Define the dense layer as part of the model architecture\n        output = tf.keras.layers.Dense(\n            units=128,\n            activation=\"relu\",\n        )(x)\n\n        # Use the Model's functional API to define inputs and outputs\n        self.model = tf.keras.Model(inputs=inputs, outputs=output)\n\n    def call(self, inputs: dict[str, tf.Tensor]) -&gt; tf.Tensor:\n        \"\"\"Call the item model with the given inputs.\"\"\"\n        return self.model(inputs)\n\n# Defining this model is not easy with builtin preprocessing layers:\n\nfrom kdp import PreprocessingModel\nfrom kdp import FeatureType\n\n# DEFINING FEATURES PROCESSORS\nfeatures_specs = {\n    # ======= NUMERICAL Features =========================\n    \"feat1\": FeatureType.FLOAT_NORMALIZED,\n    \"feat2\": FeatureType.FLOAT_RESCALED,\n    # ======= CATEGORICAL Features ========================\n    \"feat3\": FeatureType.STRING_CATEGORICAL,\n    \"feat4\": FeatureType.INTEGER_CATEGORICAL,\n    # ======= TEXT Features ========================\n    \"feat5\": FeatureType.TEXT,\n}\n\n# INSTANTIATE THE PREPROCESSING MODEL with your data\nppr = PreprocessingModel(\n    path_data=\"data/my_data.csv\",\n    features_specs=features_spec,\n)\n# construct the preprocessing pipelines\nppr.build_preprocessor()\n\n# building a production / deployment ready model\nfull_model = FunctionalModelWithPreprocessing(\n    preprocessing_model=ppr.model,\n)\n</code></pre>"},{"location":"kdp_overview/","title":"\ud83d\udca1 Understanding KDP","text":""},{"location":"kdp_overview/#what-is-kdp","title":"\ud83c\udfaf What is KDP?","text":"<p>KDP (Keras Data Processor) is a powerful preprocessing library designed to streamline and enhance data preparation for deep learning models. It combines modern deep learning techniques with traditional preprocessing methods to create a flexible and efficient data processing pipeline.</p>"},{"location":"kdp_overview/#key-features","title":"\ud83c\udf1f Key Features","text":""},{"location":"kdp_overview/#1-unified-preprocessing","title":"1. \ud83d\udd04 Unified Preprocessing","text":"<ul> <li>Single interface for all preprocessing needs</li> <li>Seamless integration with Keras models</li> <li>End-to-end differentiable pipeline</li> </ul>"},{"location":"kdp_overview/#2-advanced-feature-processing","title":"2. \ud83c\udf9b\ufe0f Advanced Feature Processing","text":"<ul> <li>Numerical Features</li> <li>Multiple scaling options</li> <li>Automatic outlier handling</li> <li> <p>Missing value imputation</p> </li> <li> <p>Categorical Features</p> </li> <li>Learned embeddings</li> <li>Automatic vocabulary management</li> <li>Handling of unknown categories</li> </ul>"},{"location":"kdp_overview/#3-deep-learning-enhancements","title":"3. \ud83e\udde0 Deep Learning Enhancements","text":"<ul> <li>Tabular Attention</li> <li>Feature interaction modeling</li> <li>Adaptive feature importance</li> <li> <p>Multi-head attention support</p> </li> <li> <p>Feature Selection</p> </li> <li>Automatic importance learning</li> <li>Dynamic feature filtering</li> <li>Interpretable weights</li> </ul>"},{"location":"kdp_overview/#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":"<pre><code>graph TD\n    A[Raw Data] --&gt; B[Feature Definition]\n    B --&gt; C[Preprocessing Model]\n    C --&gt; D[Feature Processing]\n    D --&gt; E[Deep Learning Extensions]\n    E --&gt; F[Processed Features]\n\n    subgraph \"Feature Processing\"\n    D1[Numerical Processing]\n    D2[Categorical Processing]\n    end\n\n    subgraph \"Extensions\"\n    E1[Tabular Attention]\n    E2[Feature Selection]\n    E3[Transformer Blocks]\n    end</code></pre>"},{"location":"kdp_overview/#why-choose-kdp","title":"\ud83d\udcaa Why Choose KDP?","text":""},{"location":"kdp_overview/#1-simplicity","title":"1. \ud83c\udfaf Simplicity","text":"<ul> <li>Intuitive API design</li> <li>Minimal boilerplate code</li> <li>Clear documentation</li> </ul>"},{"location":"kdp_overview/#2-performance","title":"2. \ud83d\ude80 Performance","text":"<ul> <li>Optimized for large datasets</li> <li>GPU acceleration support</li> <li>Memory-efficient processing</li> </ul>"},{"location":"kdp_overview/#3-flexibility","title":"3. \ud83d\udd27 Flexibility","text":"<ul> <li>Customizable preprocessing</li> <li>Extensible architecture</li> <li>Framework agnostic</li> </ul>"},{"location":"kdp_overview/#4-integration","title":"4. \ud83e\udd1d Integration","text":"<ul> <li>Seamless Keras integration</li> <li>Easy model export/import</li> <li>Cloud platform support</li> </ul>"},{"location":"kdp_overview/#core-components","title":"\ud83d\udee0\ufe0f Core Components","text":""},{"location":"kdp_overview/#1-feature-definitions","title":"1. Feature Definitions","text":"<ul> <li>Define data types and processing</li> <li>Configure feature-specific parameters</li> <li>Set preprocessing strategies</li> </ul>"},{"location":"kdp_overview/#2-preprocessing-model","title":"2. Preprocessing Model","text":"<ul> <li>Manages feature transformations</li> <li>Handles data flow</li> <li>Maintains state</li> </ul>"},{"location":"kdp_overview/#3-extensions","title":"3. Extensions","text":"<ul> <li>Add advanced capabilities</li> <li>Enhance preprocessing</li> <li>Improve model performance</li> </ul>"},{"location":"kdp_overview/#use-cases","title":"\ud83d\udcc8 Use Cases","text":""},{"location":"kdp_overview/#1-tabular-data","title":"1. \ud83d\udcca Tabular Data","text":"<ul> <li>Financial data processing</li> <li>Customer analytics</li> <li>Time series analysis</li> </ul>"},{"location":"kdp_overview/#2-feature-engineering","title":"2. \ud83c\udfaf Feature Engineering","text":"<ul> <li>Automatic feature selection</li> <li>Feature interaction modeling</li> <li>Dimensionality reduction</li> </ul>"},{"location":"kdp_overview/#3-model-integration","title":"3. \ud83d\udd04 Model Integration","text":"<ul> <li>Deep learning pipelines</li> <li>AutoML systems</li> <li>Production deployments</li> </ul>"},{"location":"kdp_overview/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ol> <li>Check out our Quick Start Guide</li> <li>Explore Key Features</li> <li>Try Complex Examples</li> </ol>"},{"location":"kdp_overview/#learning-path","title":"\ud83d\udcda Learning Path","text":"<ol> <li>\ud83c\udf93 Beginner</li> <li>Basic feature definition</li> <li>Simple preprocessing</li> <li> <p>Data transformation</p> </li> <li> <p>\ud83c\udfc3 Intermediate</p> </li> <li>Advanced features</li> <li>Custom preprocessing</li> <li> <p>Performance optimization</p> </li> <li> <p>\ud83d\ude80 Advanced</p> </li> <li>Extension development</li> <li>Pipeline optimization</li> <li>Production deployment</li> </ol>"},{"location":"kdp_overview/#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ul> <li>\ud83d\udee0\ufe0f Key Features</li> <li>\ud83d\ude80 Quick Start</li> <li>\ud83d\udcda Complex Examples</li> <li>\ud83e\udd1d Contributing Guide</li> </ul>"},{"location":"layers_factory/","title":"\ud83c\udfed Preprocessing Layers Factory","text":"<p>The <code>PreprocessorLayerFactory</code> class provides a convenient way to create and manage preprocessing layers for your machine learning models. It supports both standard Keras preprocessing layers and custom layers defined within the KDP framework.</p>"},{"location":"layers_factory/#using-keras-preprocessing-layers","title":"\ud83c\udfa1 Using Keras Preprocessing Layers","text":"<p>All preprocessing layers available in Keras can be used within the <code>PreprocessorLayerFactory</code>. You can access these layers by their class names. Here's an example of how to use a Keras preprocessing layer:</p> <p><pre><code>normalization_layer = PreprocessorLayerFactory.create_layer(\n    \"Normalization\",\n    axis=-1,\n    mean=None,\n    variance=None\n)\n</code></pre> Available layers:</p> <ul> <li> Normalization</li> <li> Discretization</li> <li> CategoryEncoding</li> <li> Hashing</li> <li> HashedCrossing</li> <li> StringLookup</li> <li> IntegerLookup</li> <li> TextVectorization</li> <li> ... and more</li> </ul>"},{"location":"layers_factory/#custom-kdp-preprocessing-layers","title":"\ud83c\udfd7\ufe0f Custom KDP Preprocessing Layers","text":"<p>In addition to Keras layers, the <code>PreprocessorLayerFactory</code> includes several custom layers specific to the KDP framework. Here's a list of available custom layers:</p>"},{"location":"layers_factory/#kdp.layers_factory.PreprocessorLayerFactory.cast_to_float32_layer","title":"<code>cast_to_float32_layer(name='cast_to_float32', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a CastToFloat32Layer layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'cast_to_float32'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the CastToFloat32Layer layer.</p>"},{"location":"layers_factory/#kdp.layers_factory.PreprocessorLayerFactory.create_layer","title":"<code>create_layer(layer_class, name=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a layer using the layer class name, automatically filtering kwargs based on the layer class.</p> <p>Parameters:</p> Name Type Description Default <code>layer_class</code> <code>str | Class Object</code> <p>The name of the layer class to be created (e.g., 'Normalization', 'Rescaling') or the class object itself.</p> required <code>name</code> <code>str</code> <p>The name of the layer. Optional.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the specified layer class.</p>"},{"location":"layers_factory/#kdp.layers_factory.PreprocessorLayerFactory.date_encoding_layer","title":"<code>date_encoding_layer(name='date_encoding_layer', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a DateEncodingLayer layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'date_encoding_layer'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the DateEncodingLayer layer.</p>"},{"location":"layers_factory/#kdp.layers_factory.PreprocessorLayerFactory.date_parsing_layer","title":"<code>date_parsing_layer(name='date_parsing_layer', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a DateParsingLayer layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'date_parsing_layer'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the DateParsingLayer layer.</p>"},{"location":"layers_factory/#kdp.layers_factory.PreprocessorLayerFactory.date_season_layer","title":"<code>date_season_layer(name='date_season_layer', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a SeasonLayer layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'date_season_layer'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the SeasonLayer layer.</p>"},{"location":"layers_factory/#kdp.layers_factory.PreprocessorLayerFactory.distribution_aware_encoder","title":"<code>distribution_aware_encoder(name='distribution_aware', num_bins=1000, epsilon=1e-06, detect_periodicity=True, handle_sparsity=True, adaptive_binning=True, mixture_components=3, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a DistributionAwareEncoder layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'distribution_aware'</code> <code>num_bins</code> <code>int</code> <p>Number of bins for quantile encoding</p> <code>1000</code> <code>epsilon</code> <code>float</code> <p>Small value for numerical stability</p> <code>1e-06</code> <code>detect_periodicity</code> <code>bool</code> <p>Whether to detect and handle periodic patterns</p> <code>True</code> <code>handle_sparsity</code> <code>bool</code> <p>Whether to handle sparse data specially</p> <code>True</code> <code>adaptive_binning</code> <code>bool</code> <p>Whether to use adaptive binning</p> <code>True</code> <code>mixture_components</code> <code>int</code> <p>Number of components for mixture modeling</p> <code>3</code> <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>DistributionAwareEncoder layer</p>"},{"location":"layers_factory/#kdp.layers_factory.PreprocessorLayerFactory.multi_resolution_attention_layer","title":"<code>multi_resolution_attention_layer(num_heads, d_model, embedding_dim=32, name='multi_resolution_attention', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a MultiResolutionTabularAttention layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>d_model</code> <code>int</code> <p>Dimensionality of the attention model</p> required <code>embedding_dim</code> <code>int</code> <p>Dimension for categorical embeddings</p> <code>32</code> <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'multi_resolution_attention'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to pass to the layer</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>MultiResolutionTabularAttention</code> <code>tf.keras.layers.Layer</code> <p>A MultiResolutionTabularAttention layer instance</p>"},{"location":"layers_factory/#kdp.layers_factory.PreprocessorLayerFactory.tabular_attention_layer","title":"<code>tabular_attention_layer(num_heads, d_model, name='tabular_attention', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a TabularAttention layer.</p> <p>Parameters:</p> Name Type Description Default <code>num_heads</code> <code>int</code> <p>Number of attention heads</p> required <code>d_model</code> <code>int</code> <p>Dimensionality of the attention model</p> required <code>name</code> <code>str</code> <p>Name of the layer</p> <code>'tabular_attention'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to pass to the layer</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>TabularAttention</code> <code>tf.keras.layers.Layer</code> <p>A TabularAttention layer instance</p>"},{"location":"layers_factory/#kdp.layers_factory.PreprocessorLayerFactory.text_preprocessing_layer","title":"<code>text_preprocessing_layer(name='text_preprocessing', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a TextPreprocessingLayer layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'text_preprocessing'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the TextPreprocessingLayer layer.</p>"},{"location":"layers_factory/#kdp.layers_factory.PreprocessorLayerFactory.transformer_block_layer","title":"<code>transformer_block_layer(name='transformer', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a TransformerBlock layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'transformer'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the TransformerBlock layer.</p>"},{"location":"layers_factory/#kdp.layers_factory.PreprocessorLayerFactory.variable_selection_layer","title":"<code>variable_selection_layer(name='variable_selection', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Create a VariableSelection layer.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> <code>'variable_selection'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the layer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>tf.keras.layers.Layer</code> <p>An instance of the VariableSelection layer.</p>"},{"location":"motivation/","title":"\ud83c\udf66 The Motivation Behind Keras Data Processor","text":"<p>The burning question now is \u2753:</p> <p>Why create a new preprocessing pipeline or model when we already have an excellent tool like Keras FeatureSpace?</p> <p>While <code>Keras FeatureSpace</code> has been a cornerstone in many of my projects, delivering great results, I encountered significant challenges in a high-volume data project. The tool required multiple data passes (proportional to the number of features), executing <code>.adapt</code> for each feature. This led to exceedingly long preprocessing times and frequent out-of-memory errors.</p> <p>This experience motivated a deep dive into the internal workings of Keras FeatureSpace and thus, motivated me to develop a new preprocessing pipeline that could handle data more efficiently\u2014both in terms of speed and memory usage. Thus, the journey began to craft a solution that would:</p> <ul> <li> <p> Process data in a single pass, utilizing an iterative approach to avoid loading the entire dataset into memory, managed by a batch_size parameter.</p> </li> <li> <p> Introduce custom predefined preprocessing steps tailored to the feature type, controlled by a feature_type parameter.</p> </li> <li> <p> Offer greater flexibility for custom preprocessing steps and a more Pythonic internal implementation.</p> </li> <li> <p> Align closely with the API of Keras FeatureSpace (proposing something similar), with the hope that it might eventually be integrated into the KFS ecosystem.</p> </li> </ul>"},{"location":"motivation/#quick-benchmark-overview","title":"Quick Benchmark Overview","text":"<p>To demonstrate the effectiveness of our new preprocessing pipeline, we conducted a benchmark comparing it with the traditional Keras FeatureSpace (this will give you a glimps on what was described earlier for the big data cases). Here\u2019s how we did it:</p> Benchmarking Steps: <ul> <li> <p>Setup: We configure the benchmark by specifying a set number of features in a loop. Each feature's specification (either a normalized float or a categorical string) is defined in a dictionary.</p> </li> <li> <p>Data Generation: For each set number of data points determined in another loop, we generate mock data based on the feature specifications and data points, which is then saved to a CSV file.</p> </li> <li> <p>Memory Management: We use garbage collection to free up memory before and after each benchmarking run, coupled with a 10-second cooldown period to ensure all operations complete fully.</p> </li> <li> <p>Performance Measurement: For both the Keras Data Processor (KDP) and Keras Feature Space (FS), we measure and record CPU and memory usage before and after their respective functions run, noting the time taken.</p> </li> <li> <p>Results Compilation: We collect and log results including the number of features, data points, execution time, memory, and CPU usage for each function in a structured format.</p> </li> </ul> <p>The results clearly illustrate the benefits, especially as the complexity of the data increases:</p> <p></p> <p>The graph shows a steep rise in processing time with an increase in data points for both <code>KDP</code> and <code>FS</code>. However, KDP consistently outperforms <code>FS</code>, with the gap widening as the number of data points grows.</p> <p></p> <p>This graph depicts the processing time increase with more features. Again, <code>KDP</code> outpaces <code>FS</code>, demonstrating substantial efficiency improvements.</p> <p>The combined effect of both the number of features and data points leads to significant performance gains on the <code>KDP</code> sice and time and memory hungry <code>FS</code> for the bigger and more complex datasets. This project was born from the need for better efficiency, and it\u2019s my hope to continue refining this tool with community support, pushing the boundaries of what we can achieve in data preprocessing (and maybe one day integrating it directly into Keras \u2764\ufe0f)!</p> <p>There is much to be done and many features to be added, but I am excited to see where this journey takes us. Let\u2019s build something great together! \ud83d\ude80\ud83d\udd27</p>"},{"location":"quick_start/","title":"\ud83d\ude80 Quick Start Guide","text":""},{"location":"quick_start/#installation","title":"\ud83d\udce6 Installation","text":"<pre><code>pip install keras-data-processor\n</code></pre>"},{"location":"quick_start/#basic-usage","title":"\ud83c\udfaf Basic Usage","text":""},{"location":"quick_start/#1-define-your-features","title":"1\ufe0f\u20e3 Define Your Features","text":"<pre><code>from kdp.processor import PreprocessingModel\nfrom kdp.features import NumericalFeature, CategoricalFeature\n\n# Define features\nfeatures = {\n    \"age\": NumericalFeature(),\n    \"income\": NumericalFeature(scaling=\"standard\"),\n    \"occupation\": CategoricalFeature(embedding_dim=32),\n    \"education\": CategoricalFeature(embedding_dim=16)\n}\n</code></pre>"},{"location":"quick_start/#2-create-preprocessing-model","title":"2\ufe0f\u20e3 Create Preprocessing Model","text":"<pre><code># Initialize the model\nmodel = PreprocessingModel(\n    features=features,\n    tabular_attention=True,  # Enable attention mechanism\n    feature_selection=True   # Enable feature selection\n)\n</code></pre>"},{"location":"quick_start/#useful-links","title":"\ud83d\udd17 Useful Links","text":"<ul> <li>\ud83d\udcda Full Documentation</li> <li>\ud83d\udcbb GitHub Repository</li> <li>\ud83d\udc1b Issue Tracker</li> </ul>"},{"location":"tabular_attention/","title":"\ud83c\udfaf Tabular Attention in KDP","text":""},{"location":"tabular_attention/#overview","title":"\ud83d\udcda Overview","text":"<p>KDP includes powerful attention mechanisms for tabular data processing:</p> <ol> <li>\ud83d\udd04 Standard TabularAttention: Uniform feature processing</li> <li>\ud83c\udf9b\ufe0f MultiResolutionTabularAttention: Type-specific feature processing</li> </ol>"},{"location":"tabular_attention/#standard-tabularattention","title":"\ud83d\udd04 Standard TabularAttention","text":"<p>The TabularAttention layer applies attention uniformly across all features, capturing:</p> <ul> <li>\ud83d\udd17 Dependencies between features for each sample</li> <li>\ud83d\udcca Dependencies between samples for each feature</li> </ul>"},{"location":"tabular_attention/#multiresolutiontabularattention","title":"\ud83c\udf9b\ufe0f MultiResolutionTabularAttention","text":"<p>The MultiResolutionTabularAttention implements a hierarchical attention mechanism:</p> <ul> <li>\ud83d\udcc8 Numerical Features: Full-resolution attention preserving precise numerical relationships</li> <li>\ud83c\udff7\ufe0f Categorical Features: Embedding-based attention capturing categorical patterns</li> <li>\ud83d\udd04 Cross-Feature Attention: Hierarchical attention between numerical and categorical features</li> </ul>"},{"location":"tabular_attention/#usage-examples","title":"\ud83d\udcbb Usage Examples","text":""},{"location":"tabular_attention/#standard-tabularattention_1","title":"Standard TabularAttention","text":"<pre><code>from kdp.processor import PreprocessingModel, TabularAttentionPlacementOptions\n\nmodel = PreprocessingModel(\n    # ... other parameters ...\n    tabular_attention=True,\n    tabular_attention_heads=4,\n    tabular_attention_dim=64,\n    tabular_attention_dropout=0.1,\n    tabular_attention_placement=TabularAttentionPlacementOptions.ALL_FEATURES.value,\n)\n</code></pre>"},{"location":"tabular_attention/#categorical-tabular-attention","title":"Categorical Tabular Attention","text":"<pre><code>from kdp.processor import PreprocessingModel, TabularAttentionPlacementOptions\n\nmodel = PreprocessingModel(\n    # ... other parameters ...\n    tabular_attention=True,\n    tabular_attention_heads=4,\n    tabular_attention_dim=64,\n    tabular_attention_dropout=0.1,\n    tabular_attention_embedding_dim=32,  # Dimension for categorical embeddings\n    tabular_attention_placement=TabularAttentionPlacementOptions.CATEGORICAL.value,\n)\n</code></pre>"},{"location":"tabular_attention/#multi-resolution-tabularattention","title":"Multi-Resolution TabularAttention","text":"<pre><code>from kdp.processor import PreprocessingModel, TabularAttentionPlacementOptions\n\nmodel = PreprocessingModel(\n    # ... other parameters ...\n    tabular_attention=True,\n    tabular_attention_heads=4,\n    tabular_attention_dim=64,\n    tabular_attention_dropout=0.1,\n    tabular_attention_embedding_dim=32,  # Dimension for categorical embeddings\n    tabular_attention_placement=TabularAttentionPlacementOptions.MULTI_RESOLUTION.value,\n)\n</code></pre>"},{"location":"tabular_attention/#configuration-options","title":"\u2699\ufe0f Configuration Options","text":""},{"location":"tabular_attention/#core-parameters","title":"Core Parameters","text":"Parameter Type Description <code>tabular_attention</code> bool Enable/disable attention mechanisms <code>tabular_attention_heads</code> int Number of attention heads <code>tabular_attention_dim</code> int Dimension of the attention model <code>tabular_attention_dropout</code> float Dropout rate for regularization"},{"location":"tabular_attention/#placement-options","title":"\ud83c\udfaf Placement Options","text":"<p>Choose where to apply attention using <code>tabular_attention_placement</code>:</p> <ul> <li><code>ALL_FEATURES</code>: Apply uniform attention to all features</li> <li><code>NUMERIC</code>: Apply only to numeric features</li> <li><code>CATEGORICAL</code>: Apply only to categorical features</li> <li><code>MULTI_RESOLUTION</code>: Use type-specific attention mechanisms</li> <li><code>NONE</code>: Disable attention</li> </ul>"},{"location":"tabular_attention/#multi-resolution-settings","title":"\ud83c\udf9b\ufe0f Multi-Resolution Settings","text":"<ul> <li><code>tabular_attention_embedding_dim</code>: Dimension for categorical embeddings in multi-resolution mode</li> </ul>"},{"location":"tabular_attention/#how-it-works","title":"\ud83d\udd0d How It Works","text":""},{"location":"tabular_attention/#standard-tabularattention-architecture","title":"Standard TabularAttention Architecture","text":"<ol> <li>\ud83d\udd04 Self-Attention: Applied uniformly across all features</li> <li>\ud83d\udcca Layer Normalization: Stabilizes learning</li> <li>\ud83e\uddee Feed-forward Network: Processes attention outputs</li> </ol>"},{"location":"tabular_attention/#multiresolutiontabularattention-architecture","title":"MultiResolutionTabularAttention Architecture","text":"<ol> <li>\ud83d\udcc8 Numerical Processing:</li> <li>Full-resolution self-attention</li> <li>Preserves numerical precision</li> <li> <p>Captures complex numerical relationships</p> </li> <li> <p>\ud83c\udff7\ufe0f Categorical Processing:</p> </li> <li>Embedding-based attention</li> <li>Lower-dimensional representations</li> <li> <p>Captures categorical patterns efficiently</p> </li> <li> <p>\ud83d\udd04 Cross-Feature Integration:</p> </li> <li>Hierarchical attention between feature types</li> <li>Numerical features attend to categorical features</li> <li>Preserves type-specific characteristics while enabling interaction</li> </ol>"},{"location":"tabular_attention/#best-practices","title":"\ud83d\udcc8 Best Practices","text":""},{"location":"tabular_attention/#when-to-use-standard-tabularattention","title":"When to Use Standard TabularAttention","text":"<ul> <li>Data has uniform feature importance</li> <li>Features are of similar scales</li> <li>Memory usage is a concern</li> </ul>"},{"location":"tabular_attention/#when-to-use-multiresolutiontabularattention","title":"When to Use MultiResolutionTabularAttention","text":"<ul> <li>Mixed numerical and categorical features</li> <li>Different feature types have different importance</li> <li>Need to preserve type-specific characteristics</li> <li>Complex interactions between feature types</li> </ul>"},{"location":"tabular_attention/#configuration-tips","title":"Configuration Tips","text":"<ol> <li>Attention Heads:</li> <li>Start with 4-8 heads</li> <li>Increase for complex relationships</li> <li> <p>Monitor computational cost</p> </li> <li> <p>Dimensions:</p> </li> <li><code>tabular_attention_dim</code>: Based on feature complexity</li> <li><code>tabular_attention_embedding_dim</code>: Usually smaller than main dimension</li> <li> <p>Balance between expressiveness and efficiency</p> </li> <li> <p>Dropout:</p> </li> <li>Start with 0.1</li> <li>Increase if overfitting</li> <li>Monitor validation performance</li> </ol>"},{"location":"tabular_attention/#advanced-usage","title":"\ud83e\udd16 Advanced Usage","text":""},{"location":"tabular_attention/#custom-layer-integration","title":"Custom Layer Integration","text":"<pre><code>from kdp.custom_layers import MultiResolutionTabularAttention\nimport tensorflow as tf\n\n# Create custom model with multi-resolution attention\nnumerical_inputs = tf.keras.Input(shape=(num_numerical, numerical_dim))\ncategorical_inputs = tf.keras.Input(shape=(num_categorical, categorical_dim))\n\nattention_layer = MultiResolutionTabularAttention(\n    num_heads=4,\n    d_model=64,\n    embedding_dim=32,\n    dropout_rate=0.1\n)\n\nnum_attended, cat_attended = attention_layer(numerical_inputs, categorical_inputs)\ncombined = tf.keras.layers.Concatenate(axis=1)([num_attended, cat_attended])\noutputs = tf.keras.layers.Dense(1)(combined)\n\nmodel = tf.keras.Model(\n    inputs=[numerical_inputs, categorical_inputs],\n    outputs=outputs\n)\n</code></pre>"},{"location":"tabular_attention/#layer-factory-usage","title":"Layer Factory Usage","text":"<pre><code>from kdp.layers_factory import PreprocessorLayerFactory\n\nattention_layer = PreprocessorLayerFactory.multi_resolution_attention_layer(\n    num_heads=4,\n    d_model=64,\n    embedding_dim=32,\n    dropout_rate=0.1,\n    name=\"custom_multi_attention\"\n)\n</code></pre>"},{"location":"tabular_attention/#performance-considerations","title":"\ud83d\udcca Performance Considerations","text":"<ol> <li>Memory Usage:</li> <li>MultiResolutionTabularAttention is more memory-efficient for categorical features</li> <li>Uses lower-dimensional embeddings for categorical data</li> <li> <p>Consider batch size when using multiple attention heads</p> </li> <li> <p>Computational Cost:</p> </li> <li>Standard TabularAttention: O(n\u00b2) for n features</li> <li>MultiResolutionTabularAttention: O(n_num\u00b2 + n_cat\u00b2) for numerical and categorical features</li> <li> <p>Balance between resolution and performance</p> </li> <li> <p>Training Tips:</p> </li> <li>Start with smaller dimensions and increase if needed</li> <li>Monitor memory usage and training time</li> <li>Use gradient clipping to stabilize training</li> </ol>"},{"location":"tabular_attention/#references","title":"\ud83d\udcda References","text":"<ul> <li>Attention Is All You Need - Original transformer paper</li> <li>TabNet: Attentive Interpretable Tabular Learning - Attention for tabular data</li> <li>Heterogeneous Graph Attention Network - Multi-type attention mechanisms</li> </ul>"},{"location":"transformer_blocks/","title":"\ud83e\udd16 TransformerBlocks \ud83c\udf1f","text":"<p>You can add transformer blocks to  your preprocessing model by simply defining required configuration when initializing the <code>Preprocessor</code> class:</p> <p>with the following arguments:</p> <ul> <li> <p><code>transfo_nr_blocks</code> (int): The number of transformer blocks in sequence (default=None, transformer block is disabled by default).</p> </li> <li> <p><code>transfo_nr_heads</code> (int): The number of heads for the transformer block (default=3).</p> </li> <li> <p><code>transfo_ff_units</code> (int): The number of feed forward units for the transformer (default=16).</p> </li> <li> <p><code>transfo_dropout_rate</code> (float): The dropout rate for the transformer block (default=0.25).</p> </li> <li> <p><code>transfo_placement</code> (str): The placement of the transformer block withe the following options:</p> <ul> <li><code>CATEGORICAL</code> -&gt; only after categorical and text variables</li> <li><code>ALL_FEATURES</code> -&gt; after all concatenated features).</li> </ul> </li> </ul> <p>This used a dedicated TransformerBlockLayer to handle the transformer block logic.</p>"},{"location":"transformer_blocks/#code-examples","title":"\ud83d\udcbb Code Examples:","text":"<pre><code>from kdp.processor import PreprocessingModel, OutputModeOptions, TransformerBlockPlacementOptions\n\nppr = PreprocessingModel(\n    path_data=\"data/test_data.csv\",\n    features_specs=features_specs,\n    features_stats_path=\"stats_data.json\",\n    output_mode=OutputModeOptions.CONCAT,\n    # TRANSFORMERS BLOCK CONTROLL\n    transfo_nr_blocks=3, # if 0, transformer block is disabled\n    transfo_nr_heads=3,\n    transfo_ff_units=16,\n    transfo_dropout_rate=0.25,\n    transfo_placement=TransformerBlockPlacementOptions.ALL_FEATURES,\n</code></pre> <p>There are two options for the <code>transfo_placement</code> argument controlled using <code>TransformerBlockPlacementOptions</code> class:</p> <ul> <li> <p> <code>CATEGORICAL</code>: The transformer block is applied only to the categorical + text features: <code>TransformerBlockPlacementOptions.CATEGORICAL</code> only.</p> <p>The corresponding architecture may thus look like this: </p> </li> <li> <p> <code>ALL_FEATURES</code>: The transformer block is applied to all features: <code>TransformerBlockPlacementOptions.ALL_FEATURES</code></p> <p>The corresponding architecture may thus look like this: </p> </li> </ul>"}]}