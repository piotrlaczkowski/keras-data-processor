# ğŸ¯ Feature Selection in KDP

## ğŸ“š Overview

KDP includes a sophisticated feature selection mechanism based on the Gated Residual Variable Selection Network (GRVSN) architecture. This powerful system automatically learns and selects the most important features in your data.

## ğŸ§© Core Components

### 1. ğŸ”€ GatedLinearUnit

The foundation of our feature selection system:

```python
gl = GatedLinearUnit(units=64)
x = tf.random.normal((32, 100))
y = gl(x)
```

**Key Features:**
* ğŸ”„ Applies linear transformation with sigmoid gate
* ğŸ›ï¸ Selectively filters input data
* ğŸ” Controls information flow through the network

### 2. ğŸ—ï¸ GatedResidualNetwork

Combines gated units with residual connections:

```python
grn = GatedResidualNetwork(units=64, dropout_rate=0.2)
x = tf.random.normal((32, 100))
y = grn(x)
```

**Key Features:**
* âš¡ Uses ELU activation for non-linearity
* ğŸ² Includes dropout for regularization
* ğŸ”„ Adds residual connections for better gradient flow
* ğŸ“Š Applies layer normalization for stability

### 3. ğŸ¯ VariableSelection

The main feature selection component:

```python
vs = VariableSelection(nr_features=3, units=64, dropout_rate=0.2)
x1 = tf.random.normal((32, 100))
x2 = tf.random.normal((32, 200))
x3 = tf.random.normal((32, 300))
selected_features, weights = vs([x1, x2, x3])
```

**Key Features:**
* ğŸ”„ Independent GRN processing for each feature
* âš–ï¸ Calculates feature importance weights via softmax
* ğŸ“Š Returns both selected features and their weights
* ğŸ”§ Supports varying input dimensions per feature

## ğŸ’» Usage Guide

### Configuration

Set up feature selection in your preprocessing model:

```python
model = PreprocessingModel(
    # ... other parameters ...
    feature_selection_placement="all_features",  # or "numeric" or "categorical"
    feature_selection_units=64,
    feature_selection_dropout=0.2
)
```

### ğŸ¯ Placement Options

Choose where to apply feature selection using `FeatureSelectionPlacementOptions`:

| Option | Description |
|--------|-------------|
| `NONE` | Disable feature selection |
| `NUMERIC` | Apply to numeric features only |
| `CATEGORICAL` | Apply to categorical features only |
| `ALL_FEATURES` | Apply to all features |

### ğŸ“Š Accessing Feature Weights

Monitor feature importance after processing:

```python
# Process your data
processed = model.transform(data)

# Access feature weights
numeric_weights = processed["numeric_feature_weights"]
categorical_weights = processed["categorical_feature_weights"]

# Print feature importance
for feature_name in features:
    weights = processed_data[f"{feature_name}_weights"]
    print(f"Feature {feature_name} importance: {weights.mean()}")
```

## ğŸŒŸ Benefits

1. **ğŸ¤– Automatic Feature Selection**
   * Learns feature importance automatically
   * Adapts to your specific dataset
   * Reduces manual feature engineering

2. **ğŸ“Š Interpretability**
   * Clear feature importance weights
   * Insights into model decisions
   * Easy to explain to stakeholders

3. **âš¡ Improved Performance**
   * Focuses on relevant features
   * Reduces noise in the data
   * Better model convergence

## ğŸ”§ Best Practices

### Hyperparameter Tuning

* ğŸ¯ Start with default values
* ğŸ“ˆ Adjust based on validation performance
* ğŸ”„ Monitor feature importance stability

### Performance Optimization

* âš¡ Use appropriate batch sizes
* ğŸ² Adjust dropout rates as needed
* ğŸ“Š Monitor memory usage

## ğŸ“š References

* [GRVSN Paper](https://arxiv.org/abs/xxxx.xxxxx)
* [Feature Selection in Deep Learning](https://arxiv.org/abs/xxxx.xxxxx)
* [KDP Documentation](https://kdp.readthedocs.io)

## ğŸ“š Example

Here's a complete example of using feature selection:

```python
from kdp.processor import PreprocessingModel
from kdp.features import NumericalFeature, CategoricalFeature

# Define features
features = {
    "numeric_1": NumericalFeature(
        name="numeric_1",
        feature_type=FeatureType.FLOAT_NORMALIZED
    ),
    "numeric_2": NumericalFeature(
        name="numeric_2",
        feature_type=FeatureType.FLOAT_NORMALIZED
    ),
    "category_1": CategoricalFeature(
        name="category_1",
        feature_type=FeatureType.STRING_CATEGORICAL
    )
}

# Create model with feature selection
model = PreprocessingModel(
    # ... other parameters ...
    features_specs=features,
    feature_selection_placement="all_features", # or "numeric" or "categorical"
    feature_selection_units=64,
    feature_selection_dropout=0.2
)

# Build and use the model
preprocessor = model.build_preprocessor()
processed_data = model.transform(data) # data can be pd.DataFrame, python Dict, or tf.data.Dataset

# Analyze feature importance
for feature_name in features:
    weights = processed_data[f"{feature_name}_weights"]
    print(f"Feature {feature_name} importance: {weights.mean()}")
```

## ğŸ“Š Testing

The feature selection components include comprehensive unit tests that verify:

1. Output shapes and types
2. Gating mechanism behavior
3. Residual connections
4. Dropout behavior
5. Feature weight properties
6. Serialization/deserialization

Run the tests using:
```bash
python -m pytest test/test_feature_selection.py -v
