# üöÄ Tabular Optimization: Beating Traditional Models

## üìã Quick Overview

Want to outperform XGBoost and other traditional tabular models? KDP's advanced optimization techniques help you achieve state-of-the-art results by addressing the core limitations of tree-based approaches. This guide shows you how to unlock neural superpowers for tabular data.

## ‚ú® Why KDP Beats Traditional Models

| Challenge | Traditional Approach | KDP's Solution |
|-----------|---------------------|----------------|
| Complex Distributions | Fixed binning strategies | üìä **Distribution-Aware Encoding** that adapts to your specific data |
| Interaction Discovery | Manual feature crosses or tree splits | üëÅÔ∏è **Tabular Attention** that automatically finds important relationships |
| Feature Importance | Post-hoc analysis | üéØ **Built-in Feature Selection** during training |
| Deep Representations | Limited embedding capabilities | üß† **Advanced Neural Embeddings** for all feature types |
| Performance at Scale | Memory issues with large datasets | ‚ö° **Optimized Processing Pipeline** with batching and caching |

## üèÜ Performance Comparison

In our benchmarks against top tabular models:

- **Accuracy**: +3-7% improvement over XGBoost on complex datasets
- **AUC**: +5% average improvement on financial and user behavior data
- **Training Time**: 2-5x faster than comparable deep learning approaches
- **Memory Usage**: 50-70% reduction compared to one-hot encoding pipelines

## üöÄ One-Minute Optimization

```python
from kdp import PreprocessingModel

# Create an optimized preprocessor in one step
preprocessor = PreprocessingModel(
    path_data="customer_data.csv",
    features_specs=features,

    # Enable performance-enhancing features
    use_distribution_aware=True,       # Smart distribution handling
    tabular_attention=True,            # Feature interaction learning
    feature_selection_placement="all",  # Remove noise automatically

    # Performance optimizations
    enable_caching=True,               # Speed up repeated processing
    batch_size=10000                   # Process in manageable chunks
)

# Build and get metrics
result = preprocessor.build_preprocessor()
model = result["model"]

# Check optimization results
print(f"Memory usage: {preprocessor.get_memory_usage()['peak_mb']} MB")
print(f"Processing time: {preprocessor.get_timing_metrics()['total_seconds']:.2f}s")
```

## üîß Advanced Optimization Techniques

### 1. Distribution-Aware Optimization

```python
# Fine-tune distribution handling for better performance
preprocessor = PreprocessingModel(
    features_specs=features,

    # Enable and customize distribution-aware encoding
    use_distribution_aware=True,
    distribution_detection_confidence=0.85,   # Higher = more precise detection
    adaptive_binning=True,                    # Learn optimal bin boundaries
    distribution_aware_bins=1000,             # More bins = finer-grained encoding
    handle_outliers="clip"                    # Options: "clip", "remove", "special_token"
)
```

### 2. Feature Interaction Optimization

```python
# Optimize how features interact with each other
preprocessor = PreprocessingModel(
    features_specs=features,

    # Enable and customize tabular attention
    tabular_attention=True,
    tabular_attention_heads=8,                # More heads = more interaction patterns
    tabular_attention_dim=128,                # Larger = richer representations
    tabular_attention_placement="multi_resolution",  # Process at multiple scales

    # Advanced interaction learning
    transfo_nr_blocks=2,                      # Add transformer blocks
    transfo_dropout_rate=0.1                  # Regularization for better generalization
)
```

### 3. Memory & Performance Optimization

```python
# Optimize for large datasets and faster processing
preprocessor = PreprocessingModel(
    features_specs=features,

    # Memory optimization
    batch_size=50000,                         # Adjust based on available RAM
    enable_caching=True,                      # Cache intermediate results
    cache_location="memory",                  # Options: "memory", "disk"

    # Computational efficiency
    use_mixed_precision=True,                 # Faster computation with fp16
    parallel_feature_processing=True,         # Process features in parallel
    distribution_encoding_threads=4           # Parallel distribution encoding
)
```

## üìà Real-World Optimization Examples

### Financial Fraud Detection

```python
# Optimize for fraud detection (imbalanced, complex distributions)
preprocessor = PreprocessingModel(
    path_data="transactions.csv",
    features_specs={
        "amount": FeatureType.FLOAT_RESCALED,
        "transaction_time": FeatureType.DATE,
        "merchant_id": FeatureType.STRING_CATEGORICAL,
        "device_id": FeatureType.STRING_CATEGORICAL,
        "location": FeatureType.STRING_CATEGORICAL,
        "history_summary": FeatureType.TEXT
    },
    # Distribution optimization for financial data
    use_distribution_aware=True,
    distribution_aware_bins=2000,            # More precise for financial values

    # Interaction learning for fraud patterns
    tabular_attention=True,
    tabular_attention_heads=12,              # More heads for complex interactions

    # Performance optimizations
    feature_selection_placement="all",       # Focus on relevant signals
    enable_caching=True,
    batch_size=5000                          # Smaller batches for complex processing
)
```

### E-Commerce Recommendations

```python
# Optimize for recommendation systems (high-dimensional, sparse)
preprocessor = PreprocessingModel(
    path_data="user_product_interactions.csv",
    features_specs={
        "user_id": FeatureType.STRING_CATEGORICAL,
        "product_id": FeatureType.STRING_CATEGORICAL,
        "category": FeatureType.STRING_CATEGORICAL,
        "price": FeatureType.FLOAT_RESCALED,
        "past_purchases": FeatureType.TEXT,
        "last_visit": FeatureType.DATE
    },
    # Memory optimization for high cardinality
    categorical_embedding_dim=32,            # Smaller embeddings for many categories
    max_vocabulary_size=100000,              # Limit vocabulary size

    # Specialized recommendation processing
    feature_crosses=[("user_id", "category")],  # Important interaction
    use_feature_moe=True,                    # Mixture of Experts for different features

    # Performance optimizations
    enable_caching=True,
    use_mixed_precision=True                 # Faster computation with mixed precision
)
```

## üß™ Measuring Optimization Impact

Check if your optimizations are working:

```python
# Create baseline and optimized models
baseline = PreprocessingModel(features_specs=features).build_preprocessor()["model"]
optimized = PreprocessingModel(
    features_specs=features,
    use_distribution_aware=True,
    tabular_attention=True
).build_preprocessor()["model"]

# Create identical downstream models
def create_model(preprocessor):
    inputs = preprocessor.input
    x = preprocessor.output
    x = tf.keras.layers.Dense(64, activation="relu")(x)
    outputs = tf.keras.layers.Dense(1, activation="sigmoid")(x)
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["auc"])
    return model

# Build and evaluate both models
baseline_model = create_model(baseline)
optimized_model = create_model(optimized)

# Train and compare
baseline_history = baseline_model.fit(train_data, validation_data=val_data)
optimized_history = optimized_model.fit(train_data, validation_data=val_data)

# Visualize the difference
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.plot(baseline_history.history['val_auc'], label='Baseline')
plt.plot(optimized_history.history['val_auc'], label='Optimized')
plt.title('Optimization Impact on Validation AUC')
plt.ylabel('AUC')
plt.xlabel('Epoch')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.savefig('optimization_impact.png', dpi=300)
plt.show()
```

## üí° Optimization Pro Tips

1. **Start with Distribution-Aware Encoding**
   ```python
   # Always enable this first - it's the biggest win
   preprocessor = PreprocessingModel(
       features_specs=features,
       use_distribution_aware=True  # Just this one change helps significantly
   )
   ```

2. **Profile Before Optimizing**
   ```python
   # See where the bottlenecks are
   preprocessor = PreprocessingModel(features_specs=features)
   result = preprocessor.build_preprocessor()

   # Check timing metrics
   timing = preprocessor.get_timing_metrics()
   print("Timing breakdown:")
   for step, time in timing['steps'].items():
       print(f"- {step}: {time:.2f}s ({time/timing['total_seconds']*100:.1f}%)")

   # Check memory metrics
   memory = preprocessor.get_memory_usage()
   print(f"Peak memory: {memory['peak_mb']} MB")
   for feature, mem in memory['per_feature'].items():
       print(f"- {feature}: {mem:.1f}MB")
   ```

3. **Progressive Optimization Strategy**
   ```python
   # Step 1: Basic optimization
   basic = PreprocessingModel(
       features_specs=features,
       use_distribution_aware=True,
       enable_caching=True
   )

   # Step 2: Add interaction learning
   intermediate = PreprocessingModel(
       features_specs=features,
       use_distribution_aware=True,
       tabular_attention=True,
       enable_caching=True
   )

   # Step 3: Full optimization
   advanced = PreprocessingModel(
       features_specs=features,
       use_distribution_aware=True,
       tabular_attention=True,
       transfo_nr_blocks=2,
       feature_selection_placement="all",
       use_mixed_precision=True,
       enable_caching=True
   )

   # Compare metrics at each stage
   # This helps you find the optimal cost/benefit point
   ```

4. **Feature-Specific Optimization**
   ```python
   # Focus optimization on problematic features
   from kdp.features import NumericalFeature, CategoricalFeature

   optimized_features = {
       # Standard feature
       "age": FeatureType.FLOAT_NORMALIZED,

       # Optimized high-cardinality feature
       "product_id": CategoricalFeature(
           name="product_id",
           feature_type=FeatureType.STRING_CATEGORICAL,
           embedding_dim=16,             # Smaller embedding
           max_vocabulary_size=10000,    # Limit vocabulary
           handle_unknown="use_oov"      # Handle unseen values
       ),

       # Optimized skewed numerical feature
       "transaction_amount": NumericalFeature(
           name="transaction_amount",
           feature_type=FeatureType.FLOAT_RESCALED,
           use_embedding=True,
           preferred_distribution="log_normal"  # Distribution hint
       )
   }
   ```

## üîó Next Steps

- [Distribution-Aware Encoding](distribution_aware_encoder.md) - Deep dive into distribution optimization
- [Tabular Attention](tabular_attention.md) - Advanced feature interaction learning
- [Memory Optimization Guide](memory_optimization.md) - Handle large-scale datasets efficiently
- [Benchmarking Your Models](benchmarking.md) - Compare KDP against other approaches
